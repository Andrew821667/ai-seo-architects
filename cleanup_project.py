#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ AI SEO Architects –æ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –ª–∏—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ–∞–π–ª—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
"""

import os
import sys
import shutil
from pathlib import Path
from datetime import datetime
import re

class ProjectCleaner:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.files_to_remove = []
        self.directories_to_remove = []
        self.analysis_results = {}
    
    def analyze_project(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ª–∏—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        print("üîç –ê–ù–ê–õ–ò–ó –ü–†–û–ï–ö–¢–ê –ù–ê –ü–†–ï–î–ú–ï–¢ –õ–ò–®–ù–ò–• –§–ê–ô–õ–û–í")
        print("=" * 60)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.analyze_temporary_files()
        self.analyze_test_result_files()
        self.analyze_duplicate_vector_stores()
        self.analyze_backup_files()
        self.analyze_development_files()
        self.analyze_empty_directories()
        
        self.generate_cleanup_report()
    
    def analyze_temporary_files(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        print("\nüìÅ –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        temp_patterns = [
            r'.*_\d{8}_\d{6}\.md$',  # –§–∞–π–ª—ã —Å timestamp
            r'.*\.tmp$',
            r'.*\.temp$',
            r'.*~$',
            r'__pycache__',
            r'\.pyc$',
            r'\.DS_Store$'
        ]
        
        temp_files = []
        
        for pattern in temp_patterns:
            for file_path in self.project_root.rglob('*'):
                if re.search(pattern, str(file_path)):
                    temp_files.append(file_path)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        specific_temp_files = [
            'AGENT_TEST_RESULTS_20250805_210750.md',
            'AGENT_TEST_RESULTS_20250805_210913.md', 
            'AGENT_TEST_RESULTS_20250805_211014.md',
            'COMPREHENSIVE_AGENT_ANALYSIS_20250805_212658.md',
            'COMPREHENSIVE_AGENT_ANALYSIS_20250805_212748.md',
            'COMPREHENSIVE_AGENT_ANALYSIS_20250805_212939.md',
            'COMPREHENSIVE_AGENT_ANALYSIS_20250805_213029.md',
            'DEMO_RESULTS_20250805_211112.md',
            'DEMO_RESULTS_20250805_211649.md',
            'DEMO_RESULTS_20250805_211902.md',
            'DEMO_RESULTS_20250805_211934.md',
            'DEMO_RESULTS_20250805_212024.md',
            'DEMO_RESULTS_20250805_212105.md',
            'TESTING_BREAKTHROUGH_RESULTS.md'
        ]
        
        for filename in specific_temp_files:
            file_path = self.project_root / filename
            if file_path.exists():
                temp_files.append(file_path)
        
        self.analysis_results['temporary_files'] = temp_files
        print(f"   –ù–∞–π–¥–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(temp_files)}")
    
    def analyze_test_result_files(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print("\nüß™ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        test_result_files = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        test_patterns = [
            r'test_.*\.py$',
            r'.*_test\.py$'
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        main_test_files = [
            'test_agents_integration.py',
            'test_api_endpoints.py', 
            'test_faiss_integration.py',
            'test_mcp_integration.py'
        ]
        
        # –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å (–¥—É–±–ª–∏–∫–∞—Ç—ã –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ)
        redundant_test_files = [
            'test_enhanced_integration.py',  # –î—É–±–ª–∏–∫–∞—Ç
            'test_rag_integration.py',       # –£—Å—Ç–∞—Ä–µ–ª
            'test_real_data_demo.py',        # –î–µ–º–æ –≤–µ—Ä—Å–∏—è
            'quick_demo_test.py',            # –ë—ã—Å—Ç—Ä—ã–π –¥–µ–º–æ —Ç–µ—Å—Ç
            'comprehensive_agent_test.py',   # –£—Å—Ç–∞—Ä–µ–ª
            'analyze_knowledge_quality.py', # –í—Ä–µ–º–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            'test_russian_agents_integration.py',  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ç–µ—Å—Ç - –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å
            'test_all_agents_vectorization.py',    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ç–µ—Å—Ç - –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å  
            'update_vectorization.py'       # –£—Ç–∏–ª–∏—Ç–∞ - –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        ]
        
        for filename in redundant_test_files[:-3]:  # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3
            file_path = self.project_root / filename
            if file_path.exists():
                test_result_files.append(file_path)
        
        self.analysis_results['test_files'] = test_result_files
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(test_result_files)}")
    
    def analyze_duplicate_vector_stores(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        print("\nüóÇÔ∏è –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â...")
        
        vector_stores_path = self.project_root / 'data' / 'vector_stores'
        duplicate_stores = []
        
        if vector_stores_path.exists():
            # –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º _agent
            for item in vector_stores_path.iterdir():
                if item.is_dir() and item.name.endswith('_agent'):
                    base_name = item.name[:-6]  # –£–±–∏—Ä–∞–µ–º _agent
                    base_dir = vector_stores_path / base_name
                    
                    if base_dir.exists():
                        # –ï—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç - –º–æ–∂–µ–º —É–¥–∞–ª–∏—Ç—å –≤–µ—Ä—Å–∏—é —Å _agent
                        duplicate_stores.append(item)
                        print(f"   –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω: {item.name} -> {base_name}")
        
        self.analysis_results['duplicate_vectors'] = duplicate_stores
        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â: {len(duplicate_stores)}")
    
    def analyze_backup_files(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º backup —Ñ–∞–π–ª—ã"""
        print("\nüíæ –ê–Ω–∞–ª–∏–∑ backup —Ñ–∞–π–ª–æ–≤...")
        
        backup_dirs = []
        data_path = self.project_root / 'data'
        
        if data_path.exists():
            for item in data_path.iterdir():
                if item.is_dir() and 'backup' in item.name:
                    backup_dirs.append(item)
        
        self.analysis_results['backup_dirs'] = backup_dirs
        print(f"   –ù–∞–π–¥–µ–Ω–æ backup –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(backup_dirs)}")
        
        # –ú–æ–∂–µ–º –æ—Å—Ç–∞–≤–∏—Ç—å —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π backup –∏ —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ
        if backup_dirs:
            sorted_backups = sorted(backup_dirs, key=lambda x: x.stat().st_mtime, reverse=True)
            old_backups = sorted_backups[1:]  # –í—Å–µ –∫—Ä–æ–º–µ —Å–∞–º–æ–≥–æ —Å–≤–µ–∂–µ–≥–æ
            
            self.analysis_results['old_backups'] = old_backups
            print(f"   –°—Ç–∞—Ä—ã—Ö backup'–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {len(old_backups)}")
    
    def analyze_development_files(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"""
        print("\nüîß –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏...")
        
        dev_files = []
        
        # –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        potential_dev_files = [
            'setup_seo_ai_models.py',  # –£—Å—Ç–∞—Ä–µ–ª –µ—Å–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞
            '1.md',                    # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∞–≤–∏–ª
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        for filename in potential_dev_files:
            file_path = self.project_root / filename
            if file_path.exists():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ 1.md
                if filename == '1.md':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if len(content.strip()) < 100:  # –ï—Å–ª–∏ —Ñ–∞–π–ª –ø–æ—á—Ç–∏ –ø—É—Å—Ç–æ–π
                            dev_files.append(file_path)
                else:
                    dev_files.append(file_path)
        
        self.analysis_results['dev_files'] = dev_files
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏: {len(dev_files)}")
    
    def analyze_empty_directories(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        print("\nüìÅ –ê–Ω–∞–ª–∏–∑ –ø—É—Å—Ç—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        
        empty_dirs = []
        
        for root, dirs, files in os.walk(self.project_root):
            for dir_name in dirs:
                dir_path = Path(root) / dir_name
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ __init__.py
                try:
                    contents = list(dir_path.iterdir())
                    if not contents:
                        empty_dirs.append(dir_path)
                    elif len(contents) == 1 and contents[0].name == '__init__.py':
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ __init__.py –ø—É—Å—Ç–æ–π
                        init_file = contents[0]
                        if init_file.stat().st_size < 50:  # –ú–µ–Ω—å—à–µ 50 –±–∞–π—Ç
                            empty_dirs.append(dir_path)
                except PermissionError:
                    continue
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        system_dirs = ['__pycache__', '.git', '.pytest_cache', 'chroma_db']
        empty_dirs = [d for d in empty_dirs if d.name not in system_dirs]
        
        self.analysis_results['empty_dirs'] = empty_dirs
        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—É—Å—Ç—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(empty_dirs)}")
    
    def generate_cleanup_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –ø–æ –æ—á–∏—Å—Ç–∫–µ"""
        print("\n" + "=" * 60)
        print("üìã –û–¢–ß–ï–¢ –ü–û –û–ß–ò–°–¢–ö–ï –ü–†–û–ï–ö–¢–ê")
        print("=" * 60)
        
        total_files = 0
        total_size = 0
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        for category, files in self.analysis_results.items():
            if files:
                category_size = 0
                for file_path in files:
                    if file_path.exists():
                        if file_path.is_file():
                            size = file_path.stat().st_size
                            category_size += size
                        elif file_path.is_dir():
                            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                            for root, dirs, filenames in os.walk(file_path):
                                for filename in filenames:
                                    fp = Path(root) / filename
                                    if fp.exists():
                                        category_size += fp.stat().st_size
                
                total_files += len(files)
                total_size += category_size
                
                print(f"\nüìÇ {category.upper().replace('_', ' ')}:")
                print(f"   –§–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(files)}")
                print(f"   –†–∞–∑–º–µ—Ä: {self.format_size(category_size)}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
                for i, file_path in enumerate(files[:3]):
                    rel_path = file_path.relative_to(self.project_root)
                    print(f"   ‚Ä¢ {rel_path}")
                
                if len(files) > 3:
                    print(f"   ... –∏ –µ—â–µ {len(files) - 3} —Ñ–∞–π–ª–æ–≤")
        
        print(f"\nüéØ –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {total_files}")
        print(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {self.format_size(total_size)}")
        
        return total_files > 0
    
    def format_size(self, size_bytes: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞"""
        if size_bytes == 0:
            return "0 B"
        
        units = ['B', 'KB', 'MB', 'GB']
        size = float(size_bytes)
        unit_index = 0
        
        while size >= 1024 and unit_index < len(units) - 1:
            size /= 1024
            unit_index += 1
        
        return f"{size:.1f} {units[unit_index]}"
    
    def confirm_cleanup(self) -> bool:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ"""
        print("\n" + "=" * 60)
        print("‚ö†Ô∏è  –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–ï –£–î–ê–õ–ï–ù–ò–Ø")
        print("=" * 60)
        
        print("üî• –í–ù–ò–ú–ê–ù–ò–ï! –°–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—É–¥—É—Ç –£–î–ê–õ–ï–ù–´:")
        
        for category, files in self.analysis_results.items():
            if files:
                print(f"\nüìÇ {category.upper().replace('_', ' ')}:")
                for file_path in files:
                    rel_path = file_path.relative_to(self.project_root)
                    print(f"   üóëÔ∏è  {rel_path}")
        
        print(f"\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ? (yes/no): ", end="")
        response = input().strip().lower()
        
        return response in ['yes', 'y', '–¥–∞', '–¥']
    
    def perform_cleanup(self):
        """–í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É"""
        print("\nüßπ –í–´–ü–û–õ–ù–ï–ù–ò–ï –û–ß–ò–°–¢–ö–ò...")
        print("-" * 40)
        
        removed_count = 0
        errors = []
        
        for category, files in self.analysis_results.items():
            if files:
                print(f"\nüìÇ –û—á–∏—Å—Ç–∫–∞ {category.replace('_', ' ')}...")
                
                for file_path in files:
                    try:
                        if file_path.exists():
                            if file_path.is_file():
                                file_path.unlink()
                                print(f"   ‚úÖ –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª: {file_path.name}")
                            elif file_path.is_dir():
                                shutil.rmtree(file_path)
                                print(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {file_path.name}")
                            
                            removed_count += 1
                        
                    except Exception as e:
                        error_msg = f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}"
                        errors.append(error_msg)
                        print(f"   ‚ùå {error_msg}")
        
        print(f"\nüéâ –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–æ: {removed_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if errors:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–æ–∫: {len(errors)}")
            for error in errors:
                print(f"      ‚Ä¢ {error}")
        else:
            print(f"   üèÜ –ë–µ–∑ –æ—à–∏–±–æ–∫!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –°–ò–°–¢–ï–ú–ê –û–ß–ò–°–¢–ö–ò –ü–†–û–ï–ö–¢–ê AI SEO ARCHITECTS")
    print(f"üïí –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().isoformat()}")
    
    project_root = os.path.dirname(os.path.abspath(__file__))
    cleaner = ProjectCleaner(project_root)
    
    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç
        cleaner.analyze_project()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —á—Ç–æ —É–¥–∞–ª—è—Ç—å
        has_files_to_remove = any(files for files in cleaner.analysis_results.values())
        
        if not has_files_to_remove:
            print("\n‚ú® –ü–†–û–ï–ö–¢ –£–ñ–ï –ß–ò–°–¢–´–ô!")
            print("   –õ–∏—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        if cleaner.confirm_cleanup():
            cleaner.perform_cleanup()
        else:
            print("\nüëã –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    except KeyboardInterrupt:
        print("\nüëã –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()