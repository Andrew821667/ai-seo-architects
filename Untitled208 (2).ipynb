{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tBLh9XNpOSB",
        "outputId": "45f25dea-3071-48b6-d0a0-7c546406db62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ ÐšÐ›ÐžÐÐ˜Ð ÐžÐ’ÐÐÐ˜Ð• AI SEO ARCHITECTS\n",
            "=============================================\n",
            "ðŸ”— Ð ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹: https://github.com/Andrew821667/ai-seo-architects.git\n",
            "ðŸ“ Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÑƒÑ‚ÑŒ: /content/ai-seo-architects\n",
            "â¬‡ï¸ ÐšÐ»Ð¾Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚...\n",
            "âœ… ÐŸÑ€Ð¾ÐµÐºÑ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÐºÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½!\n",
            "ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ agents Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\n",
            "ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ core Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\n",
            "ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ knowledge Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\n",
            "ðŸ“ Ð Ð°Ð±Ð¾Ñ‡Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ: /content/ai-seo-architects\n",
            "âœ… ÐŸÑƒÑ‚ÑŒ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð² Python path\n",
            "\n",
            "ðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð³Ð¾Ñ‚Ð¾Ð² Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹\n",
            "âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‡ÐµÐ¹ÐºÐµ\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“¦ Ð¯Ð§Ð•Ð™ÐšÐ 1: ÐšÐ›ÐžÐÐ˜Ð ÐžÐ’ÐÐÐ˜Ð• ÐŸÐ ÐžÐ•ÐšÐ¢Ð AI SEO ARCHITECTS\n",
        "print(\"ðŸ“¦ ÐšÐ›ÐžÐÐ˜Ð ÐžÐ’ÐÐÐ˜Ð• AI SEO ARCHITECTS\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ\n",
        "REPO_URL = \"https://github.com/Andrew821667/ai-seo-architects.git\"\n",
        "LOCAL_PATH = \"/content/ai-seo-architects\"\n",
        "\n",
        "print(f\"ðŸ”— Ð ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹: {REPO_URL}\")\n",
        "print(f\"ðŸ“ Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÑƒÑ‚ÑŒ: {LOCAL_PATH}\")\n",
        "\n",
        "# Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ\n",
        "if os.path.exists(LOCAL_PATH):\n",
        "    print(\"ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°...\")\n",
        "    subprocess.run(['rm', '-rf', LOCAL_PATH], capture_output=True)\n",
        "\n",
        "# ÐšÐ»Ð¾Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹\n",
        "print(\"â¬‡ï¸ ÐšÐ»Ð¾Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚...\")\n",
        "try:\n",
        "    result = subprocess.run(['git', 'clone', REPO_URL, LOCAL_PATH],\n",
        "                          capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… ÐŸÑ€Ð¾ÐµÐºÑ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÐºÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½!\")\n",
        "\n",
        "        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ\n",
        "        if os.path.exists(f\"{LOCAL_PATH}/agents\"):\n",
        "            print(\"ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ agents Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\")\n",
        "        if os.path.exists(f\"{LOCAL_PATH}/core\"):\n",
        "            print(\"ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ core Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\")\n",
        "        if os.path.exists(f\"{LOCAL_PATH}/knowledge\"):\n",
        "            print(\"ðŸ“ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ knowledge Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\")\n",
        "\n",
        "        # ÐœÐµÐ½ÑÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‡ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ\n",
        "        os.chdir(LOCAL_PATH)\n",
        "        print(f\"ðŸ“ Ð Ð°Ð±Ð¾Ñ‡Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ: {os.getcwd()}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: {result.stderr}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ: {e}\")\n",
        "\n",
        "# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² Python path\n",
        "import sys\n",
        "if LOCAL_PATH not in sys.path:\n",
        "    sys.path.insert(0, LOCAL_PATH)\n",
        "    print(\"âœ… ÐŸÑƒÑ‚ÑŒ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð² Python path\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð³Ð¾Ñ‚Ð¾Ð² Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹\")\n",
        "print(\"âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‡ÐµÐ¹ÐºÐµ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Ð¯Ð§Ð•Ð™ÐšÐ 2: Ð£Ð¡Ð¢ÐÐÐžÐ’ÐšÐ Ð—ÐÐ’Ð˜Ð¡Ð˜ÐœÐžÐ¡Ð¢Ð•Ð™ Ð‘Ð•Ð— ÐšÐžÐÐ¤Ð›Ð˜ÐšÐ¢ÐžÐ’\n",
        "print(\"ðŸ“¦ Ð£Ð¡Ð¢ÐÐÐžÐ’ÐšÐ Ð—ÐÐ’Ð˜Ð¡Ð˜ÐœÐžÐ¡Ð¢Ð•Ð™ AI SEO ARCHITECTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ numpy/pandas ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ñ‹ Ð·Ð°Ñ€Ð°Ð½ÐµÐµ\n",
        "print(\"ðŸ”§ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐ˜Ð• Ð¡ÐžÐ’ÐœÐ•Ð¡Ð¢Ð˜ÐœÐžÐ¡Ð¢Ð˜ NUMPY/PANDAS:\")\n",
        "try:\n",
        "    # ÐŸÐµÑ€ÐµÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', '-q', 'numpy'],\n",
        "                   capture_output=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'numpy==1.24.3'],\n",
        "                   capture_output=True)\n",
        "    print(\"âœ… NumPy 1.24.3 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½\")\n",
        "\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', '-q', 'pandas'],\n",
        "                   capture_output=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'pandas==2.0.3'],\n",
        "                   capture_output=True)\n",
        "    print(\"âœ… Pandas 2.0.3 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ðµ: {e}\")\n",
        "\n",
        "# ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ð°ÐºÐµÑ‚Ñ‹ Ð´Ð»Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°\n",
        "required_packages = [\n",
        "    'openai==1.35.0',           # Ð¡Ð¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð°Ñ Ñ Colab Ð²ÐµÑ€ÑÐ¸Ñ\n",
        "    'langgraph==0.2.39',\n",
        "    'langchain==0.2.16',\n",
        "    'langchain-openai==0.1.25',\n",
        "    'langchain-community==0.2.17',\n",
        "    'faiss-cpu==1.7.4',\n",
        "    'pydantic==2.9.2',\n",
        "    'nest-asyncio==1.6.0',\n",
        "    'python-dotenv==1.0.1'\n",
        "]\n",
        "\n",
        "print(f\"\\nðŸ”§ Ð£Ð¡Ð¢ÐÐÐžÐ’ÐšÐ {len(required_packages)} ÐšÐ›Ð®Ð§Ð•Ð’Ð«Ð¥ ÐŸÐÐšÐ•Ð¢ÐžÐ’:\")\n",
        "success_count = 0\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        package_name = package.split('==')[0]\n",
        "        print(f\"â³ {package_name}...\", end=' ')\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ…\")\n",
        "            success_count += 1\n",
        "        else:\n",
        "            print(f\"âŒ\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ’¥\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢: {success_count}/{len(required_packages)} ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾\")\n",
        "\n",
        "# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹\n",
        "print(\"\\nðŸ§ª ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð˜ÐœÐŸÐžÐ Ð¢ÐžÐ’:\")\n",
        "critical_imports = [\n",
        "    ('openai', 'OpenAI ÐºÐ»Ð¸ÐµÐ½Ñ‚'),\n",
        "    ('langgraph', 'LangGraph Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€'),\n",
        "    ('langchain', 'LangChain Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº'),\n",
        "    ('faiss', 'FAISS Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð‘Ð”'),\n",
        "    ('numpy', 'NumPy'),\n",
        "    ('nest_asyncio', 'Async Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°')\n",
        "]\n",
        "\n",
        "working_imports = 0\n",
        "for module, description in critical_imports:\n",
        "    try:\n",
        "        __import__(module)\n",
        "        print(f\"âœ… {description}\")\n",
        "        working_imports += 1\n",
        "    except ImportError:\n",
        "        print(f\"âŒ {description}\")\n",
        "    except ValueError as e:\n",
        "        if \"numpy.dtype size changed\" in str(e):\n",
        "            print(f\"âš ï¸ {description} (ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚ÑŒ ÐžÐš)\")\n",
        "            working_imports += 1\n",
        "        else:\n",
        "            print(f\"âŒ {description}\")\n",
        "\n",
        "# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° async Ð´Ð»Ñ Jupyter\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    print(\"âœ… Async Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð°\")\n",
        "    working_imports += 1\n",
        "except:\n",
        "    print(\"âŒ Async Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°\")\n",
        "\n",
        "# Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°\n",
        "if working_imports >= 5:\n",
        "    print(f\"\\nðŸš€ ÐžÐ¢Ð›Ð˜Ð§ÐÐž! {working_imports}/{len(critical_imports)+1} ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹\")\n",
        "    print(\"âœ… Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ OpenAI API\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ Ð§ÐÐ¡Ð¢Ð˜Ð§ÐÐÐ¯ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚ÑŒ: {working_imports}/{len(critical_imports)+1}\")\n",
        "    print(\"ðŸ”§ ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ñ‹\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: Ð—Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹\")\n",
        "print(\"âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ OpenAI API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMH2dcqPpTCY",
        "outputId": "1989e64a-1847-40ab-b6b1-6bae47da4094"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Ð£Ð¡Ð¢ÐÐÐžÐ’ÐšÐ Ð—ÐÐ’Ð˜Ð¡Ð˜ÐœÐžÐ¡Ð¢Ð•Ð™ AI SEO ARCHITECTS\n",
            "==================================================\n",
            "ðŸ”§ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐ˜Ð• Ð¡ÐžÐ’ÐœÐ•Ð¡Ð¢Ð˜ÐœÐžÐ¡Ð¢Ð˜ NUMPY/PANDAS:\n",
            "âœ… NumPy 1.24.3 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½\n",
            "âœ… Pandas 2.0.3 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½\n",
            "\n",
            "ðŸ”§ Ð£Ð¡Ð¢ÐÐÐžÐ’ÐšÐ 9 ÐšÐ›Ð®Ð§Ð•Ð’Ð«Ð¥ ÐŸÐÐšÐ•Ð¢ÐžÐ’:\n",
            "â³ openai... âœ…\n",
            "â³ langgraph... âœ…\n",
            "â³ langchain... âœ…\n",
            "â³ langchain-openai... âœ…\n",
            "â³ langchain-community... âœ…\n",
            "â³ faiss-cpu... âœ…\n",
            "â³ pydantic... âœ…\n",
            "â³ nest-asyncio... âœ…\n",
            "â³ python-dotenv... âœ…\n",
            "\n",
            "ðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢: 9/9 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾\n",
            "\n",
            "ðŸ§ª ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð˜ÐœÐŸÐžÐ Ð¢ÐžÐ’:\n",
            "âœ… OpenAI ÐºÐ»Ð¸ÐµÐ½Ñ‚\n",
            "âœ… LangGraph Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€\n",
            "âœ… LangChain Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº\n",
            "âœ… FAISS Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð‘Ð”\n",
            "âœ… NumPy\n",
            "âœ… Async Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°\n",
            "âœ… Async Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð°\n",
            "\n",
            "ðŸš€ ÐžÐ¢Ð›Ð˜Ð§ÐÐž! 7/7 ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹\n",
            "âœ… Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ OpenAI API\n",
            "\n",
            "ðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: Ð—Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹\n",
            "âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ OpenAI API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”‘ Ð¯Ð§Ð•Ð™ÐšÐ 3: ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ OPENAI API Ð”Ð›Ð¯ Ð Ð•ÐÐ›Ð¬ÐÐžÐ™ Ð ÐÐ‘ÐžÐ¢Ð« ÐÐ“Ð•ÐÐ¢ÐžÐ’\n",
        "print(\"ðŸ”‘ ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ OPENAI API Ð”Ð›Ð¯ ÐÐ“Ð•ÐÐ¢ÐžÐ’\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð²ÑÐµ proxy Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÑÑ€Ð°Ð·Ñƒ\n",
        "print(\"ðŸ§¹ ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ proxy Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸...\")\n",
        "proxy_vars = ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy', 'ALL_PROXY', 'all_proxy']\n",
        "for var in proxy_vars:\n",
        "    os.environ.pop(var, None)\n",
        "print(\"âœ… Proxy Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹\")\n",
        "\n",
        "# ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ API ÐºÐ»ÑŽÑ‡\n",
        "print(\"ðŸ” ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ OpenAI API ÐºÐ»ÑŽÑ‡...\")\n",
        "api_key = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if api_key:\n",
        "        print(f\"âœ… ÐšÐ»ÑŽÑ‡ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½: {api_key[:7]}...{api_key[-8:]}\")\n",
        "        print(f\"ðŸ”¢ Ð”Ð»Ð¸Ð½Ð°: {len(api_key)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²\")\n",
        "    else:\n",
        "        raise Exception(\"ÐšÐ»ÑŽÑ‡ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}\")\n",
        "    print(\"\\nðŸ“ ÐÐÐ¡Ð¢Ð ÐžÐ™Ð¢Ð• API ÐšÐ›Ð®Ð§:\")\n",
        "    print(\"1. ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ðŸ”‘ Ð² Ð»ÐµÐ²Ð¾Ð¼ Ð¼ÐµÐ½ÑŽ Colab\")\n",
        "    print(\"2. Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ: Name = OPENAI_API_KEY\")\n",
        "    print(\"3. Value = Ð²Ð°Ñˆ ÐºÐ»ÑŽÑ‡ Ð¾Ñ‚ OpenAI\")\n",
        "    print(\"4. ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÑÑ‚Ñƒ ÑÑ‡ÐµÐ¹ÐºÑƒ\")\n",
        "    raise Exception(\"OpenAI API ÐºÐ»ÑŽÑ‡ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÐµÐ½!\")\n",
        "\n",
        "# Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡ Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ¿Ð¾ÑÐ¾Ð±Ð¾Ð² Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ\n",
        "print(\"\\nðŸ§ª Ð¢Ð•Ð¡Ð¢Ð˜Ð Ð£Ð•Ðœ OPENAI API...\")\n",
        "\n",
        "# Ð¡Ð¿Ð¾ÑÐ¾Ð± 1: Ð§ÐµÑ€ÐµÐ· requests (Ð¾Ð±Ñ…Ð¾Ð´Ð¸Ñ‚ proxy Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹)\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def test_openai_requests():\n",
        "    \"\"\"Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€ÑÐ¼Ñ‹Ðµ HTTP Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹\"\"\"\n",
        "    try:\n",
        "        url = \"https://api.openai.com/v1/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": \"ÐžÑ‚Ð²ÐµÑ‚ÑŒ: API Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚\"}],\n",
        "            \"max_tokens\": 10\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            content = result['choices'][0]['message']['content']\n",
        "            tokens = result['usage']['total_tokens']\n",
        "            return {\"success\": True, \"content\": content, \"tokens\": tokens, \"method\": \"requests\"}\n",
        "        else:\n",
        "            return {\"success\": False, \"error\": f\"HTTP {response.status_code}\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Ð¡Ð¿Ð¾ÑÐ¾Ð± 2: Ð§ÐµÑ€ÐµÐ· OpenAI library Ñ Ð¾Ð±Ñ…Ð¾Ð´Ð¾Ð¼ proxy\n",
        "def test_openai_library():\n",
        "    \"\"\"Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· OpenAI library\"\"\"\n",
        "    try:\n",
        "        import openai\n",
        "\n",
        "        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ\n",
        "        openai.api_key = api_key\n",
        "\n",
        "        # Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· Ð½Ð¾Ð²Ñ‹Ð¹ API\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"ÐžÑ‚Ð²ÐµÑ‚ÑŒ: Library Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚\"}],\n",
        "            max_tokens=10\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"content\": response.choices[0].message.content,\n",
        "            \"tokens\": response.usage.total_tokens,\n",
        "            \"method\": \"library\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ð±Ð° ÑÐ¿Ð¾ÑÐ¾Ð±Ð°\n",
        "print(\"1ï¸âƒ£ Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· requests...\")\n",
        "requests_result = test_openai_requests()\n",
        "if requests_result[\"success\"]:\n",
        "    print(f\"   âœ… Ð£ÑÐ¿ÐµÑ…! ÐžÑ‚Ð²ÐµÑ‚: '{requests_result['content']}'\")\n",
        "    print(f\"   ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: {requests_result['tokens']}\")\n",
        "else:\n",
        "    print(f\"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {requests_result['error'][:100]}\")\n",
        "\n",
        "print(\"2ï¸âƒ£ Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· OpenAI library...\")\n",
        "library_result = test_openai_library()\n",
        "if library_result[\"success\"]:\n",
        "    print(f\"   âœ… Ð£ÑÐ¿ÐµÑ…! ÐžÑ‚Ð²ÐµÑ‚: '{library_result['content']}'\")\n",
        "    print(f\"   ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: {library_result['tokens']}\")\n",
        "else:\n",
        "    print(f\"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {library_result['error'][:100]}\")\n",
        "\n",
        "# Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¼ÐµÑ‚Ð¾Ð´\n",
        "working_method = None\n",
        "if requests_result[\"success\"]:\n",
        "    working_method = \"requests\"\n",
        "    print(\"\\nðŸŽ¯ Ð’Ð«Ð‘Ð ÐÐ ÐœÐ•Ð¢ÐžÐ”: HTTP requests (Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹)\")\n",
        "elif library_result[\"success\"]:\n",
        "    working_method = \"library\"\n",
        "    print(\"\\nðŸŽ¯ Ð’Ð«Ð‘Ð ÐÐ ÐœÐ•Ð¢ÐžÐ”: OpenAI library\")\n",
        "else:\n",
        "    print(\"\\nâŒ ÐÐ˜ ÐžÐ”Ð˜Ð ÐœÐ•Ð¢ÐžÐ” ÐÐ• Ð ÐÐ‘ÐžÐ¢ÐÐ•Ð¢!\")\n",
        "    raise Exception(\"OpenAI API Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½!\")\n",
        "\n",
        "# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
        "def universal_openai_call(messages, model=\"gpt-4o-mini\", max_tokens=2000, temperature=0.3):\n",
        "    \"\"\"Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² OpenAI API Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\"\"\"\n",
        "\n",
        "    if working_method == \"requests\":\n",
        "        try:\n",
        "            url = \"https://api.openai.com/v1/chat/completions\"\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "            data = {\n",
        "                \"model\": model,\n",
        "                \"messages\": messages,\n",
        "                \"max_tokens\": max_tokens,\n",
        "                \"temperature\": temperature\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=60)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"content\": result['choices'][0]['message']['content'],\n",
        "                    \"tokens\": result['usage']['total_tokens'],\n",
        "                    \"method\": \"requests\"\n",
        "                }\n",
        "            else:\n",
        "                return {\"success\": False, \"error\": f\"HTTP {response.status_code}: {response.text}\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": f\"Requests error: {str(e)}\"}\n",
        "\n",
        "    elif working_method == \"library\":\n",
        "        try:\n",
        "            import openai\n",
        "            response = openai.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"content\": response.choices[0].message.content,\n",
        "                \"tokens\": response.usage.total_tokens,\n",
        "                \"method\": \"library\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": f\"Library error: {str(e)}\"}\n",
        "\n",
        "    else:\n",
        "        return {\"success\": False, \"error\": \"No working method available\"}\n",
        "\n",
        "# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
        "globals()['UNIVERSAL_OPENAI_CALL'] = universal_openai_call\n",
        "globals()['OPENAI_API_KEY'] = api_key\n",
        "globals()['OPENAI_WORKING_METHOD'] = working_method\n",
        "\n",
        "# Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÑÑ‚ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸\n",
        "print(\"\\nðŸ”¬ Ð¤Ð˜ÐÐÐ›Ð¬ÐÐ«Ð™ Ð¢Ð•Ð¡Ð¢ Ð£ÐÐ˜Ð’Ð•Ð Ð¡ÐÐ›Ð¬ÐÐžÐ™ Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜:\")\n",
        "test_messages = [{\"role\": \"user\", \"content\": \"Ð¡ÐºÐ°Ð¶Ð¸ 'Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð°!' Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\"}]\n",
        "final_result = universal_openai_call(test_messages, max_tokens=10)\n",
        "\n",
        "if final_result[\"success\"]:\n",
        "    print(f\"âœ… Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚: {final_result['content']}\")\n",
        "    print(f\"ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: {final_result['tokens']}\")\n",
        "    print(f\"ðŸ› ï¸ ÐœÐµÑ‚Ð¾Ð´: {final_result['method']}\")\n",
        "    print(\"\\nðŸŽ‰ OPENAI API ÐŸÐžÐ›ÐÐžÐ¡Ð¢Ð¬Ð® Ð“ÐžÐ¢ÐžÐ’ Ð”Ð›Ð¯ ÐÐ“Ð•ÐÐ¢ÐžÐ’!\")\n",
        "    print(\"âœ… Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ UNIVERSAL_OPENAI_CALL Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾\")\n",
        "else:\n",
        "    print(f\"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {final_result['error']}\")\n",
        "    raise Exception(\"Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚!\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: OpenAI API Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¸ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½\")\n",
        "print(\"âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÑŽ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_1i7Y0Vqbvz",
        "outputId": "b313659b-bf32-468a-f140-1435be576cd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”‘ ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ OPENAI API Ð”Ð›Ð¯ ÐÐ“Ð•ÐÐ¢ÐžÐ’\n",
            "========================================\n",
            "ðŸ§¹ ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ proxy Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸...\n",
            "âœ… Proxy Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹\n",
            "ðŸ” ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ OpenAI API ÐºÐ»ÑŽÑ‡...\n",
            "âœ… ÐšÐ»ÑŽÑ‡ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½: sk-proj...CeCqIqgA\n",
            "ðŸ”¢ Ð”Ð»Ð¸Ð½Ð°: 164 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²\n",
            "\n",
            "ðŸ§ª Ð¢Ð•Ð¡Ð¢Ð˜Ð Ð£Ð•Ðœ OPENAI API...\n",
            "1ï¸âƒ£ Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· requests...\n",
            "   âœ… Ð£ÑÐ¿ÐµÑ…! ÐžÑ‚Ð²ÐµÑ‚: 'Ð”Ð°, API Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚. Ð•ÑÐ»Ð¸ Ñƒ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚'\n",
            "   ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: 22\n",
            "2ï¸âƒ£ Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· OpenAI library...\n",
            "   âœ… Ð£ÑÐ¿ÐµÑ…! ÐžÑ‚Ð²ÐµÑ‚: 'Ð”Ð°, Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚. ÐšÐ°ÐºÐ¾Ð¹ Ñƒ Ð²Ð°Ñ'\n",
            "   ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: 22\n",
            "\n",
            "ðŸŽ¯ Ð’Ð«Ð‘Ð ÐÐ ÐœÐ•Ð¢ÐžÐ”: HTTP requests (Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹)\n",
            "\n",
            "ðŸ”¬ Ð¤Ð˜ÐÐÐ›Ð¬ÐÐ«Ð™ Ð¢Ð•Ð¡Ð¢ Ð£ÐÐ˜Ð’Ð•Ð Ð¡ÐÐ›Ð¬ÐÐžÐ™ Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜:\n",
            "âœ… Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚: Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð°!\n",
            "ðŸ”¢ Ð¢Ð¾ÐºÐµÐ½Ñ‹: 23\n",
            "ðŸ› ï¸ ÐœÐµÑ‚Ð¾Ð´: requests\n",
            "\n",
            "ðŸŽ‰ OPENAI API ÐŸÐžÐ›ÐÐžÐ¡Ð¢Ð¬Ð® Ð“ÐžÐ¢ÐžÐ’ Ð”Ð›Ð¯ ÐÐ“Ð•ÐÐ¢ÐžÐ’!\n",
            "âœ… Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ UNIVERSAL_OPENAI_CALL Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾\n",
            "\n",
            "ðŸŽ¯ Ð¡Ð¢ÐÐ¢Ð£Ð¡: OpenAI API Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¸ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½\n",
            "âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÑŽ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ” ÐÐÐÐ›Ð˜Ð— Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð« ÐŸÐ ÐžÐ•ÐšÐ¢Ð AI SEO ARCHITECTS\n",
        "print(\"ðŸ” ÐŸÐžÐ›ÐÐ«Ð™ ÐÐÐÐ›Ð˜Ð— Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð« ÐŸÐ ÐžÐ•ÐšÐ¢Ð\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ÐŸÑƒÑ‚ÑŒ Ðº Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ\n",
        "project_path = \"/content/ai-seo-architects\"\n",
        "\n",
        "print(\"\\nðŸ“ Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð ÐŸÐÐŸÐšÐ˜ agents/:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð²ÑÐµ ÑƒÑ€Ð¾Ð²Ð½Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
        "levels = ['executive', 'management', 'operational']\n",
        "\n",
        "for level in levels:\n",
        "    level_path = os.path.join(project_path, 'agents', level)\n",
        "    print(f\"\\nðŸ“‚ {level.upper()} LEVEL:\")\n",
        "    if os.path.exists(level_path):\n",
        "        files = [f for f in os.listdir(level_path) if f.endswith('.py') and not f.startswith('__')]\n",
        "        for file in sorted(files):\n",
        "            file_path = os.path.join(level_path, file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"   âœ… {file} ({file_size:,} bytes)\")\n",
        "\n",
        "            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ° Ð² Ñ„Ð°Ð¹Ð»Ðµ\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                if 'class ' in content:\n",
        "                    # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð¸Ð¼Ñ ÐºÐ»Ð°ÑÑÐ°\n",
        "                    for line in content.split('\\n'):\n",
        "                        if 'class ' in line and 'Agent' in line:\n",
        "                            class_name = line.split('class ')[1].split('(')[0].strip()\n",
        "                            print(f\"      â””â”€ ÐšÐ»Ð°ÑÑ: {class_name}\")\n",
        "                            break\n",
        "    else:\n",
        "        print(f\"   âŒ ÐŸÐ°Ð¿ÐºÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ ÐÐ“Ð•ÐÐ¢ÐžÐ’:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
        "total_agents = 0\n",
        "agent_list = []\n",
        "\n",
        "for level in levels:\n",
        "    level_path = os.path.join(project_path, 'agents', level)\n",
        "    if os.path.exists(level_path):\n",
        "        py_files = [f for f in os.listdir(level_path) if f.endswith('.py') and not f.startswith('__')]\n",
        "        total_agents += len(py_files)\n",
        "        for f in py_files:\n",
        "            agent_name = f.replace('.py', '').replace('_', ' ').title()\n",
        "            agent_list.append((level, agent_name, f))\n",
        "\n",
        "print(f\"ðŸ¤– Ð’Ð¡Ð•Ð“Ðž ÐÐÐ™Ð”Ð•ÐÐž ÐÐ“Ð•ÐÐ¢ÐžÐ’: {total_agents}\")\n",
        "print(\"\\nðŸ“‹ ÐŸÐžÐ›ÐÐ«Ð™ Ð¡ÐŸÐ˜Ð¡ÐžÐš ÐÐ“Ð•ÐÐ¢ÐžÐ’:\")\n",
        "for level, name, filename in agent_list:\n",
        "    print(f\"   [{level:12}] {name:40} ({filename})\")\n",
        "\n",
        "# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ knowledge Ð±Ð°Ð·Ñ‹\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“š Ð‘ÐÐ—Ð« Ð—ÐÐÐÐ˜Ð™ (knowledge/):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "knowledge_path = os.path.join(project_path, 'knowledge')\n",
        "for level in levels:\n",
        "    level_path = os.path.join(knowledge_path, level)\n",
        "    print(f\"\\nðŸ“‚ {level.upper()} KNOWLEDGE:\")\n",
        "    if os.path.exists(level_path):\n",
        "        files = [f for f in os.listdir(level_path) if f.endswith('.md')]\n",
        "        for file in sorted(files):\n",
        "            file_path = os.path.join(level_path, file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"   ðŸ“„ {file} ({file_size:,} bytes)\")\n",
        "    else:\n",
        "        print(f\"   âŒ ÐŸÐ°Ð¿ÐºÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ÐÐÐÐ›Ð˜Ð— Ð—ÐÐ’Ð•Ð Ð¨Ð•Ð\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JDYDsCaiSgV",
        "outputId": "0a276f6f-0325-4067-f68b-79e97aefcc92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” ÐŸÐžÐ›ÐÐ«Ð™ ÐÐÐÐ›Ð˜Ð— Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð« ÐŸÐ ÐžÐ•ÐšÐ¢Ð\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð ÐŸÐÐŸÐšÐ˜ agents/:\n",
            "------------------------------------------------------------\n",
            "\n",
            "ðŸ“‚ EXECUTIVE LEVEL:\n",
            "   âœ… business_development_director.py (41,370 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: BusinessDevelopmentDirectorAgent\n",
            "   âœ… chief_seo_strategist.py (53,501 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: ChiefSEOStrategistAgent\n",
            "\n",
            "ðŸ“‚ MANAGEMENT LEVEL:\n",
            "   âœ… client_success_manager.py (42,133 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: ClientSuccessManagerAgent\n",
            "   âœ… sales_operations_manager.py (54,550 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: SalesOperationsManagerAgent\n",
            "   âœ… task_coordination.py (15,453 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: TaskCoordinationAgent\n",
            "   âœ… technical_seo_operations_manager.py (51,679 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: TechnicalSEOOperationsManagerAgent\n",
            "\n",
            "ðŸ“‚ OPERATIONAL LEVEL:\n",
            "   âœ… competitive_analysis.py (70,304 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: CompetitiveAnalysisAgent\n",
            "   âœ… content_strategy.py (90,080 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: ContentStrategyAgent\n",
            "   âœ… lead_qualification.py (49,091 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: LeadQualificationAgent\n",
            "   âœ… link_building.py (50,254 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: LinkBuildingAgent\n",
            "   âœ… proposal_generation.py (46,877 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: ProposalGenerationAgent\n",
            "   âœ… reporting.py (59,504 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: ReportingAgent\n",
            "   âœ… sales_conversation.py (39,993 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: SalesConversationAgent\n",
            "   âœ… technical_seo_auditor.py (62,401 bytes)\n",
            "      â””â”€ ÐšÐ»Ð°ÑÑ: TechnicalSEOAuditorAgent\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ ÐÐ“Ð•ÐÐ¢ÐžÐ’:\n",
            "------------------------------------------------------------\n",
            "ðŸ¤– Ð’Ð¡Ð•Ð“Ðž ÐÐÐ™Ð”Ð•ÐÐž ÐÐ“Ð•ÐÐ¢ÐžÐ’: 14\n",
            "\n",
            "ðŸ“‹ ÐŸÐžÐ›ÐÐ«Ð™ Ð¡ÐŸÐ˜Ð¡ÐžÐš ÐÐ“Ð•ÐÐ¢ÐžÐ’:\n",
            "   [executive   ] Business Development Director            (business_development_director.py)\n",
            "   [executive   ] Chief Seo Strategist                     (chief_seo_strategist.py)\n",
            "   [management  ] Client Success Manager                   (client_success_manager.py)\n",
            "   [management  ] Technical Seo Operations Manager         (technical_seo_operations_manager.py)\n",
            "   [management  ] Sales Operations Manager                 (sales_operations_manager.py)\n",
            "   [management  ] Task Coordination                        (task_coordination.py)\n",
            "   [operational ] Link Building                            (link_building.py)\n",
            "   [operational ] Lead Qualification                       (lead_qualification.py)\n",
            "   [operational ] Sales Conversation                       (sales_conversation.py)\n",
            "   [operational ] Competitive Analysis                     (competitive_analysis.py)\n",
            "   [operational ] Content Strategy                         (content_strategy.py)\n",
            "   [operational ] Reporting                                (reporting.py)\n",
            "   [operational ] Proposal Generation                      (proposal_generation.py)\n",
            "   [operational ] Technical Seo Auditor                    (technical_seo_auditor.py)\n",
            "\n",
            "============================================================\n",
            "ðŸ“š Ð‘ÐÐ—Ð« Ð—ÐÐÐÐ˜Ð™ (knowledge/):\n",
            "------------------------------------------------------------\n",
            "\n",
            "ðŸ“‚ EXECUTIVE KNOWLEDGE:\n",
            "   ðŸ“„ business_development_director.md (22,769 bytes)\n",
            "   ðŸ“„ chief_seo_strategist.md (24,395 bytes)\n",
            "\n",
            "ðŸ“‚ MANAGEMENT KNOWLEDGE:\n",
            "   ðŸ“„ client_success_manager.md (32,436 bytes)\n",
            "   ðŸ“„ sales_operations_manager.md (28,950 bytes)\n",
            "   ðŸ“„ task_coordination.md (36,285 bytes)\n",
            "   ðŸ“„ technical_seo_operations_manager.md (37,957 bytes)\n",
            "\n",
            "ðŸ“‚ OPERATIONAL KNOWLEDGE:\n",
            "   ðŸ“„ competitive_analysis.md (18,152 bytes)\n",
            "   ðŸ“„ content_strategy.md (27,644 bytes)\n",
            "   ðŸ“„ lead_qualification.md (38,587 bytes)\n",
            "   ðŸ“„ link_building.md (14,064 bytes)\n",
            "   ðŸ“„ proposal_generation.md (36,072 bytes)\n",
            "   ðŸ“„ reporting.md (17,706 bytes)\n",
            "   ðŸ“„ sales_conversation.md (41,826 bytes)\n",
            "   ðŸ“„ technical_seo_auditor.md (46,280 bytes)\n",
            "\n",
            "============================================================\n",
            "âœ… ÐÐÐÐ›Ð˜Ð— Ð—ÐÐ’Ð•Ð Ð¨Ð•Ð\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§ª ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð˜ÐœÐŸÐžÐ Ð¢Ð Ð’Ð¡Ð•Ð¥ ÐÐ“Ð•ÐÐ¢ÐžÐ’\n",
        "print(\"ðŸ§ª ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð’ÐžÐ—ÐœÐžÐ–ÐÐžÐ¡Ð¢Ð˜ Ð˜ÐœÐŸÐžÐ Ð¢Ð ÐÐ“Ð•ÐÐ¢ÐžÐ’\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/ai-seo-architects')\n",
        "\n",
        "# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n",
        "agents_to_check = [\n",
        "    # Executive Level\n",
        "    ('agents.executive.chief_seo_strategist', 'ChiefSEOStrategistAgent'),\n",
        "    ('agents.executive.business_development_director', 'BusinessDevelopmentDirectorAgent'),\n",
        "\n",
        "    # Management Level\n",
        "    ('agents.management.task_coordination', 'TaskCoordinationAgent'),\n",
        "    ('agents.management.sales_operations_manager', 'SalesOperationsManagerAgent'),\n",
        "    ('agents.management.technical_seo_operations_manager', 'TechnicalSEOOperationsManagerAgent'),\n",
        "    ('agents.management.client_success_manager', 'ClientSuccessManagerAgent'),\n",
        "\n",
        "    # Operational Level\n",
        "    ('agents.operational.lead_qualification', 'LeadQualificationAgent'),\n",
        "    ('agents.operational.sales_conversation', 'SalesConversationAgent'),\n",
        "    ('agents.operational.proposal_generation', 'ProposalGenerationAgent'),\n",
        "    ('agents.operational.technical_seo_auditor', 'TechnicalSEOAuditorAgent'),\n",
        "    ('agents.operational.content_strategy', 'ContentStrategyAgent'),\n",
        "    ('agents.operational.link_building', 'LinkBuildingAgent'),\n",
        "    ('agents.operational.competitive_analysis', 'CompetitiveAnalysisAgent'),\n",
        "    ('agents.operational.reporting', 'ReportingAgent'),\n",
        "]\n",
        "\n",
        "successful_imports = []\n",
        "failed_imports = []\n",
        "\n",
        "for module_path, class_name in agents_to_check:\n",
        "    try:\n",
        "        module = __import__(module_path, fromlist=[class_name])\n",
        "        if hasattr(module, class_name):\n",
        "            successful_imports.append((module_path, class_name))\n",
        "            print(f\"âœ… {class_name:45} - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\")\n",
        "        else:\n",
        "            failed_imports.append((module_path, class_name, \"ÐšÐ»Ð°ÑÑ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½\"))\n",
        "            print(f\"âŒ {class_name:45} - ÐšÐ»Ð°ÑÑ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² Ð¼Ð¾Ð´ÑƒÐ»Ðµ\")\n",
        "    except ImportError as e:\n",
        "        failed_imports.append((module_path, class_name, str(e)))\n",
        "        print(f\"âŒ {class_name:45} - ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½\")\n",
        "    except Exception as e:\n",
        "        failed_imports.append((module_path, class_name, str(e)))\n",
        "        print(f\"âš ï¸ {class_name:45} - ÐžÑˆÐ¸Ð±ÐºÐ°: {str(e)[:50]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"ðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð«:\")\n",
        "print(f\"   âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: {len(successful_imports)} Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\")\n",
        "print(f\"   âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ: {len(failed_imports)} Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\")\n",
        "print(f\"   ðŸ“Š ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸: {len(successful_imports)/14*100:.1f}%\")\n",
        "\n",
        "if successful_imports:\n",
        "    print(\"\\nâœ… Ð“ÐžÐ¢ÐžÐ’Ð«Ð• Ðš Ð ÐÐ‘ÐžÐ¢Ð• ÐÐ“Ð•ÐÐ¢Ð«:\")\n",
        "    for module, cls in successful_imports:\n",
        "        print(f\"   â€¢ {cls}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eWDi1eZiZ5K",
        "outputId": "da29905d-1441-48bc-d5b8-b905009a01a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð’ÐžÐ—ÐœÐžÐ–ÐÐžÐ¡Ð¢Ð˜ Ð˜ÐœÐŸÐžÐ Ð¢Ð ÐÐ“Ð•ÐÐ¢ÐžÐ’\n",
            "============================================================\n",
            "âœ… ChiefSEOStrategistAgent                       - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… BusinessDevelopmentDirectorAgent              - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… TaskCoordinationAgent                         - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… SalesOperationsManagerAgent                   - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… TechnicalSEOOperationsManagerAgent            - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… ClientSuccessManagerAgent                     - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… LeadQualificationAgent                        - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… SalesConversationAgent                        - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… ProposalGenerationAgent                       - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… TechnicalSEOAuditorAgent                      - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… ContentStrategyAgent                          - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… LinkBuildingAgent                             - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… CompetitiveAnalysisAgent                      - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "âœ… ReportingAgent                                - Ð˜ÐœÐŸÐžÐ Ð¢Ð˜Ð ÐžÐ’ÐÐ\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð«:\n",
            "   âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: 14 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
            "   âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ: 0 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
            "   ðŸ“Š ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸: 100.0%\n",
            "\n",
            "âœ… Ð“ÐžÐ¢ÐžÐ’Ð«Ð• Ðš Ð ÐÐ‘ÐžÐ¢Ð• ÐÐ“Ð•ÐÐ¢Ð«:\n",
            "   â€¢ ChiefSEOStrategistAgent\n",
            "   â€¢ BusinessDevelopmentDirectorAgent\n",
            "   â€¢ TaskCoordinationAgent\n",
            "   â€¢ SalesOperationsManagerAgent\n",
            "   â€¢ TechnicalSEOOperationsManagerAgent\n",
            "   â€¢ ClientSuccessManagerAgent\n",
            "   â€¢ LeadQualificationAgent\n",
            "   â€¢ SalesConversationAgent\n",
            "   â€¢ ProposalGenerationAgent\n",
            "   â€¢ TechnicalSEOAuditorAgent\n",
            "   â€¢ ContentStrategyAgent\n",
            "   â€¢ LinkBuildingAgent\n",
            "   â€¢ CompetitiveAnalysisAgent\n",
            "   â€¢ ReportingAgent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ Ð¯Ð§Ð•Ð™ÐšÐ 6: ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð’Ð¡Ð•Ð¥ Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™ Ð”Ð›Ð¯ Ð”Ð•ÐœÐžÐÐ¡Ð¢Ð ÐÐ¦Ð˜Ð˜\n",
        "print(\"ðŸš€ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð’Ð¡Ð•Ð¥ Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™ AI SEO ARCHITECTS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð»Ñ Ð²ÑÐµÑ… 14 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¼Ð¸Ñ€Ð¾Ð²Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ°\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n",
        "# ÐŸÑƒÑ‚ÑŒ Ðº Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ\n",
        "project_path = \"/content/ai-seo-architects\"\n",
        "\n",
        "# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð°Ð¿ÐºÑƒ Ð´Ð»Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð±Ð°Ð·\n",
        "vector_store_path = f\"{project_path}/data/vector_stores\"\n",
        "os.makedirs(vector_store_path, exist_ok=True)\n",
        "\n",
        "print(f\"ðŸ“ Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð±Ð°Ð·Ñ‹ Ð±ÑƒÐ´ÑƒÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {vector_store_path}\")\n",
        "\n",
        "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· OpenAI\n",
        "def create_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð° Ñ‡ÐµÑ€ÐµÐ· OpenAI API\"\"\"\n",
        "    try:\n",
        "        import requests\n",
        "\n",
        "        api_key = globals().get('OPENAI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise Exception(\"OpenAI API ÐºÐ»ÑŽÑ‡ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½\")\n",
        "\n",
        "        url = \"https://api.openai.com/v1/embeddings\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"model\": model,\n",
        "            \"input\": text\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return result['data'][0]['embedding']\n",
        "        else:\n",
        "            raise Exception(f\"API Error: {response.status_code}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸\n",
        "def split_text_into_chunks(text, chunk_size=2000, overlap=200):\n",
        "    \"\"\"Ð Ð°Ð·Ð±Ð¸Ð²ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ñ‡Ð°Ð½ÐºÐ¸\"\"\"\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "        if len(chunk_text.strip()) > 100:  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ‡Ð°Ð½ÐºÐ°\n",
        "            chunks.append({\n",
        "                'text': chunk_text,\n",
        "                'start_word': i,\n",
        "                'end_word': min(i + chunk_size, len(words)),\n",
        "                'word_count': len(chunk_words)\n",
        "            })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÑÐµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹\n",
        "print(\"\\nðŸ“Š ÐÐÐÐ›Ð˜Ð— Ð’Ð¡Ð•Ð¥ Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "knowledge_base_path = f\"{project_path}/knowledge\"\n",
        "all_knowledge_files = []\n",
        "\n",
        "levels = ['executive', 'management', 'operational']\n",
        "for level in levels:\n",
        "    level_path = os.path.join(knowledge_base_path, level)\n",
        "    if os.path.exists(level_path):\n",
        "        md_files = glob.glob(f\"{level_path}/*.md\")\n",
        "        for file_path in md_files:\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            file_name = os.path.basename(file_path)\n",
        "            agent_name = file_name.replace('.md', '')\n",
        "\n",
        "            all_knowledge_files.append({\n",
        "                'level': level,\n",
        "                'agent_name': agent_name,\n",
        "                'file_path': file_path,\n",
        "                'file_name': file_name,\n",
        "                'size_bytes': file_size,\n",
        "                'size_kb': file_size / 1024\n",
        "            })\n",
        "\n",
        "            print(f\"ðŸ“„ {level:12} | {agent_name:35} | {file_size/1024:.1f} KB\")\n",
        "\n",
        "total_size_kb = sum([f['size_kb'] for f in all_knowledge_files])\n",
        "print(f\"\\nðŸ“Š Ð’Ð¡Ð•Ð“Ðž ÐÐÐ™Ð”Ð•ÐÐž: {len(all_knowledge_files)} Ð±Ð°Ð· Ð·Ð½Ð°Ð½Ð¸Ð¹\")\n",
        "print(f\"ðŸ“¦ ÐžÐ‘Ð©Ð˜Ð™ Ð ÐÐ—ÐœÐ•Ð : {total_size_kb:.1f} KB ({total_size_kb/1024:.1f} MB)\")\n",
        "\n",
        "# ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ\n",
        "print(f\"\\nðŸ”„ ÐÐÐ§Ð˜ÐÐÐ•Ðœ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð® {len(all_knowledge_files)} Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "vectorization_results = []\n",
        "total_chunks = 0\n",
        "total_embeddings = 0\n",
        "\n",
        "for i, kb_file in enumerate(all_knowledge_files, 1):\n",
        "    print(f\"\\n{i:2d}/14 ðŸ”„ {kb_file['agent_name']:35} ({kb_file['size_kb']:.1f} KB)\")\n",
        "\n",
        "    try:\n",
        "        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð»\n",
        "        with open(kb_file['file_path'], 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
        "        file_hash = hashlib.md5(content.encode()).hexdigest()[:8]\n",
        "\n",
        "        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸\n",
        "        chunks = split_text_into_chunks(content, chunk_size=1000, overlap=150)\n",
        "        print(f\"      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {len(chunks)} Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\")\n",
        "\n",
        "        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‡Ð°Ð½ÐºÐ°\n",
        "        chunk_embeddings = []\n",
        "        success_count = 0\n",
        "\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ðº Ñ‡Ð°Ð½ÐºÑƒ\n",
        "            contextualized_text = f\"\"\"\n",
        "ÐÐ³ÐµÐ½Ñ‚: {kb_file['agent_name']}\n",
        "Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ: {kb_file['level']}\n",
        "Ð Ð°Ð·Ð´ÐµÐ» {j+1}/{len(chunks)}:\n",
        "\n",
        "{chunk['text']}\n",
        "\"\"\"\n",
        "\n",
        "            embedding = create_embedding(contextualized_text.strip())\n",
        "            if embedding:\n",
        "                chunk_embeddings.append({\n",
        "                    'chunk_id': j,\n",
        "                    'text': chunk['text'][:500] + '...' if len(chunk['text']) > 500 else chunk['text'],\n",
        "                    'full_text': chunk['text'],\n",
        "                    'embedding': embedding,\n",
        "                    'word_count': chunk['word_count'],\n",
        "                    'metadata': {\n",
        "                        'agent': kb_file['agent_name'],\n",
        "                        'level': kb_file['level'],\n",
        "                        'chunk_index': j,\n",
        "                        'total_chunks': len(chunks),\n",
        "                        'file_hash': file_hash\n",
        "                    }\n",
        "                })\n",
        "                success_count += 1\n",
        "\n",
        "            # ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²\n",
        "            if len(chunks) > 10 and (j + 1) % 5 == 0:\n",
        "                print(f\"         ðŸ“Š ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: {j+1}/{len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð²\")\n",
        "\n",
        "        print(f\"      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {success_count}/{len(chunks)} ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\")\n",
        "\n",
        "        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°\n",
        "        vector_file_path = f\"{vector_store_path}/{kb_file['agent_name']}_vectors.json\"\n",
        "\n",
        "        vector_data = {\n",
        "            'agent_name': kb_file['agent_name'],\n",
        "            'level': kb_file['level'],\n",
        "            'file_hash': file_hash,\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'total_chunks': len(chunks),\n",
        "            'successful_embeddings': success_count,\n",
        "            'embedding_model': 'text-embedding-ada-002',\n",
        "            'embedding_dimension': 1536,\n",
        "            'chunks': chunk_embeddings\n",
        "        }\n",
        "\n",
        "        with open(vector_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vector_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        file_size_mb = os.path.getsize(vector_file_path) / (1024 * 1024)\n",
        "        print(f\"      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: {os.path.basename(vector_file_path)} ({file_size_mb:.2f} MB)\")\n",
        "\n",
        "        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚\n",
        "        vectorization_results.append({\n",
        "            'agent_name': kb_file['agent_name'],\n",
        "            'level': kb_file['level'],\n",
        "            'original_size_kb': kb_file['size_kb'],\n",
        "            'chunks_created': len(chunks),\n",
        "            'embeddings_created': success_count,\n",
        "            'vector_file_size_mb': file_size_mb,\n",
        "            'success_rate': success_count / len(chunks) * 100 if chunks else 0\n",
        "        })\n",
        "\n",
        "        total_chunks += len(chunks)\n",
        "        total_embeddings += success_count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"      âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ: {str(e)}\")\n",
        "        vectorization_results.append({\n",
        "            'agent_name': kb_file['agent_name'],\n",
        "            'level': kb_file['level'],\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ¯ Ð˜Ð¢ÐžÐ“Ð˜ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð˜:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "successful_agents = [r for r in vectorization_results if 'error' not in r]\n",
        "failed_agents = [r for r in vectorization_results if 'error' in r]\n",
        "\n",
        "print(f\"âœ… Ð£Ð¡ÐŸÐ•Ð¨ÐÐž Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐžÐ’ÐÐÐž: {len(successful_agents)}/14 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\")\n",
        "print(f\"âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ˜: {len(failed_agents)} Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\")\n",
        "print(f\"ðŸ“¦ Ð’Ð¡Ð•Ð“Ðž Ð§ÐÐÐšÐžÐ’: {total_chunks:,}\")\n",
        "print(f\"ðŸ§  Ð’Ð¡Ð•Ð“Ðž Ð­ÐœÐ‘Ð•Ð”Ð”Ð˜ÐÐ“ÐžÐ’: {total_embeddings:,}\")\n",
        "\n",
        "if successful_agents:\n",
        "    total_vector_size = sum([r['vector_file_size_mb'] for r in successful_agents])\n",
        "    avg_success_rate = sum([r['success_rate'] for r in successful_agents]) / len(successful_agents)\n",
        "\n",
        "    print(f\"ðŸ’¾ Ð ÐÐ—ÐœÐ•Ð  Ð’Ð•ÐšÐ¢ÐžÐ ÐÐ«Ð¥ Ð‘ÐÐ—: {total_vector_size:.1f} MB\")\n",
        "    print(f\"ðŸ“Š Ð¡Ð Ð•Ð”ÐÐ˜Ð™ SUCCESS RATE: {avg_success_rate:.1f}%\")\n",
        "\n",
        "print(f\"\\nðŸ“ Ð’Ð¡Ð• Ð’Ð•ÐšÐ¢ÐžÐ ÐÐ«Ð• Ð‘ÐÐ—Ð« Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ« Ð’: {vector_store_path}\")\n",
        "\n",
        "# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ²Ð¾Ð´Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸\n",
        "summary_data = {\n",
        "    'project': 'AI SEO Architects',\n",
        "    'vectorization_date': datetime.now().isoformat(),\n",
        "    'total_agents': 14,\n",
        "    'successful_vectorizations': len(successful_agents),\n",
        "    'total_chunks': total_chunks,\n",
        "    'total_embeddings': total_embeddings,\n",
        "    'embedding_model': 'text-embedding-ada-002',\n",
        "    'embedding_dimension': 1536,\n",
        "    'agents': vectorization_results\n",
        "}\n",
        "\n",
        "summary_path = f\"{vector_store_path}/vectorization_summary.json\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"ðŸ“‹ Ð¡Ð’ÐžÐ”ÐšÐ Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ: {os.path.basename(summary_path)}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ!\")\n",
        "print(\"âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‡ÐµÐ¹ÐºÐµ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ RAG Ð¿Ð¾Ð¸ÑÐºÐ°\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7ue8Bs6ndbi",
        "outputId": "8b54ab11-486f-4076-fecc-b7003bb53fde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð’Ð¡Ð•Ð¥ Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™ AI SEO ARCHITECTS\n",
            "======================================================================\n",
            "Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð»Ñ Ð²ÑÐµÑ… 14 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¼Ð¸Ñ€Ð¾Ð²Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ°\n",
            "======================================================================\n",
            "ðŸ“ Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð±Ð°Ð·Ñ‹ Ð±ÑƒÐ´ÑƒÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: /content/ai-seo-architects/data/vector_stores\n",
            "\n",
            "ðŸ“Š ÐÐÐÐ›Ð˜Ð— Ð’Ð¡Ð•Ð¥ Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™:\n",
            "----------------------------------------------------------------------\n",
            "ðŸ“„ executive    | chief_seo_strategist                | 23.8 KB\n",
            "ðŸ“„ executive    | business_development_director       | 22.2 KB\n",
            "ðŸ“„ management   | client_success_manager              | 31.7 KB\n",
            "ðŸ“„ management   | sales_operations_manager            | 28.3 KB\n",
            "ðŸ“„ management   | task_coordination                   | 35.4 KB\n",
            "ðŸ“„ management   | technical_seo_operations_manager    | 37.1 KB\n",
            "ðŸ“„ operational  | proposal_generation                 | 35.2 KB\n",
            "ðŸ“„ operational  | link_building                       | 13.7 KB\n",
            "ðŸ“„ operational  | reporting                           | 17.3 KB\n",
            "ðŸ“„ operational  | competitive_analysis                | 17.7 KB\n",
            "ðŸ“„ operational  | content_strategy                    | 27.0 KB\n",
            "ðŸ“„ operational  | lead_qualification                  | 37.7 KB\n",
            "ðŸ“„ operational  | sales_conversation                  | 40.8 KB\n",
            "ðŸ“„ operational  | technical_seo_auditor               | 45.2 KB\n",
            "\n",
            "ðŸ“Š Ð’Ð¡Ð•Ð“Ðž ÐÐÐ™Ð”Ð•ÐÐž: 14 Ð±Ð°Ð· Ð·Ð½Ð°Ð½Ð¸Ð¹\n",
            "ðŸ“¦ ÐžÐ‘Ð©Ð˜Ð™ Ð ÐÐ—ÐœÐ•Ð : 413.2 KB (0.4 MB)\n",
            "\n",
            "ðŸ”„ ÐÐÐ§Ð˜ÐÐÐ•Ðœ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð® 14 Ð‘ÐÐ— Ð—ÐÐÐÐ˜Ð™...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " 1/14 ðŸ”„ chief_seo_strategist                (23.8 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: chief_seo_strategist_vectors.json (0.12 MB)\n",
            "\n",
            " 2/14 ðŸ”„ business_development_director       (22.2 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: business_development_director_vectors.json (0.12 MB)\n",
            "\n",
            " 3/14 ðŸ”„ client_success_manager              (31.7 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: client_success_manager_vectors.json (0.13 MB)\n",
            "\n",
            " 4/14 ðŸ”„ sales_operations_manager            (28.3 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: sales_operations_manager_vectors.json (0.13 MB)\n",
            "\n",
            " 5/14 ðŸ”„ task_coordination                   (35.4 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: task_coordination_vectors.json (0.14 MB)\n",
            "\n",
            " 6/14 ðŸ”„ technical_seo_operations_manager    (37.1 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5/5 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: technical_seo_operations_manager_vectors.json (0.20 MB)\n",
            "\n",
            " 7/14 ðŸ”„ proposal_generation                 (35.2 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5/5 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: proposal_generation_vectors.json (0.20 MB)\n",
            "\n",
            " 8/14 ðŸ”„ link_building                       (13.7 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2/2 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: link_building_vectors.json (0.08 MB)\n",
            "\n",
            " 9/14 ðŸ”„ reporting                           (17.3 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2/2 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: reporting_vectors.json (0.08 MB)\n",
            "\n",
            "10/14 ðŸ”„ competitive_analysis                (17.7 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 2/2 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: competitive_analysis_vectors.json (0.08 MB)\n",
            "\n",
            "11/14 ðŸ”„ content_strategy                    (27.0 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 3/3 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: content_strategy_vectors.json (0.13 MB)\n",
            "\n",
            "12/14 ðŸ”„ lead_qualification                  (37.7 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5/5 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: lead_qualification_vectors.json (0.21 MB)\n",
            "\n",
            "13/14 ðŸ”„ sales_conversation                  (40.8 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5/5 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: sales_conversation_vectors.json (0.21 MB)\n",
            "\n",
            "14/14 ðŸ”„ technical_seo_auditor               (45.2 KB)\n",
            "      ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²\n",
            "      âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ 5/5 ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²\n",
            "      ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: technical_seo_auditor_vectors.json (0.21 MB)\n",
            "\n",
            "======================================================================\n",
            "ðŸŽ¯ Ð˜Ð¢ÐžÐ“Ð˜ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð˜:\n",
            "======================================================================\n",
            "âœ… Ð£Ð¡ÐŸÐ•Ð¨ÐÐž Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐžÐ’ÐÐÐž: 14/14 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
            "âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ˜: 0 Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²\n",
            "ðŸ“¦ Ð’Ð¡Ð•Ð“Ðž Ð§ÐÐÐšÐžÐ’: 49\n",
            "ðŸ§  Ð’Ð¡Ð•Ð“Ðž Ð­ÐœÐ‘Ð•Ð”Ð”Ð˜ÐÐ“ÐžÐ’: 49\n",
            "ðŸ’¾ Ð ÐÐ—ÐœÐ•Ð  Ð’Ð•ÐšÐ¢ÐžÐ ÐÐ«Ð¥ Ð‘ÐÐ—: 2.1 MB\n",
            "ðŸ“Š Ð¡Ð Ð•Ð”ÐÐ˜Ð™ SUCCESS RATE: 100.0%\n",
            "\n",
            "ðŸ“ Ð’Ð¡Ð• Ð’Ð•ÐšÐ¢ÐžÐ ÐÐ«Ð• Ð‘ÐÐ—Ð« Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ« Ð’: /content/ai-seo-architects/data/vector_stores\n",
            "ðŸ“‹ Ð¡Ð’ÐžÐ”ÐšÐ Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ: vectorization_summary.json\n",
            "\n",
            "ðŸŽ‰ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ Ð’Ð•ÐšÐ¢ÐžÐ Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ!\n",
            "âž¡ï¸ ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‡ÐµÐ¹ÐºÐµ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ RAG Ð¿Ð¾Ð¸ÑÐºÐ°\n"
          ]
        }
      ]
    }
  ]
}