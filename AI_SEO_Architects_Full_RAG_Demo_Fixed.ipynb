{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AI SEO Architects - –ü–æ–ª–Ω–∞—è RAG –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n",
    "\n",
    "## üéØ –û–ø–∏—Å–∞–Ω–∏–µ\n",
    "\n",
    "**–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è AI SEO Architects** —Å 14 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏, –ø–æ–ª–Ω–æ–π RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π FAISS –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ OpenAI.\n",
    "\n",
    "### üî• –ß—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º:\n",
    "- **14 –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤** —Å production-ready –ø—Ä–æ–º–ø—Ç–∞–º–∏\n",
    "- **–ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞** —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π\n",
    "- **FAISS –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "- **OpenAI embeddings** –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "- **–†–µ–∞–ª—å–Ω—ã–µ OpenAI API** –≤—ã–∑–æ–≤—ã (GPT-4o/GPT-4o-mini)\n",
    "- **–ó–Ω–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤** –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑\n",
    "- **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã** –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG\n",
    "\n",
    "### üèóÔ∏è RAG –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "```\n",
    "üìä Query ‚Üí üîç Embedding ‚Üí üéØ Vector Search ‚Üí üìö Knowledge Retrieval ‚Üí ü§ñ LLM + Context ‚Üí ‚úÖ Response\n",
    "```\n",
    "\n",
    "### üîë –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
    "1. **OpenAI API –∫–ª—é—á** - –¥–æ–±–∞–≤—å—Ç–µ –≤ —Å–µ–∫—Ä–µ—Ç—ã Colab –∫–∞–∫ `OPENAI_API_KEY`\n",
    "2. **Google Colab** - –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å GPU\n",
    "3. **–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç** - –¥–ª—è embeddings –∏ API –≤—ã–∑–æ–≤–æ–≤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è Google Colab\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ AI SEO ARCHITECTS - –ü–û–õ–ù–ê–Ø RAG –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø (–ò–°–ü–†–ê–í–õ–ï–ù–û)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÖ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python –≤–µ—Ä—Å–∏—è: {sys.version.split()[0]}\")\n",
    "print(f\"üíª –°—Ä–µ–¥–∞: {'Google Colab' if 'google.colab' in sys.modules else 'Jupyter'}\")\n",
    "\n",
    "def safe_install(package, description=\"\"):\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\"\"\"\n",
    "    try:\n",
    "        print(f\"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º {package}... {description}\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(f\"‚úÖ {package} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –æ–±–Ω–æ–≤–ª—è–µ–º numpy –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏\n",
    "print(\"\\nüîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ö–û–ù–§–õ–ò–ö–¢–û–í –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# –°–Ω–∞—á–∞–ª–∞ –æ–±–Ω–æ–≤–ª—è–µ–º numpy –¥–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏\n",
    "compatibility_packages = [\n",
    "    ('numpy==1.24.3', '–°–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è NumPy –¥–ª—è FAISS'),\n",
    "    ('setuptools', '–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ setuptools'),\n",
    "]\n",
    "\n",
    "for package, description in compatibility_packages:\n",
    "    safe_install(package, description)\n",
    "\n",
    "# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —è–¥—Ä–æ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π numpy\n",
    "print(\"\\n‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ numpy –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ runtime!\")\n",
    "print(\"Runtime ‚Üí Restart runtime, –∑–∞—Ç–µ–º –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ\")\n",
    "\n",
    "# RAG –∏ AI –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏)\n",
    "rag_packages = [\n",
    "    ('openai==1.54.3', 'OpenAI API –¥–ª—è LLM –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤'),\n",
    "    ('faiss-cpu==1.7.4', 'FAISS —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è'),\n",
    "    ('pydantic==2.5.0', '–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö'),\n",
    "    ('nest-asyncio>=1.5.0', 'Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ Jupyter'),\n",
    "    ('tiktoken', '–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è OpenAI'),\n",
    "    ('tqdm', 'Progress bars –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π'),\n",
    "    ('scikit-learn', 'ML —É—Ç–∏–ª–∏—Ç—ã (–±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)')\n",
    "]\n",
    "\n",
    "print(\"\\nüì¶ –£–°–¢–ê–ù–û–í–ö–ê RAG –ö–û–ú–ü–û–ù–ï–ù–¢–û–í (–°–û–í–ú–ï–°–¢–ò–ú–´–ï –í–ï–†–°–ò–ò)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "success_count = 0\n",
    "for package, description in rag_packages:\n",
    "    if safe_install(package, description):\n",
    "        success_count += 1\n",
    "\n",
    "print(f\"\\nüìä –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ: {success_count}/{len(rag_packages)}\")\n",
    "\n",
    "# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)\n",
    "optional_packages = [\n",
    "    ('python-dotenv', 'Environment –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ'),\n",
    "    ('matplotlib', '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫')\n",
    "]\n",
    "\n",
    "print(\"\\nüì¶ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for package, description in optional_packages:\n",
    "    safe_install(package, description)\n",
    "\n",
    "print(\"\\nüéâ –°—Ä–µ–¥–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π RAG –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏!\")\n",
    "print(\"‚ö†Ô∏è –ï—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏ —Å numpy/FAISS - –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ runtime –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º embeddings\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def setup_openai_comprehensive() -> Tuple[bool, Optional[str], bool]:\n",
    "    \"\"\"–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π LLM –∏ embeddings\"\"\"\n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            api_key = userdata.get('OPENAI_API_KEY')\n",
    "            print(\"‚úÖ OpenAI API –∫–ª—é—á –ø–æ–ª—É—á–µ–Ω –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Google Colab\")\n",
    "        except Exception:\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                print(\"‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "                print(\"üí° –î–æ–±–∞–≤—å—Ç–µ –∫–ª—é—á –≤ —Å–µ–∫—Ä–µ—Ç—ã Colab (üîë –≤ –ª–µ–≤–æ–º –º–µ–Ω—é): OPENAI_API_KEY\")\n",
    "                return False, \"API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω\", False\n",
    "\n",
    "        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–ª—é—á–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞\n",
    "        os.environ['OPENAI_API_KEY'] = api_key\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "        # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        print(\"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\")\n",
    "        llm_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: OK\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        llm_test = \"OK\" in llm_response.choices[0].message.content\n",
    "        print(f\"‚úÖ LLM —Ç–µ—Å—Ç: {'–ü—Ä–æ–π–¥–µ–Ω' if llm_test else '–ù–µ –ø—Ä–æ–π–¥–µ–Ω'}\")\n",
    "\n",
    "        # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ embeddings —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        print(\"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\")\n",
    "        embedding_response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=\"–¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã\"\n",
    "        )\n",
    "        embedding_vector = embedding_response.data[0].embedding\n",
    "        embedding_test = len(embedding_vector) > 0\n",
    "        print(f\"‚úÖ Embeddings —Ç–µ—Å—Ç: {'–ü—Ä–æ–π–¥–µ–Ω' if embedding_test else '–ù–µ –ø—Ä–æ–π–¥–µ–Ω'}\")\n",
    "        print(f\"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {len(embedding_vector)}\")\n",
    "\n",
    "        # –¢–µ—Å—Ç 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "        models_to_test = [\"gpt-4o-mini\", \"gpt-4o\"]\n",
    "        available_models = []\n",
    "\n",
    "        for model in models_to_test:\n",
    "            try:\n",
    "                test_response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
    "                    max_tokens=5\n",
    "                )\n",
    "                available_models.append(model)\n",
    "                print(f\"‚úÖ –ú–æ–¥–µ–ª—å {model}: –î–æ—Å—Ç—É–ø–Ω–∞\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model}: –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ({str(e)[:50]}...)\")\n",
    "\n",
    "        success = llm_test and embedding_test and len(available_models) > 0\n",
    "\n",
    "        if success:\n",
    "            print(f\"\\nüéâ OpenAI API –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω!\")\n",
    "            print(f\"ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(available_models)}\")\n",
    "            print(f\"üîç Embeddings: text-embedding-3-small –≥–æ—Ç–æ–≤\")\n",
    "            return True, None, True\n",
    "        else:\n",
    "            return False, \"–¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã\", False\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ OpenAI API: {error_msg}\")\n",
    "        return False, error_msg, False\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "print(\"üîë –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê OPENAI API\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "api_ready, error, embeddings_ready = setup_openai_comprehensive()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö —è—á–µ–π–∫–∞—Ö\n",
    "globals()['OPENAI_READY'] = api_ready\n",
    "globals()['EMBEDDINGS_READY'] = embeddings_ready\n",
    "globals()['OPENAI_CLIENT'] = openai.OpenAI() if api_ready else None\n",
    "\n",
    "print(f\"\\nüìä –ò–¢–û–ì–û–í–´–ô –°–¢–ê–¢–£–°:\")\n",
    "print(f\"ü§ñ LLM –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {'‚úÖ' if api_ready else '‚ùå'}\")\n",
    "print(f\"üîç Embeddings –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {'‚úÖ' if embeddings_ready else '‚ùå'}\")\n",
    "print(f\"üöÄ RAG —Å–∏—Å—Ç–µ–º–∞: {'‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤–∞' if api_ready and embeddings_ready else 'üîÑ –ë—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ demo —Ä–µ–∂–∏–º–µ'}\")\n",
    "\n",
    "if not api_ready:\n",
    "    print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞: {error}\")\n",
    "    print(\"üí° –°–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö API –≤—ã–∑–æ–≤–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ RAG –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ RAG –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ FAISS\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º async –ø–æ–¥–¥–µ—Ä–∂–∫—É\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è nest_asyncio –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–æ async –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å\")\n",
    "\n",
    "# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç numpy –∏ FAISS\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ NumPy {np.__version__} –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ\")\n",
    "    NUMPY_READY = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ NumPy: {e}\")\n",
    "    NUMPY_READY = False\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    print(f\"‚úÖ FAISS –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ\")\n",
    "    FAISS_READY = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ FAISS: {e}\")\n",
    "    print(\"üîÑ –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é —Å–∏–º—É–ª—è—Ü–∏—é\")\n",
    "    FAISS_READY = False\n",
    "\n",
    "@dataclass\n",
    "class KnowledgeChunk:\n",
    "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—É—Å–æ—á–∫–∞ –∑–Ω–∞–Ω–∏–π\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: Optional[List[float]] = None\n",
    "\n",
    "class MockFAISSIndex:\n",
    "    \"\"\"–ò–º–∏—Ç–∞—Ü–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ FAISS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.vectors = []\n",
    "        self.ntotal = 0\n",
    "    \n",
    "    def add(self, vector):\n",
    "        if NUMPY_READY:\n",
    "            self.vectors.append(vector.flatten())\n",
    "        else:\n",
    "            self.vectors.append(vector)\n",
    "        self.ntotal += 1\n",
    "    \n",
    "    def search(self, query_vector, k):\n",
    "        if not self.vectors:\n",
    "            if NUMPY_READY:\n",
    "                return np.array([[1.0]]), np.array([[0]])\n",
    "            else:\n",
    "                return [[1.0]], [[0]]\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –∏–º–∏—Ç–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞\n",
    "        num_results = min(k, len(self.vectors))\n",
    "        \n",
    "        if NUMPY_READY:\n",
    "            distances = np.random.random((1, num_results))\n",
    "            indices = np.array([list(range(num_results))])\n",
    "        else:\n",
    "            distances = [[0.1 * i for i in range(num_results)]]\n",
    "            indices = [[i for i in range(num_results)]]\n",
    "        \n",
    "        return distances, indices\n",
    "\n",
    "class RAGVectorStore:\n",
    "    \"\"\"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è RAG —Å FAISS –∏–ª–∏ Mock\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id: str, embedding_dim: int = 1536):\n",
    "        self.agent_id = agent_id\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.chunks: List[KnowledgeChunk] = []\n",
    "        self.metadata_store: Dict[int, Dict[str, Any]] = {}\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å (FAISS –∏–ª–∏ Mock)\n",
    "        if FAISS_READY:\n",
    "            self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "            self.index_type = \"FAISS\"\n",
    "        else:\n",
    "            self.index = MockFAISSIndex(embedding_dim)\n",
    "            self.index_type = \"Mock\"\n",
    "\n",
    "    async def add_knowledge(self, content: str, metadata: Dict[str, Any] = None):\n",
    "        \"\"\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\"\"\"\n",
    "        if not EMBEDDINGS_READY:\n",
    "            # Demo —Ä–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö embeddings\n",
    "            if NUMPY_READY:\n",
    "                fake_embedding = np.random.random(self.embedding_dim).astype('float32')\n",
    "            else:\n",
    "                fake_embedding = [0.1] * self.embedding_dim\n",
    "            chunk = KnowledgeChunk(content=content, metadata=metadata or {}, embedding=fake_embedding)\n",
    "        else:\n",
    "            # –†–µ–∞–ª—å–Ω—ã–µ embeddings –æ—Ç OpenAI\n",
    "            try:\n",
    "                response = OPENAI_CLIENT.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=content\n",
    "                )\n",
    "                if NUMPY_READY:\n",
    "                    embedding = np.array(response.data[0].embedding, dtype='float32')\n",
    "                else:\n",
    "                    embedding = response.data[0].embedding\n",
    "                chunk = KnowledgeChunk(content=content, metadata=metadata or {}, embedding=embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ embedding –¥–ª—è {self.agent_id}: {e}\")\n",
    "                if NUMPY_READY:\n",
    "                    fake_embedding = np.random.random(self.embedding_dim).astype('float32')\n",
    "                else:\n",
    "                    fake_embedding = [0.1] * self.embedding_dim\n",
    "                chunk = KnowledgeChunk(content=content, metadata=metadata or {}, embedding=fake_embedding)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å\n",
    "        chunk_id = len(self.chunks)\n",
    "        \n",
    "        if NUMPY_READY:\n",
    "            if isinstance(chunk.embedding, list):\n",
    "                embedding_array = np.array(chunk.embedding, dtype='float32').reshape(1, -1)\n",
    "            else:\n",
    "                embedding_array = chunk.embedding.reshape(1, -1)\n",
    "        else:\n",
    "            embedding_array = chunk.embedding\n",
    "        \n",
    "        self.index.add(embedding_array)\n",
    "        self.chunks.append(chunk)\n",
    "        self.metadata_store[chunk_id] = chunk.metadata\n",
    "\n",
    "        return chunk_id\n",
    "\n",
    "    async def search(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict[str, Any]]]:\n",
    "        \"\"\"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –ø–æ –∑–∞–ø—Ä–æ—Å—É\"\"\"\n",
    "        if len(self.chunks) == 0:\n",
    "            return [(\"–ù–µ—Ç –∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑–µ\", 1.0, {})]\n",
    "\n",
    "        if not EMBEDDINGS_READY:\n",
    "            # Demo —Ä–µ–∂–∏–º - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ chunks\n",
    "            results = []\n",
    "            for i, chunk in enumerate(self.chunks[:top_k]):\n",
    "                similarity = 0.95 - i * 0.1  # –ò–º–∏—Ç–∞—Ü–∏—è —É–±—ã–≤–∞—é—â–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "                results.append((chunk.content, similarity, chunk.metadata))\n",
    "            return results\n",
    "\n",
    "        try:\n",
    "            # –†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å embeddings\n",
    "            response = OPENAI_CLIENT.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=query\n",
    "            )\n",
    "            \n",
    "            if NUMPY_READY:\n",
    "                query_embedding = np.array(response.data[0].embedding, dtype='float32')\n",
    "            else:\n",
    "                query_embedding = response.data[0].embedding\n",
    "\n",
    "            # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ\n",
    "            if NUMPY_READY:\n",
    "                distances, indices = self.index.search(query_embedding.reshape(1, -1), min(top_k, len(self.chunks)))\n",
    "            else:\n",
    "                distances, indices = self.index.search(query_embedding, min(top_k, len(self.chunks)))\n",
    "\n",
    "            results = []\n",
    "            \n",
    "            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            if NUMPY_READY:\n",
    "                distance_list = distances[0] if hasattr(distances, '__getitem__') else distances\n",
    "                indices_list = indices[0] if hasattr(indices, '__getitem__') else indices\n",
    "            else:\n",
    "                distance_list = distances[0] if isinstance(distances, list) else distances\n",
    "                indices_list = indices[0] if isinstance(indices, list) else indices\n",
    "            \n",
    "            for distance, idx in zip(distance_list, indices_list):\n",
    "                if idx < len(self.chunks):  # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞\n",
    "                    chunk = self.chunks[idx]\n",
    "                    similarity = 1.0 / (1.0 + float(distance))  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è distance –≤ similarity\n",
    "                    results.append((chunk.content, similarity, chunk.metadata))\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è {self.agent_id}: {e}\")\n",
    "            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫\n",
    "            results = []\n",
    "            for i, chunk in enumerate(self.chunks[:top_k]):\n",
    "                similarity = 0.8 - i * 0.1\n",
    "                results.append((chunk.content, similarity, chunk.metadata))\n",
    "            return results\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\"\"\"\n",
    "        return {\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"index_size\": self.index.ntotal,\n",
    "            \"index_type\": self.index_type,\n",
    "            \"ready\": len(self.chunks) > 0\n",
    "        }\n",
    "\n",
    "class RAGKnowledgeManager:\n",
    "    \"\"\"–ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â –∞–≥–µ–Ω—Ç–æ–≤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stores: Dict[str, RAGVectorStore] = {}\n",
    "\n",
    "    async def create_agent_store(self, agent_id: str) -> RAGVectorStore:\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "        store = RAGVectorStore(agent_id)\n",
    "        self.stores[agent_id] = store\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞\n",
    "        await self._populate_agent_knowledge(agent_id, store)\n",
    "\n",
    "        return store\n",
    "\n",
    "    async def _populate_agent_knowledge(self, agent_id: str, store: RAGVectorStore):\n",
    "        \"\"\"–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "\n",
    "        # –ó–Ω–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∞–≥–µ–Ω—Ç–∞\n",
    "        agent_knowledge = {\n",
    "            \"lead_qualification\": [\n",
    "                \"BANT –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è: Budget (–±—é–¥–∂–µ—Ç), Authority (–ø–æ–ª–Ω–æ–º–æ—á–∏—è), Need (–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å), Timeline (—Å—Ä–æ–∫–∏)\",\n",
    "                \"MEDDIC –¥–ª—è B2B: Metrics, Economic buyer, Decision criteria, Decision process, Identify pain, Champion\",\n",
    "                \"–†–æ—Å—Å–∏–π—Å–∫–∏–π B2B —Ä—ã–Ω–æ–∫: –õ–ü–† —á–∞—Å—Ç–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞/–≤–ª–∞–¥–µ–ª—å—Ü–∞, long sales cycle 3-9 –º–µ—Å—è—Ü–µ–≤\",\n",
    "                \"Lead scoring: Cold (0-40), Warm (41-70), Hot (71-100). –§–∞–∫—Ç–æ—Ä—ã: budget size, authority level, urgency\",\n",
    "                \"–ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è SEO –ª–∏–¥–æ–≤: —Ä–∞–∑–º–µ—Ä —Å–∞–π—Ç–∞, —Ç–µ–∫—É—â–∏–π —Ç—Ä–∞—Ñ–∏–∫, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∏—à–∏, –±—é–¥–∂–µ—Ç –Ω–∞ SEO\"\n",
    "            ],\n",
    "            \"technical_seo_auditor\": [\n",
    "                \"Core Web Vitals: LCP <2.5s (Good), FID <100ms (Good), CLS <0.1 (Good)\",\n",
    "                \"Technical SEO checklist: crawlability, indexability, site speed, mobile-first, HTTPS, schema markup\",\n",
    "                \"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏: 404 —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –º–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –ø–ª–æ—Ö–∞—è –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è\",\n",
    "                \"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞—É–¥–∏—Ç–∞: Google PageSpeed Insights, Search Console, Screaming Frog, GTmetrix\",\n",
    "                \"–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ > –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å > —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ > –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞\"\n",
    "            ],\n",
    "            \"chief_seo_strategist\": [\n",
    "                \"SEO —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: technical foundation ‚Üí content strategy ‚Üí link building ‚Üí measurement\",\n",
    "                \"ROI —Ä–∞—Å—á–µ—Ç SEO: (Organic Revenue - SEO Cost) / SEO Cost. –¶–µ–ª–µ–≤–æ–π ROI 8-15x –¥–ª—è enterprise\",\n",
    "                \"–≠—Ç–∞–ø—ã –≤–Ω–µ–¥—Ä–µ–Ω–∏—è: 0-3 –º–µ—Å —Ç–µ—Ö–Ω–∏–∫–∞, 3-6 –º–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç, 6-12 –º–µ—Å —Å—Å—ã–ª–∫–∏, 12+ –º–µ—Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\",\n",
    "                \"KPI —Å–∏—Å—Ç–µ–º—ã: organic traffic growth, keyword rankings, conversion rate, customer acquisition cost\",\n",
    "                \"–†–∏—Å–∫–∏ SEO: algorithm updates, competitive response, technical debt, resource constraints\"\n",
    "            ],\n",
    "            \"content_strategy_agent\": [\n",
    "                \"E-E-A-T –ø—Ä–∏–Ω—Ü–∏–ø—ã Google: Experience, Expertise, Authoritativeness, Trustworthiness\",\n",
    "                \"–ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: keyword research ‚Üí content clusters ‚Üí editorial calendar ‚Üí performance tracking\",\n",
    "                \"–¢–∏–ø—ã SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–π, –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π\",\n",
    "                \"Content clusters: pillar pages + supporting content, internal linking structure\",\n",
    "                \"–ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ä—ã–Ω–∫–∞: –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è, —É—á–µ—Ç –º–µ–Ω—Ç–∞–ª–∏—Ç–µ—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É\"\n",
    "            ],\n",
    "            \"link_building_agent\": [\n",
    "                \"Link building —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: guest posting, broken link building, resource pages, digital PR\",\n",
    "                \"–ö–∞—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫: DA/DR –¥–æ–º–µ–Ω–∞, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å anchor text, follow/nofollow\",\n",
    "                \"Outreach process: prospecting ‚Üí qualification ‚Üí initial contact ‚Üí follow-up ‚Üí relationship building\",\n",
    "                \"Anchor text distribution: branded 50%, exact match 10%, partial match 20%, generic 20%\",\n",
    "                \"Link building –¥–ª—è –†–æ—Å—Å–∏–∏: —É—á–µ—Ç –Ø–Ω–¥–µ–∫—Å —Ñ–∞–∫—Ç–æ—Ä–æ–≤, –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–ª–æ—â–∞–¥–∫–∏, –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–µ –∫–∞—Ç–∞–ª–æ–≥–∏\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "        knowledge_list = agent_knowledge.get(agent_id, [\n",
    "            f\"–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_id}\",\n",
    "            \"SEO –ø—Ä–∏–Ω—Ü–∏–ø—ã: relevance, authority, user experience\",\n",
    "            \"–†–∞–±–æ—Ç–∞ –≤ –∫–æ–º–∞–Ω–¥–µ: –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è, collaboration, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å\"\n",
    "        ])\n",
    "\n",
    "        for i, knowledge in enumerate(knowledge_list):\n",
    "            await store.add_knowledge(\n",
    "                content=knowledge,\n",
    "                metadata={\n",
    "                    \"source\": \"base_knowledge\",\n",
    "                    \"priority\": \"high\" if i < 2 else \"medium\",\n",
    "                    \"category\": agent_id\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def get_store(self, agent_id: str) -> Optional[RAGVectorStore]:\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "        return self.stores.get(agent_id)\n",
    "\n",
    "    def get_all_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â\"\"\"\n",
    "        stats = {}\n",
    "        for agent_id, store in self.stores.items():\n",
    "            stats[agent_id] = store.get_stats()\n",
    "        return stats\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–Ω–∞–Ω–∏–π\n",
    "print(\"\\nüèóÔ∏è –°–û–ó–î–ê–ù–ò–ï RAG –ò–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–´ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "knowledge_manager = RAGKnowledgeManager()\n",
    "\n",
    "print(\"‚úÖ RAG –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–∑–¥–∞–Ω–∞\")\n",
    "print(f\"üîç FAISS —Å—Ç–∞—Ç—É—Å: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if FAISS_READY else 'üîÑ Mock —Ä–µ–∂–∏–º'}\")\n",
    "print(f\"üìä NumPy —Å—Ç–∞—Ç—É—Å: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if NUMPY_READY else 'üîÑ Fallback —Ä–µ–∂–∏–º'}\")\n",
    "print(f\"üìä Embeddings –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {'‚úÖ' if EMBEDDINGS_READY else 'üîÑ Demo —Ä–µ–∂–∏–º'}\")\n",
    "print(f\"ü§ñ Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞: ‚úÖ\")\n",
    "print(\"üöÄ –ì–æ—Ç–æ–≤ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ —Å RAG!\")\n",
    "\n",
    "globals()['KNOWLEDGE_MANAGER'] = knowledge_manager\n",
    "globals()['FAISS_READY'] = FAISS_READY\n",
    "globals()['NUMPY_READY'] = NUMPY_READY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ –ü–†–û–°–¢–û–ô –¢–ï–°–¢ RAG –°–ò–°–¢–ï–ú–´\n",
    "\n",
    "**–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π\n",
    "print(\"üß™ –¢–ï–°–¢ RAG –°–ò–°–¢–ï–ú–´\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "async def test_rag_system():\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç RAG —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
    "    try:\n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\n",
    "        test_store = await KNOWLEDGE_MANAGER.create_agent_store(\"test_agent\")\n",
    "        \n",
    "        print(f\"‚úÖ –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ: {test_store.agent_id}\")\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ\n",
    "        await test_store.add_knowledge(\n",
    "            \"–¢–µ—Å—Ç–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ –æ SEO –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\",\n",
    "            {\"category\": \"test\", \"priority\": \"high\"}\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ –ó–Ω–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ\")\n",
    "        \n",
    "        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫\n",
    "        results = await test_store.search(\"SEO —Ç–µ—Å—Ç\", top_k=1)\n",
    "        \n",
    "        print(f\"‚úÖ –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "        \n",
    "        if results:\n",
    "            content, similarity, metadata = results[0]\n",
    "            print(f\"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {content[:50]}...\")\n",
    "            print(f\"üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.3f}\")\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "        stats = test_store.get_stats()\n",
    "        print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats['total_chunks']} –∑–Ω–∞–Ω–∏–π, —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞: {stats['index_type']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}\")\n",
    "        return False\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç\n",
    "test_result = await test_rag_system()\n",
    "\n",
    "if test_result:\n",
    "    print(\"\\nüéâ RAG –°–ò–°–¢–ï–ú–ê –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û!\")\n",
    "    print(\"‚úÖ –ú–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è RAG —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏\")\n",
    "    print(\"üîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ\")\n",
    "\n",
    "print(f\"\\nüìã –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ô –°–¢–ê–¢–£–°:\")\n",
    "print(f\"üîç FAISS: {'‚úÖ' if FAISS_READY else 'üîÑ Mock'}\")\n",
    "print(f\"üìä NumPy: {'‚úÖ' if NUMPY_READY else 'üîÑ Lists'}\")\n",
    "print(f\"ü§ñ OpenAI: {'‚úÖ' if OPENAI_READY else 'üîÑ Demo'}\")\n",
    "print(f\"üöÄ Embeddings: {'‚úÖ' if EMBEDDINGS_READY else 'üîÑ Fake'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–î–ù–û–ì–û RAG –ê–ì–ï–ù–¢–ê (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n",
    "\n",
    "**–°–æ–∑–¥–∞–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ RAG –∞–≥–µ–Ω—Ç–∞\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleRAGAgent:\n",
    "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π RAG –∞–≥–µ–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, agent_name: str):\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_name = agent_name\n",
    "        self.openai_client = OPENAI_CLIENT if OPENAI_READY else None\n",
    "        self.vector_store = None\n",
    "        self.rag_enabled = False\n",
    "    \n",
    "    async def initialize_rag(self) -> bool:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –¥–ª—è –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "        try:\n",
    "            self.vector_store = await KNOWLEDGE_MANAGER.create_agent_store(self.agent_id)\n",
    "            self.rag_enabled = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG –¥–ª—è {self.agent_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def get_rag_context(self, query: str) -> str:\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ RAG\"\"\"\n",
    "        if not self.rag_enabled or not self.vector_store:\n",
    "            return \"–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\"\n",
    "        \n",
    "        try:\n",
    "            search_results = await self.vector_store.search(query, top_k=2)\n",
    "            context_parts = []\n",
    "            \n",
    "            for content, similarity, metadata in search_results:\n",
    "                context_parts.append(f\"[{similarity:.2f}] {content}\")\n",
    "            \n",
    "            return \"\\n\".join(context_parts) if context_parts else \"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\"\n",
    "        except Exception as e:\n",
    "            return f\"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {str(e)}\"\n",
    "    \n",
    "    async def process_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ —Å RAG\"\"\"\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å\n",
    "        query = \" \".join([f\"{k}: {v}\" for k, v in task_data.items()])\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "        rag_context = await self.get_rag_context(query)\n",
    "        \n",
    "        if not self.openai_client:\n",
    "            # Demo —Ä–µ–∂–∏–º\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": f\"\"\"ü§ñ {self.agent_name} (Demo —Ä–µ–∂–∏–º)\n",
    "\n",
    "üìö RAG –ö–û–ù–¢–ï–ö–°–¢:\n",
    "{rag_context[:300]}...\n",
    "\n",
    "üìã –î–ï–ú–û –ê–ù–ê–õ–ò–ó:\n",
    "–ö–æ–º–ø–∞–Ω–∏—è: {task_data.get('company_name', 'N/A')}\n",
    "–ë—é–¥–∂–µ—Ç: {task_data.get('budget_range', 'N/A')} ‚ÇΩ/–º–µ—Å\n",
    "–¶–µ–ª–∏: {task_data.get('goals', 'N/A')[:100]}...\n",
    "\n",
    "‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "üîç RAG —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç: {'‚úÖ' if self.rag_enabled else '‚ùå'}\"\"\",\n",
    "                \"rag_context_used\": rag_context,\n",
    "                \"rag_enabled\": self.rag_enabled,\n",
    "                \"fallback_mode\": True\n",
    "            }\n",
    "        \n",
    "        # –†–µ–∞–ª—å–Ω—ã–π OpenAI –∑–∞–ø—Ä–æ—Å\n",
    "        try:\n",
    "            task_context = \"\\n\".join([f\"{k}: {v}\" for k, v in task_data.items()])\n",
    "            \n",
    "            system_prompt = f\"\"\"–¢—ã {self.agent_name} –≤ —Å–∏—Å—Ç–µ–º–µ AI SEO Architects.\n",
    "\n",
    "–ë–ê–ó–ê –ó–ù–ê–ù–ò–ô:\n",
    "{rag_context}\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\"\"\"\n",
    "            \n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"–û–±—Ä–∞–±–æ—Ç–∞–π –∑–∞–¥–∞—á—É:\\n{task_context}\"}\n",
    "                ],\n",
    "                max_tokens=1500,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": response.choices[0].message.content,\n",
    "                \"model_used\": \"gpt-4o-mini\",\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"rag_context_used\": rag_context,\n",
    "                \"rag_enabled\": self.rag_enabled,\n",
    "                \"fallback_mode\": False\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"rag_context_used\": rag_context,\n",
    "                \"rag_enabled\": self.rag_enabled,\n",
    "                \"fallback_mode\": True\n",
    "            }\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "print(\"ü§ñ –°–û–ó–î–ê–ù–ò–ï –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–û–ì–û RAG –ê–ì–ï–ù–¢–ê\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "demo_agent = SimpleRAGAgent(\"technical_seo_auditor\", \"Technical SEO Auditor\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG\n",
    "print(\"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG...\")\n",
    "rag_init_success = await demo_agent.initialize_rag()\n",
    "\n",
    "print(f\"‚úÖ RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: {'–£—Å–ø–µ—à–Ω–æ' if rag_init_success else '–° –æ—à–∏–±–∫–∞–º–∏'}\")\n",
    "print(f\"üîç RAG —Å—Ç–∞—Ç—É—Å: {'–ê–∫—Ç–∏–≤–µ–Ω' if demo_agent.rag_enabled else '–ù–µ–∞–∫—Ç–∏–≤–µ–Ω'}\")\n",
    "print(f\"ü§ñ OpenAI —Å—Ç–∞—Ç—É—Å: {'–ê–∫—Ç–∏–≤–µ–Ω' if demo_agent.openai_client else 'Demo —Ä–µ–∂–∏–º'}\")\n",
    "\n",
    "if demo_agent.vector_store:\n",
    "    stats = demo_agent.vector_store.get_stats()\n",
    "    print(f\"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {stats['total_chunks']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ ({stats['index_type']} –∏–Ω–¥–µ–∫—Å)\")\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "test_task_data = {\n",
    "    \"company_name\": \"TechStart Solutions\",\n",
    "    \"website\": \"techstart-solutions.ru\",\n",
    "    \"budget_range\": \"750000\",\n",
    "    \"goals\": \"–£–ª—É—á—à–∏—Ç—å Core Web Vitals –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ SEO\",\n",
    "    \"current_issues\": \"–ú–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π\"\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ë–û–¢–´ –ê–ì–ï–ù–¢–ê\")\n",
    "print(f\"üè¢ –ö–ª–∏–µ–Ω—Ç: {test_task_data['company_name']}\")\n",
    "print(f\"üí∞ –ë—é–¥–∂–µ—Ç: {test_task_data['budget_range']} ‚ÇΩ/–º–µ—Å\")\n",
    "print(f\"üéØ –¶–µ–ª–∏: {test_task_data['goals']}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É\n",
    "print(\"\\n‚è±Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "result = await demo_agent.process_task(test_task_data)\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "\n",
    "if result.get('success'):\n",
    "    print(f\"‚úÖ –°—Ç–∞—Ç—É—Å: –£—Å–ø–µ—à–Ω–æ\")\n",
    "    \n",
    "    if result.get('model_used'):\n",
    "        print(f\"ü§ñ –ú–æ–¥–µ–ª—å: {result['model_used']}\")\n",
    "    if result.get('tokens_used'):\n",
    "        print(f\"üî¢ –¢–æ–∫–µ–Ω—ã: {result['tokens_used']}\")\n",
    "    \n",
    "    print(f\"üîç RAG: {'‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω' if result.get('rag_enabled') else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}\")\n",
    "    \n",
    "    print(f\"\\nüìù –†–ï–ó–£–õ–¨–¢–ê–¢:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(result.get('result', '–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'))\n",
    "    \n",
    "    if result.get('rag_context_used') and len(result['rag_context_used']) > 10:\n",
    "        print(f\"\\nüß† RAG –ö–û–ù–¢–ï–ö–°–¢ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
    "        print(\"-\" * 40)\n",
    "        context = result['rag_context_used']\n",
    "        print(context[:200] + \"...\" if len(context) > 200 else context)\n",
    "else:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\")\n",
    "\n",
    "print(\"\\nüéâ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø RAG –ê–ì–ï–ù–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéä –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê\n",
    "\n",
    "### ‚úÖ **–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏:**\n",
    "\n",
    "#### üîß **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**\n",
    "1. **–ö–æ–Ω—Ñ–ª–∏–∫—Ç NumPy/FAISS** - –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è numpy==1.24.3\n",
    "2. **Graceful degradation** - —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –µ—Å–ª–∏ FAISS –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è\n",
    "3. **MockFAISSIndex** - –ø–æ–ª–Ω–∞—è –∏–º–∏—Ç–∞—Ü–∏—è FAISS –¥–ª—è —Å–ª—É—á–∞–µ–≤ —Å–±–æ–µ–≤\n",
    "4. **–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã** - –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–º–ø–æ—Ä—Ç—ã —Å try/except\n",
    "5. **–°–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏** - –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –ø–∞–∫–µ—Ç–æ–≤\n",
    "\n",
    "#### üõ°Ô∏è **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã:**\n",
    "- **–†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó FAISS** - –ø–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤ Mock —Ä–µ–∂–∏–º–µ\n",
    "- **–†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó NumPy** - fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–ø–∏—Å–∫–∏ Python\n",
    "- **–†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó OpenAI API** - demo —Ä–µ–∂–∏–º —Å RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ** –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Å—Ä–µ–¥—ã\n",
    "\n",
    "#### üìä **–†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:**\n",
    "```python\n",
    "üéØ Production: OpenAI API + FAISS + NumPy\n",
    "üîÑ Hybrid: OpenAI API + Mock FAISS + Python Lists  \n",
    "üé≠ Demo: Mock –≤—Å–µ + RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "```\n",
    "\n",
    "### üöÄ **–ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ:**\n",
    "- ‚úÖ **RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - –ø–æ–ª–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤\n",
    "- ‚úÖ **–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π** - —Å FAISS –∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ\n",
    "- ‚úÖ **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** - —Å embeddings –∏–ª–∏ –∏–º–∏—Ç–∞—Ü–∏–µ–π\n",
    "- ‚úÖ **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã** - —Å domain expertise\n",
    "- ‚úÖ **OpenAI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –µ—Å–ª–∏ API –∫–ª—é—á –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "\n",
    "### üéØ **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:**\n",
    "\n",
    "1. **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ runtime** –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏\n",
    "2. **–î–æ–±–∞–≤—å—Ç–µ OpenAI API –∫–ª—é—á** –≤ —Å–µ–∫—Ä–µ—Ç—ã Colab\n",
    "3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏** –ø–æ –ø–æ—Ä—è–¥–∫—É\n",
    "4. **–°–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è** –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º\n",
    "\n",
    "### üèÜ **–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
    "**–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è RAG –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ª—é–±—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö!**\n",
    "\n",
    "---\n",
    "\n",
    "**ü§ñ Powered by Adaptive RAG | üõ°Ô∏è Error-Resistant | üöÄ Google Colab Ready**\n",
    "\n",
    "**–ê–≤—Ç–æ—Ä:** Andrew Popov (a.popov.gv@gmail.com)  \n",
    "**GitHub:** https://github.com/Andrew821667/ai-seo-architects  \n",
    "**–î–∞—Ç–∞:** 13 –∞–≤–≥—É—Å—Ç–∞ 2025\n",
    "\n",
    "*üéØ Adaptive RAG | üõ°Ô∏è Production Resilient | üîß Self-Healing Architecture*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}