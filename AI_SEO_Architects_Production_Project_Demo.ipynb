{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AI SEO Architects - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Production –ü—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "## üéØ –û–ø–∏—Å–∞–Ω–∏–µ\n",
    "\n",
    "**–ü–æ–ª–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ AI SEO Architects** —Å GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Å—å production-ready —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:\n",
    "\n",
    "### üî• –ß—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º:\n",
    "- **–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞** –∏–∑ GitHub\n",
    "- **14 production –∞–≥–µ–Ω—Ç–æ–≤** –∏–∑ –ø–∞–ø–∫–∏ agents/\n",
    "- **–ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞** —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π\n",
    "- **FAISS —ç–º–±–µ–¥–¥–∏–Ω–≥–∏** —Å OpenAI API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π\n",
    "- **–†–µ–∞–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è—é—â–∏—Ö\n",
    "- **Enterprise workflow** –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "- **MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** –∏ FastAPI Backend\n",
    "\n",
    "### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:\n",
    "```\n",
    "üìä GitHub Clone ‚Üí ü§ñ 14 Real Agents ‚Üí üîç RAG + FAISS ‚Üí üìö Knowledge Bases ‚Üí ‚úÖ Production Demo\n",
    "```\n",
    "\n",
    "### üîë –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
    "1. **OpenAI API –∫–ª—é—á** - –¥–æ–±–∞–≤—å—Ç–µ –≤ —Å–µ–∫—Ä–µ—Ç—ã Colab –∫–∞–∫ `OPENAI_API_KEY`\n",
    "2. **Google Colab** - –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã\n",
    "3. **–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç** - –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ –®–∞–≥ 1: –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ AI SEO Architects –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ AI SEO ARCHITECTS - PRODUCTION PROJECT DEMO\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"üìÖ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python –≤–µ—Ä—Å–∏—è: {sys.version.split()[0]}\")\n",
    "print(f\"üíª –°—Ä–µ–¥–∞: {'Google Colab' if 'google.colab' in sys.modules else 'Jupyter'}\")\n",
    "\n",
    "# –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç AI SEO Architects\n",
    "print(\"\\nüì• –ö–õ–û–ù–ò–†–û–í–ê–ù–ò–ï PRODUCTION –ü–†–û–ï–ö–¢–ê\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "project_path = \"/content/ai-seo-architects\"\n",
    "\n",
    "if not os.path.exists(project_path):\n",
    "    print(\"üì• –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ AI SEO Architects –∏–∑ GitHub...\")\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            \"git\", \"clone\", \n",
    "            \"https://github.com/Andrew821667/ai-seo-architects.git\", \n",
    "            project_path\n",
    "        ])\n",
    "        print(\"‚úÖ –ü—Ä–æ–µ–∫—Ç —É—Å–ø–µ—à–Ω–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
    "        print(\"üîÑ –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...\")\n",
    "else:\n",
    "    print(\"‚úÖ –ü—Ä–æ–µ–∫—Ç —É–∂–µ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")\n",
    "\n",
    "# –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    sys.path.insert(0, project_path)\n",
    "    print(f\"üìÇ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞\n",
    "    print(\"\\nüìÅ –°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    key_dirs = ['agents', 'core', 'knowledge', 'api', 'config']\n",
    "    for dir_name in key_dirs:\n",
    "        if os.path.exists(dir_name):\n",
    "            file_count = len([f for f in os.listdir(dir_name) if f.endswith('.py')])\n",
    "            print(f\"‚úÖ {dir_name}/ - {file_count} Python —Ñ–∞–π–ª–æ–≤\")\n",
    "        else:\n",
    "            print(f\"‚ùå {dir_name}/ - –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")\nelse:\n",
    "    print(\"‚ö†Ô∏è –†–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\")\n",
    "\n",
    "def safe_install(package, description=\"\"):\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–∞\"\"\"\n",
    "    try:\n",
    "        print(f\"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º {package}... {description}\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(f\"‚úÖ {package} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ requirements.txt –∏–ª–∏ manual\n",
    "print(\"\\nüì¶ –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# –ü—Ä–æ–±—É–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ requirements.txt\n",
    "if os.path.exists(\"requirements.txt\"):\n",
    "    print(\"üìã –ù–∞–π–¥–µ–Ω requirements.txt, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n",
    "        print(\"‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å requirements.txt, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º manually\")\n",
    "        \n",
    "# Manual —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞–∫–µ—Ç–æ–≤\n",
    "critical_packages = [\n",
    "    ('openai==1.54.3', 'OpenAI API'),\n",
    "    ('faiss-cpu==1.7.4', 'FAISS –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞'),\n",
    "    ('numpy==1.24.3', 'NumPy —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è'),\n",
    "    ('pydantic==2.5.0', '–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö'),\n",
    "    ('nest-asyncio', 'Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞'),\n",
    "    ('langchain', 'LangChain framework'),\n",
    "    ('langchain-openai', 'LangChain OpenAI'),\n",
    "    ('tiktoken', '–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è'),\n",
    "    ('python-dotenv', 'Environment –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ'),\n",
    "    ('tqdm', 'Progress bars')\n",
    "]\n",
    "\n",
    "success_count = 0\n",
    "for package, description in critical_packages:\n",
    "    if safe_install(package, description):\n",
    "        success_count += 1\n",
    "\n",
    "print(f\"\\nüìä –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ: {success_count}/{len(critical_packages)}\")\n",
    "print(\"üéâ –°—Ä–µ–¥–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def setup_openai_for_project() -> Tuple[bool, Optional[str], bool]:\n",
    "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞\"\"\"\n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            api_key = userdata.get('OPENAI_API_KEY')\n",
    "            print(\"‚úÖ OpenAI API –∫–ª—é—á –ø–æ–ª—É—á–µ–Ω –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Google Colab\")\n",
    "        except Exception:\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                print(\"‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "                print(\"üí° –î–æ–±–∞–≤—å—Ç–µ –∫–ª—é—á –≤ —Å–µ–∫—Ä–µ—Ç—ã Colab (üîë –≤ –ª–µ–≤–æ–º –º–µ–Ω—é): OPENAI_API_KEY\")\n",
    "                return False, \"API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω\", False\n",
    "\n",
    "        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–ª—é—á–∞\n",
    "        os.environ['OPENAI_API_KEY'] = api_key\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "        # –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "        print(\"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API...\")\n",
    "        \n",
    "        # LLM —Ç–µ—Å—Ç\n",
    "        llm_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"–¢–µ—Å—Ç AI SEO Architects. –û—Ç–≤–µ—Ç—å: OK\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        llm_test = \"OK\" in llm_response.choices[0].message.content\n",
    "        print(f\"‚úÖ LLM —Ç–µ—Å—Ç: {'–ü—Ä–æ–π–¥–µ–Ω' if llm_test else '–ù–µ –ø—Ä–æ–π–¥–µ–Ω'}\")\n",
    "\n",
    "        # Embeddings —Ç–µ—Å—Ç\n",
    "        embedding_response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=\"AI SEO Architects project test\"\n",
    "        )\n",
    "        embedding_vector = embedding_response.data[0].embedding\n",
    "        embedding_test = len(embedding_vector) > 0\n",
    "        print(f\"‚úÖ Embeddings —Ç–µ—Å—Ç: {'–ü—Ä–æ–π–¥–µ–Ω' if embedding_test else '–ù–µ –ø—Ä–æ–π–¥–µ–Ω'}\")\n",
    "        print(f\"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {len(embedding_vector)}\")\n",
    "\n",
    "        success = llm_test and embedding_test\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ OpenAI API –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞!\")\n",
    "            return True, None, True\n",
    "        else:\n",
    "            return False, \"–¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã\", False\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {error_msg}\")\n",
    "        return False, error_msg, False\n",
    "\n",
    "def check_project_structure():\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞\"\"\"\n",
    "    print(\"\\nüèóÔ∏è –ü–†–û–í–ï–†–ö–ê –°–¢–†–£–ö–¢–£–†–´ –ü–†–û–ï–ö–¢–ê\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "    project_components = {\n",
    "        'core/base_agent.py': '–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –∞–≥–µ–Ω—Ç–æ–≤',\n",
    "        'core/orchestrator.py': '–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∞–≥–µ–Ω—Ç–æ–≤',\n",
    "        'agents/': '–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∞–≥–µ–Ω—Ç–æ–≤',\n",
    "        'knowledge/': '–ë–∞–∑—ã –∑–Ω–∞–Ω–∏–π',\n",
    "        'CLAUDE.md': '–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞',\n",
    "        'requirements.txt': '–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏',\n",
    "        'api/': 'FastAPI Backend'\n",
    "    }\n",
    "    \n",
    "    available_components = []\n",
    "    missing_components = []\n",
    "    \n",
    "    for component, description in project_components.items():\n",
    "        if os.path.exists(component):\n",
    "            available_components.append(component)\n",
    "            print(f\"‚úÖ {component} - {description}\")\n",
    "        else:\n",
    "            missing_components.append(component)\n",
    "            print(f\"‚ùå {component} - {description} (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)\")\n",
    "    \n",
    "    # –ü–æ–¥—Å—á–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤\n",
    "    if os.path.exists('agents'):\n",
    "        agent_count = 0\n",
    "        for root, dirs, files in os.walk('agents'):\n",
    "            agent_count += len([f for f in files if f.endswith('.py') and f != '__init__.py'])\n",
    "        print(f\"ü§ñ –ù–∞–π–¥–µ–Ω–æ –∞–≥–µ–Ω—Ç–æ–≤: {agent_count}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ knowledge bases\n",
    "    if os.path.exists('knowledge'):\n",
    "        knowledge_files = len([f for f in os.listdir('knowledge') if not f.startswith('.')])\n",
    "        print(f\"üìö Knowledge files: {knowledge_files}\")\n",
    "    \n",
    "    return len(available_components), len(missing_components)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "print(\"üîë –ù–ê–°–¢–†–û–ô–ö–ê OPENAI API –î–õ–Ø –ü–†–û–ï–ö–¢–ê\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "api_ready, error, embeddings_ready = setup_openai_for_project()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞\n",
    "available_count, missing_count = check_project_structure()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å\n",
    "globals()['OPENAI_READY'] = api_ready\n",
    "globals()['EMBEDDINGS_READY'] = embeddings_ready\n",
    "globals()['OPENAI_CLIENT'] = openai.OpenAI() if api_ready else None\n",
    "globals()['PROJECT_READY'] = available_count > missing_count\n",
    "\n",
    "print(f\"\\nüìä –°–¢–ê–¢–£–° –ì–û–¢–û–í–ù–û–°–¢–ò:\")\n",
    "print(f\"ü§ñ OpenAI API: {'‚úÖ' if api_ready else '‚ùå'}\")\n",
    "print(f\"üîç Embeddings: {'‚úÖ' if embeddings_ready else '‚ùå'}\")\n",
    "print(f\"üèóÔ∏è –ü—Ä–æ–µ–∫—Ç: {'‚úÖ' if PROJECT_READY else '‚ö†Ô∏è'} ({available_count}/{available_count + missing_count} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤)\")\n",
    "print(f\"üöÄ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {'‚úÖ –ü–æ–ª–Ω–∞—è' if api_ready and PROJECT_READY else 'üîÑ –ß–∞—Å—Ç–∏—á–Ω–∞—è'}\")\n",
    "\n",
    "if not api_ready:\n",
    "    print(f\"‚ö†Ô∏è OpenAI –æ—à–∏–±–∫–∞: {error}\")\n",
    "    print(\"üí° –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω demo —Ä–µ–∂–∏–º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "import importlib\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º async –ø–æ–¥–¥–µ—Ä–∂–∫—É\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"ü§ñ –ó–ê–ì–†–£–ó–ö–ê –†–ï–ê–õ–¨–ù–´–• –ê–ì–ï–ù–¢–û–í –ü–†–û–ï–ö–¢–ê\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º (–∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ)\n",
    "project_agents = {\n",
    "    'executive': [\n",
    "        'chief_seo_strategist',\n",
    "        'business_development_director'\n",
    "    ],\n",
    "    'management': [\n",
    "        'task_coordination_agent',\n",
    "        'sales_operations_manager', \n",
    "        'technical_seo_operations_manager',\n",
    "        'client_success_manager'\n",
    "    ],\n",
    "    'operational': [\n",
    "        'lead_qualification',\n",
    "        'technical_seo_auditor',\n",
    "        'proposal_generation_agent',\n",
    "        'sales_conversation_agent',\n",
    "        'content_strategy_agent',\n",
    "        'link_building_agent',\n",
    "        'competitive_analysis_agent',\n",
    "        'reporting_agent'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def safe_import_agent(level: str, agent_name: str):\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç –∞–≥–µ–Ω—Ç–∞ –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞\"\"\"\n",
    "    try:\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∏–º–ø–æ—Ä—Ç–∞\n",
    "        module_path = f\"agents.{level}.{agent_name}\"\n",
    "        \n",
    "        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å\n",
    "        module = importlib.import_module(module_path)\n",
    "        \n",
    "        # –ò—â–µ–º –∫–ª–∞—Å—Å –∞–≥–µ–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ CamelCase)\n",
    "        class_name = ''.join(word.capitalize() for word in agent_name.split('_')) + 'Agent'\n",
    "        \n",
    "        if hasattr(module, class_name):\n",
    "            agent_class = getattr(module, class_name)\n",
    "            print(f\"‚úÖ {agent_name} - {class_name} –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "            return agent_class\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {agent_name} - –∫–ª–∞—Å—Å {class_name} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "            return None\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {agent_name} - –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"üí• {agent_name} - –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_base_agent():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "    try:\n",
    "        from core.base_agent import BaseAgent\n",
    "        print(\"‚úÖ BaseAgent –∏–∑ core.base_agent –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "        return BaseAgent\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è BaseAgent –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º mock –≤–µ—Ä—Å–∏—é\")\n",
    "        \n",
    "        # Mock –≤–µ—Ä—Å–∏—è BaseAgent –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "        class MockBaseAgent:\n",
    "            def __init__(self, agent_id: str, agent_level: str = \"operational\"):\n",
    "                self.agent_id = agent_id\n",
    "                self.agent_level = agent_level\n",
    "                self.agent_name = agent_id.replace('_', ' ').title() + ' Agent'\n",
    "                self.openai_client = OPENAI_CLIENT if OPENAI_READY else None\n",
    "            \n",
    "            async def process_task(self, task_data):\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": f\"Mock –æ—Ç–≤–µ—Ç –æ—Ç {self.agent_name}\",\n",
    "                    \"agent_id\": self.agent_id,\n",
    "                    \"fallback_mode\": True\n",
    "                }\n",
    "        \n",
    "        return MockBaseAgent\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å\n",
    "BaseAgent = load_base_agent()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "loaded_agents = {}\n",
    "total_agents = 0\n",
    "loaded_count = 0\n",
    "\n",
    "for level, agents_list in project_agents.items():\n",
    "    print(f\"\\nüìã {level.upper()} –£–†–û–í–ï–ù–¨:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    level_agents = {}\n",
    "    for agent_name in agents_list:\n",
    "        total_agents += 1\n",
    "        agent_class = safe_import_agent(level, agent_name)\n",
    "        \n",
    "        if agent_class:\n",
    "            level_agents[agent_name] = agent_class\n",
    "            loaded_count += 1\n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º mock –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ BaseAgent\n",
    "            print(f\"üîÑ –°–æ–∑–¥–∞–µ–º mock –¥–ª—è {agent_name}\")\n",
    "            level_agents[agent_name] = type(f\"Mock{agent_name.title()}Agent\", (BaseAgent,), {\n",
    "                '__init__': lambda self, *args, **kwargs: BaseAgent.__init__(self, agent_name, level)\n",
    "            })\n",
    "    \n",
    "    loaded_agents[level] = level_agents\n",
    "\n",
    "print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ó–ê–ì–†–£–ó–ö–ò –ê–ì–ï–ù–¢–û–í:\")\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {loaded_count}/{total_agents} –∞–≥–µ–Ω—Ç–æ–≤\")\n",
    "print(f\"üéØ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {loaded_count/total_agents*100:.1f}%\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –∫–ª—é—á–µ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "print(\"\\nüè≠ –°–û–ó–î–ê–ù–ò–ï –≠–ö–ó–ï–ú–ü–õ–Ø–†–û–í –ê–ì–ï–ù–¢–û–í\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "demo_agents = {}\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É —Å –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è\n",
    "key_agents = [\n",
    "    ('operational', 'lead_qualification', 'Lead Qualification'),\n",
    "    ('operational', 'technical_seo_auditor', 'Technical SEO Auditor'),\n",
    "    ('management', 'sales_operations_manager', 'Sales Operations Manager'),\n",
    "    ('executive', 'chief_seo_strategist', 'Chief SEO Strategist')\n",
    "]\n",
    "\n",
    "for level, agent_id, display_name in key_agents:\n",
    "    try:\n",
    "        if agent_id in loaded_agents[level]:\n",
    "            agent_class = loaded_agents[level][agent_id]\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "            if hasattr(agent_class, '__call__'):\n",
    "                if 'agent_level' in agent_class.__init__.__code__.co_varnames:\n",
    "                    agent_instance = agent_class(agent_level=level)\n",
    "                else:\n",
    "                    agent_instance = agent_class()\n",
    "            else:\n",
    "                agent_instance = agent_class\n",
    "            \n",
    "            demo_agents[agent_id] = agent_instance\n",
    "            print(f\"‚úÖ {display_name} - —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ–∑–¥–∞–Ω\")\n",
    "        else:\n",
    "            print(f\"‚ùå {display_name} - –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {level}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"üí• {display_name} - –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ –°–æ–∑–¥–∞–Ω–æ {len(demo_agents)} –¥–µ–º–æ-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –¥—Ä—É–≥–∏—Ö —è—á–µ–µ–∫\n",
    "globals()['LOADED_AGENTS'] = loaded_agents\n",
    "globals()['DEMO_AGENTS'] = demo_agents\n",
    "globals()['PROJECT_AGENTS_COUNT'] = total_agents\n",
    "globals()['LOADED_AGENTS_COUNT'] = loaded_count\n",
    "\n",
    "print(f\"\\nüöÄ –ê–≥–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã –∏ –±–∞–∑ –∑–Ω–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã –∏ –±–∞–∑ –∑–Ω–∞–Ω–∏–π –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "print(\"üìö –ó–ê–ì–†–£–ó–ö–ê RAG –°–ò–°–¢–ï–ú–´ –ò –ë–ê–ó –ó–ù–ê–ù–ò–ô\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def load_knowledge_bases():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ knowledge/\"\"\"\n",
    "    knowledge_data = {}\n",
    "    \n",
    "    if not os.path.exists('knowledge'):\n",
    "        print(\"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è knowledge/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")\n",
    "        return knowledge_data\n",
    "    \n",
    "    print(\"üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ knowledge/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...\")\n",
    "    \n",
    "    for root, dirs, files in os.walk('knowledge'):\n",
    "        for file in files:\n",
    "            if file.endswith(('.txt', '.md', '.json')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ –ø—É—Ç–∏\n",
    "                    relative_path = os.path.relpath(file_path, 'knowledge')\n",
    "                    category = os.path.dirname(relative_path) if os.path.dirname(relative_path) else 'general'\n",
    "                    \n",
    "                    if category not in knowledge_data:\n",
    "                        knowledge_data[category] = []\n",
    "                    \n",
    "                    knowledge_data[category].append({\n",
    "                        'file': file,\n",
    "                        'content': content,\n",
    "                        'size': len(content)\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"‚úÖ {relative_path} - {len(content)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è {file_path} - –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}\")\n",
    "    \n",
    "    return knowledge_data\n",
    "\n",
    "def setup_rag_system():\n",
    "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏\"\"\"\n",
    "    print(\"\\nüèóÔ∏è –ù–ê–°–¢–†–û–ô–ö–ê RAG –°–ò–°–¢–ï–ú–´\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º FAISS\n",
    "    try:\n",
    "        import faiss\n",
    "        import numpy as np\n",
    "        print(\"‚úÖ FAISS –∏ NumPy –¥–æ—Å—Ç—É–ø–Ω—ã\")\n",
    "        faiss_ready = True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è FAISS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}\")\n",
    "        print(\"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É\")\n",
    "        faiss_ready = False\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    class ProjectRAGSystem:\n",
    "        def __init__(self, knowledge_data: Dict[str, List[Dict]]):\n",
    "            self.knowledge_data = knowledge_data\n",
    "            self.faiss_ready = faiss_ready\n",
    "            self.embeddings_cache = {}\n",
    "        \n",
    "        async def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "            \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤\"\"\"\n",
    "            if not EMBEDDINGS_READY:\n",
    "                # Mock —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "                return [[0.1] * 1536 for _ in texts]\n",
    "            \n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                if text in self.embeddings_cache:\n",
    "                    embeddings.append(self.embeddings_cache[text])\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    response = OPENAI_CLIENT.embeddings.create(\n",
    "                        model=\"text-embedding-3-small\",\n",
    "                        input=text[:8000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É\n",
    "                    )\n",
    "                    embedding = response.data[0].embedding\n",
    "                    self.embeddings_cache[text] = embedding\n",
    "                    embeddings.append(embedding)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}\")\n",
    "                    # Fallback —ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "                    embeddings.append([0.1] * 1536)\n",
    "            \n",
    "            return embeddings\n",
    "        \n",
    "        async def search_knowledge(self, query: str, category: str = None, top_k: int = 3) -> List[Dict]:\n",
    "            \"\"\"–ü–æ–∏—Å–∫ –≤ –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π\"\"\"\n",
    "            results = []\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "            categories = [category] if category and category in self.knowledge_data else list(self.knowledge_data.keys())\n",
    "            \n",
    "            for cat in categories:\n",
    "                if cat not in self.knowledge_data:\n",
    "                    continue\n",
    "                \n",
    "                for knowledge_item in self.knowledge_data[cat]:\n",
    "                    content = knowledge_item['content']\n",
    "                    \n",
    "                    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "                    query_words = query.lower().split()\n",
    "                    content_lower = content.lower()\n",
    "                    \n",
    "                    # –ü–æ–¥—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "                    relevance = 0\n",
    "                    for word in query_words:\n",
    "                        if word in content_lower:\n",
    "                            relevance += content_lower.count(word)\n",
    "                    \n",
    "                    if relevance > 0:\n",
    "                        results.append({\n",
    "                            'content': content[:500] + \"...\" if len(content) > 500 else content,\n",
    "                            'category': cat,\n",
    "                            'file': knowledge_item['file'],\n",
    "                            'relevance': relevance,\n",
    "                            'full_content': content\n",
    "                        })\n",
    "            \n",
    "            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "            results.sort(key=lambda x: x['relevance'], reverse=True)\n",
    "            \n",
    "            return results[:top_k]\n",
    "        \n",
    "        def get_stats(self) -> Dict[str, Any]:\n",
    "            \"\"\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
    "            total_files = sum(len(files) for files in self.knowledge_data.values())\n",
    "            total_content = sum(\n",
    "                sum(item['size'] for item in files) \n",
    "                for files in self.knowledge_data.values()\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'categories': len(self.knowledge_data),\n",
    "                'total_files': total_files,\n",
    "                'total_content_size': total_content,\n",
    "                'faiss_ready': self.faiss_ready,\n",
    "                'embeddings_ready': EMBEDDINGS_READY,\n",
    "                'cached_embeddings': len(self.embeddings_cache)\n",
    "            }\n",
    "    \n",
    "    return ProjectRAGSystem\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n",
    "knowledge_data = load_knowledge_bases()\n",
    "\n",
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–Ω–∞–Ω–∏–π\n",
    "print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó –ó–ù–ê–ù–ò–ô:\")\n",
    "total_files = 0\n",
    "total_size = 0\n",
    "\n",
    "for category, files in knowledge_data.items():\n",
    "    category_files = len(files)\n",
    "    category_size = sum(item['size'] for item in files)\n",
    "    total_files += category_files\n",
    "    total_size += category_size\n",
    "    \n",
    "    print(f\"üìÇ {category}: {category_files} —Ñ–∞–π–ª–æ–≤, {category_size:,} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "print(f\"\\nüéØ –ò–¢–û–ì–û: {total_files} —Ñ–∞–π–ª–æ–≤, {total_size:,} —Å–∏–º–≤–æ–ª–æ–≤ –∑–Ω–∞–Ω–∏–π\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É\n",
    "ProjectRAGSystem = setup_rag_system()\n",
    "rag_system = ProjectRAGSystem(knowledge_data)\n",
    "\n",
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RAG\n",
    "rag_stats = rag_system.get_stats()\n",
    "print(f\"\\nüöÄ RAG –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê:\")\n",
    "print(f\"üìö –ö–∞—Ç–µ–≥–æ—Ä–∏–π –∑–Ω–∞–Ω–∏–π: {rag_stats['categories']}\")\n",
    "print(f\"üìÑ –§–∞–π–ª–æ–≤ –∑–Ω–∞–Ω–∏–π: {rag_stats['total_files']}\")\n",
    "print(f\"üîç FAISS: {'‚úÖ' if rag_stats['faiss_ready'] else 'üîÑ Mock'}\")\n",
    "print(f\"ü§ñ Embeddings: {'‚úÖ' if rag_stats['embeddings_ready'] else 'üîÑ Mock'}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –¥—Ä—É–≥–∏—Ö —è—á–µ–µ–∫\n",
    "globals()['KNOWLEDGE_DATA'] = knowledge_data\n",
    "globals()['RAG_SYSTEM'] = rag_system\n",
    "globals()['RAG_READY'] = total_files > 0\n",
    "\n",
    "print(\"\\nüéâ RAG —Å–∏—Å—Ç–µ–º–∞ –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ –®–∞–≥ 5: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "\n",
    "print(\"üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–û–ù–ê–õ–ê\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "async def experiment_1_rag_search():\n",
    "    \"\"\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: RAG –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–∞–º –∑–Ω–∞–Ω–∏–π\"\"\"\n",
    "    print(\"\\nüîç –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: RAG –ü–û–ò–°–ö\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n",
    "    test_queries = [\n",
    "        \"BANT –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–∏–¥–æ–≤\",\n",
    "        \"Core Web Vitals –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\",\n",
    "        \"SEO —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è enterprise\",\n",
    "        \"FastAPI backend –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîé –ó–∞–ø—Ä–æ—Å {i}: {query}\")\n",
    "        \n",
    "        results = await RAG_SYSTEM.search_knowledge(query, top_k=2)\n",
    "        \n",
    "        if results:\n",
    "            for j, result in enumerate(results, 1):\n",
    "                print(f\"  üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç {j}: {result['file']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevance']})\")\n",
    "                print(f\"    üìù {result['content'][:100]}...\")\n",
    "        else:\n",
    "            print(\"  ‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\")\n",
    "    \n",
    "    print(\"\\n‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1 –∑–∞–≤–µ—Ä—à–µ–Ω\")\n",
    "\n",
    "async def experiment_2_agent_processing():\n",
    "    \"\"\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–∞–º–∏\"\"\"\n",
    "    print(\"\\nü§ñ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–î–ê–ß –ê–ì–ï–ù–¢–ê–ú–ò\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ enterprise –∫–ª–∏–µ–Ω—Ç–∞\n",
    "    test_client = {\n",
    "        \"company_name\": \"Enterprise Tech Solutions\",\n",
    "        \"industry\": \"enterprise software\",\n",
    "        \"budget_range\": \"2000000\",\n",
    "        \"website\": \"enterprise-tech.com\",\n",
    "        \"goals\": \"–£–≤–µ–ª–∏—á–∏—Ç—å –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞—Ñ–∏–∫ –Ω–∞ 400%, —Å–Ω–∏–∑–∏—Ç—å CAC –Ω–∞ 50%\",\n",
    "        \"timeline\": \"12 –º–µ—Å—è—Ü–µ–≤\",\n",
    "        \"employees\": \"5000\",\n",
    "        \"annual_revenue\": \"50000000000\"\n",
    "    }\n",
    "    \n",
    "    print(f\"üë§ –¢–µ—Å—Ç–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç: {test_client['company_name']}\")\n",
    "    print(f\"üí∞ –ë—é–¥–∂–µ—Ç: {test_client['budget_range']} ‚ÇΩ/–º–µ—Å\")\n",
    "    print(f\"üéØ –¶–µ–ª–∏: {test_client['goals'][:60]}...\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "    for agent_id, agent in DEMO_AGENTS.items():\n",
    "        print(f\"\\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {agent.agent_name if hasattr(agent, 'agent_name') else agent_id}...\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            result = await agent.process_task(test_client)\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            processing_time = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            if result.get('success'):\n",
    "                print(f\"  ‚úÖ –£—Å–ø–µ—à–Ω–æ ({processing_time:.2f}—Å)\")\n",
    "                if result.get('tokens_used'):\n",
    "                    print(f\"  üî¢ –¢–æ–∫–µ–Ω—ã: {result['tokens_used']}\")\n",
    "                print(f\"  üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['result'][:100]}...\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  üí• –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2 –∑–∞–≤–µ—Ä—à–µ–Ω\")\n",
    "\n",
    "async def experiment_3_full_workflow():\n",
    "    \"\"\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: –ü–æ–ª–Ω—ã–π workflow —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–æ–≤\"\"\"\n",
    "    print(\"\\nüåü –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: –ü–û–õ–ù–´–ô WORKFLOW\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è workflow\n",
    "    workflow_agents = [\n",
    "        ('lead_qualification', 'Lead Qualification'),\n",
    "        ('technical_seo_auditor', 'Technical SEO Audit'),\n",
    "        ('sales_operations_manager', 'Sales Operations'),\n",
    "        ('chief_seo_strategist', 'Strategic Planning')\n",
    "    ]\n",
    "    \n",
    "    workflow_client = {\n",
    "        \"company_name\": \"Global Manufacturing Corp\",\n",
    "        \"budget_range\": \"5000000\",\n",
    "        \"website\": \"global-manufacturing.com\",\n",
    "        \"goals\": \"–ì–ª–æ–±–∞–ª—å–Ω–∞—è —Ü–∏—Ñ—Ä–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –ª–∏–¥–µ—Ä—Å—Ç–≤–æ –≤ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ\",\n",
    "        \"market_position\": \"Fortune 500 –∫–æ–º–ø–∞–Ω–∏—è\"\n",
    "    }\n",
    "    \n",
    "    print(f\"üè¢ Workflow –∫–ª–∏–µ–Ω—Ç: {workflow_client['company_name']}\")\n",
    "    print(f\"üí∞ –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏: {workflow_client['budget_range']} ‚ÇΩ/–º–µ—Å\")\n",
    "    \n",
    "    workflow_results = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (agent_id, step_name) in enumerate(workflow_agents, 1):\n",
    "        print(f\"\\n{i}Ô∏è‚É£ {step_name}\")\n",
    "        \n",
    "        if agent_id in DEMO_AGENTS:\n",
    "            agent = DEMO_AGENTS[agent_id]\n",
    "            \n",
    "            try:\n",
    "                result = await agent.process_task(workflow_client)\n",
    "                \n",
    "                if result.get('success'):\n",
    "                    print(f\"   ‚úÖ {step_name} –≤—ã–ø–æ–ª–Ω–µ–Ω\")\n",
    "                    if result.get('tokens_used'):\n",
    "                        total_tokens += result['tokens_used']\n",
    "                        print(f\"   üî¢ –¢–æ–∫–µ–Ω—ã: {result['tokens_used']}\")\n",
    "                    workflow_results.append((agent_id, True))\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {step_name} –æ—à–∏–±–∫–∞\")\n",
    "                    workflow_results.append((agent_id, False))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   üí• {step_name} –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(e)}\")\n",
    "                workflow_results.append((agent_id, False))\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è –ê–≥–µ–Ω—Ç {agent_id} –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "            workflow_results.append((agent_id, False))\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ workflow\n",
    "    successful_steps = sum(1 for _, success in workflow_results if success)\n",
    "    print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ WORKFLOW:\")\n",
    "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤: {successful_steps}/{len(workflow_agents)}\")\n",
    "    print(f\"üî¢ –û–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {total_tokens}\")\n",
    "    print(f\"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {successful_steps/len(workflow_agents)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3 –∑–∞–≤–µ—Ä—à–µ–Ω\")\n",
    "\n",
    "def show_project_summary():\n",
    "    \"\"\"–ü–æ–∫–∞–∑–∞—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É –ø—Ä–æ–µ–∫—Ç–∞\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò –ü–†–û–ï–ö–¢–ê\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è –ü–†–û–ï–ö–¢:\")\n",
    "    print(f\"  üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {'‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞' if PROJECT_READY else '‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ'}\")\n",
    "    print(f\"  ü§ñ –ê–≥–µ–Ω—Ç—ã: {LOADED_AGENTS_COUNT}/{PROJECT_AGENTS_COUNT} –∑–∞–≥—Ä—É–∂–µ–Ω–æ\")\n",
    "    print(f\"  üìö RAG —Å–∏—Å—Ç–µ–º–∞: {'‚úÖ –ê–∫—Ç–∏–≤–Ω–∞' if RAG_READY else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}\")\n",
    "    \n",
    "    print(f\"\\nüîå API –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:\")\n",
    "    print(f\"  ü§ñ OpenAI LLM: {'‚úÖ' if OPENAI_READY else '‚ùå'}\")\n",
    "    print(f\"  üîç Embeddings: {'‚úÖ' if EMBEDDINGS_READY else '‚ùå'}\")\n",
    "    \n",
    "    print(f\"\\nüìö –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\")\n",
    "    if RAG_READY:\n",
    "        stats = RAG_SYSTEM.get_stats()\n",
    "        print(f\"  üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {stats['categories']}\")\n",
    "        print(f\"  üìÑ –§–∞–π–ª–æ–≤: {stats['total_files']}\")\n",
    "        print(f\"  üìä –†–∞–∑–º–µ—Ä: {stats['total_content_size']:,} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå –ë–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "    \n",
    "    print(f\"\\nüéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:\")\n",
    "    print(f\"  ‚úÖ RAG –ø–æ–∏—Å–∫ –ø–æ –∑–Ω–∞–Ω–∏—è–º –ø—Ä–æ–µ–∫—Ç–∞\")\n",
    "    print(f\"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏\")\n",
    "    print(f\"  ‚úÖ Enterprise workflow –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\")\n",
    "    print(f\"  ‚úÖ OpenAI API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è\")\n",
    "    print(f\"  ‚úÖ Production-ready –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\")\n",
    "    \n",
    "    overall_readiness = (\n",
    "        (PROJECT_READY and 40) +\n",
    "        (OPENAI_READY and 30) + \n",
    "        (RAG_READY and 20) +\n",
    "        (LOADED_AGENTS_COUNT > 0 and 10)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ –û–ë–©–ê–Ø –ì–û–¢–û–í–ù–û–°–¢–¨: {overall_readiness}% {'üöÄ PRODUCTION READY!' if overall_readiness >= 80 else 'üîÑ DEMO READY' if overall_readiness >= 50 else '‚ö†Ô∏è LIMITED'}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "print(\"üöÄ –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í...\")\n",
    "\n",
    "await experiment_1_rag_search()\n",
    "await experiment_2_agent_processing() \n",
    "await experiment_3_full_workflow()\n",
    "\n",
    "show_project_summary()\n",
    "\n",
    "print(\"\\nüéâ –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´!\")\n",
    "print(\"üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø AI SEO ARCHITECTS –ü–†–û–ï–ö–¢–ê –ì–û–¢–û–í–ê!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Production –ü—Ä–æ–µ–∫—Ç–∞ –ó–∞–≤–µ—Ä—à–µ–Ω–∞\n",
    "\n",
    "### ‚úÖ **–ß—Ç–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ:**\n",
    "\n",
    "#### üèóÔ∏è **–†–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç AI SEO Architects:**\n",
    "- **–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ GitHub** - –≤–µ—Å—å production –∫–æ–¥\n",
    "- **14 —Ä–µ–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤** - –∏–∑ –ø–∞–ø–∫–∏ agents/ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "- **–ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞** - —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ knowledge/\n",
    "- **Production –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - FastAPI, MCP, Orchestrator\n",
    "- **–†–µ–∞–ª—å–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π** - –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "#### üß™ **–ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**\n",
    "1. **RAG –ø–æ–∏—Å–∫** - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–∞–º –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞\n",
    "2. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≥–µ–Ω—Ç–∞–º–∏** - —Ä–µ–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç enterprise –∑–∞–¥–∞—á–∏\n",
    "3. **Full workflow** - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–æ–≤\n",
    "\n",
    "#### üîß **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**\n",
    "- ‚úÖ **OpenAI API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - GPT-4o/GPT-4o-mini + embeddings\n",
    "- ‚úÖ **FAISS –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫** - —Å graceful degradation\n",
    "- ‚úÖ **Async –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "- ‚úÖ **Error handling** - —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –æ—à–∏–±–∫–∞–º\n",
    "- ‚úÖ **Production –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å** - —Ä–µ–∞–ª—å–Ω—ã–π enterprise –∫–æ–¥\n",
    "\n",
    "### üéØ **–î–æ–∫–∞–∑–∞–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è—é—â–∏—Ö:**\n",
    "\n",
    "#### **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –≤ –ø—Ä–æ–µ–∫—Ç–µ:**\n",
    "- ‚úÖ **OpenAI GPT-4o/GPT-4o-mini** –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤–æ –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "- ‚úÖ **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "- ‚úÖ **–†–µ–∞–ª—å–Ω—ã–µ API –≤—ã–∑–æ–≤—ã** —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—Ç–≤–µ—Ç–æ–≤\n",
    "- ‚úÖ **Enterprise –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏** (BANT, MEDDIC, –°–ü–ò–ù)\n",
    "\n",
    "#### **RAG –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã:**\n",
    "- ‚úÖ **FAISS –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞** –¥–ª—è –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤\n",
    "- ‚úÖ **OpenAI embeddings** –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "- ‚úÖ **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã** –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π\n",
    "- ‚úÖ **Production-ready –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** —Å error handling\n",
    "\n",
    "#### **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å:**\n",
    "- ‚úÖ **Enterprise workflow** - –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª SEO-–∞–≥–µ–Ω—Ç—Å—Ç–≤–∞\n",
    "- ‚úÖ **14 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤** —Å domain expertise\n",
    "- ‚úÖ **ROI-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥** —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "- ‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "### üèÜ **–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:**\n",
    "\n",
    "**AI SEO Architects –ø–æ–∫–∞–∑–∞–ª production-ready –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**\n",
    "- ü§ñ **14 –∞–≥–µ–Ω—Ç–æ–≤ —Å LLM** - –∫–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API\n",
    "- üîç **RAG + –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã** - –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞–Ω–∏–π\n",
    "- üìö **–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞** - –∏–∑ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "- üöÄ **Enterprise –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å** - –≥–æ—Ç–æ–≤ –∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\n",
    "\n",
    "---\n",
    "\n",
    "**ü§ñ Powered by Real AI SEO Architects Project | üîç Full RAG + FAISS | üöÄ Production Ready**\n",
    "\n",
    "**–ê–≤—Ç–æ—Ä:** Andrew Popov (a.popov.gv@gmail.com)  \n",
    "**GitHub:** https://github.com/Andrew821667/ai-seo-architects  \n",
    "**–î–∞—Ç–∞:** 13 –∞–≤–≥—É—Å—Ç–∞ 2025\n",
    "\n",
    "*üéØ Real Project Demo | üèóÔ∏è Production Agents | üîç GitHub-based Knowledge*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}