#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG –Ω–æ—É—Ç–±—É–∫–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞
"""

import sys
import os
import asyncio
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, '/Users/andrew/claude/ai-seo-architects')

print("üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô RAG –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
print("=" * 50)

def test_real_faiss():
    """–¢–µ—Å—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ FAISS"""
    try:
        import faiss
        import numpy as np
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π FAISS –∏–Ω–¥–µ–∫—Å
        dimension = 1536
        index = faiss.IndexFlatL2(dimension)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        test_vectors = np.random.random((5, dimension)).astype('float32')
        index.add(test_vectors)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        query_vector = np.random.random((1, dimension)).astype('float32')
        distances, indices = index.search(query_vector, 3)
        
        assert index.ntotal == 5
        assert len(distances[0]) == 3
        assert len(indices[0]) == 3
        
        print("‚úÖ –†–µ–∞–ª—å–Ω—ã–π FAISS —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ FAISS: {e}")
        return False

def test_openai_mock():
    """–¢–µ—Å—Ç OpenAI –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ API –∫–ª—é—á–∞"""
    try:
        import openai
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ (–±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fake –∫–ª—é—á –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞
        fake_client = openai.OpenAI(api_key="fake-key-for-testing")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–±—ä–µ–∫—Ç —Å–æ–∑–¥–∞–ª—Å—è
        assert hasattr(fake_client, 'chat')
        assert hasattr(fake_client, 'embeddings')
        
        print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        return False

def test_async_support():
    """–¢–µ—Å—Ç async –ø–æ–¥–¥–µ—Ä–∂–∫–∏"""
    try:
        import nest_asyncio
        import asyncio
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º nest_asyncio (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ)
        nest_asyncio.apply()
        
        async def test_async_function():
            await asyncio.sleep(0.001)
            return "success"
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —á—Ç–æ async —Ä–∞–±–æ—Ç–∞–µ—Ç
        result = asyncio.run(test_async_function())
        assert result == "success"
        
        print("‚úÖ Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ async –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {e}")
        return False

def test_project_structure():
    """–¢–µ—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    try:
        project_paths = [
            '/Users/andrew/claude/ai-seo-architects/core',
            '/Users/andrew/claude/ai-seo-architects/agents',
            '/Users/andrew/claude/ai-seo-architects/knowledge'
        ]
        
        existing_paths = []
        for path in project_paths:
            if os.path.exists(path):
                existing_paths.append(path)
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(existing_paths)}/{len(project_paths)}")
        for path in existing_paths:
            print(f"  ‚úÖ {path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∏–º –µ—Å—Ç—å –ª–∏ –±–∞–∑–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        key_files = [
            '/Users/andrew/claude/ai-seo-architects/core/base_agent.py',
            '/Users/andrew/claude/ai-seo-architects/CLAUDE.md'
        ]
        
        existing_files = []
        for file_path in key_files:
            if os.path.exists(file_path):
                existing_files.append(file_path)
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(existing_files)}/{len(key_files)}")
        
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
        return False

def test_core_base_agent():
    """–¢–µ—Å—Ç –∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π BaseAgent
        from core.base_agent import BaseAgent
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–ª–∞—Å—Å –∏–º–µ–µ—Ç –Ω—É–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        agent_methods = ['process_task', '__init__']
        for method in agent_methods:
            assert hasattr(BaseAgent, method), f"–ú–µ—Ç–æ–¥ {method} –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        print("‚úÖ BaseAgent –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
    except ImportError as e:
        print(f"‚ö†Ô∏è BaseAgent –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞): {e}")
        return True  # –≠—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–æ—É—Ç–±—É–∫–∞
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ BaseAgent: {e}")
        return False

def test_knowledge_directories():
    """–¢–µ—Å—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π knowledge"""
    try:
        knowledge_base_path = '/Users/andrew/claude/ai-seo-architects/knowledge'
        
        if os.path.exists(knowledge_base_path):
            knowledge_files = os.listdir(knowledge_base_path)
            print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∑–Ω–∞–Ω–∏–π: {len(knowledge_files)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
            for i, file_name in enumerate(knowledge_files[:3]):
                print(f"  üìÑ {file_name}")
        else:
            print("üìö –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è knowledge –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –≤ –Ω–æ—É—Ç–±—É–∫–µ)")
        
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ knowledge: {e}")
        return False

def test_notebook_dependencies():
    """–¢–µ—Å—Ç –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–æ—É—Ç–±—É–∫–∞"""
    try:
        dependencies = [
            'openai',
            'faiss',
            'numpy', 
            'nest_asyncio',
            'pydantic'
        ]
        
        available = []
        missing = []
        
        for dep in dependencies:
            try:
                __import__(dep.replace('-', '_'))
                available.append(dep)
            except ImportError:
                missing.append(dep)
        
        print(f"üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {len(available)}/{len(dependencies)}")
        for dep in available:
            print(f"  ‚úÖ {dep}")
        
        if missing:
            print(f"üì¶ –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
            for dep in missing:
                print(f"  ‚ùå {dep}")
        
        # –ö—Ä–∏—Ç–∏—á–Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–∞–∑–æ–≤—ã–µ –ø–∞–∫–µ—Ç—ã
        critical_missing = [dep for dep in missing if dep in ['numpy', 'openai']]
        
        return len(critical_missing) == 0
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {e}")
        return False

def test_full_rag_pipeline():
    """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ RAG pipeline —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏"""
    try:
        import faiss
        import numpy as np
        from dataclasses import dataclass
        from typing import Dict, Any, List, Optional, Tuple
        
        @dataclass
        class TestKnowledgeChunk:
            content: str
            metadata: Dict[str, Any]
            embedding: Optional[np.ndarray] = None
        
        class TestRAGVectorStore:
            def __init__(self, agent_id: str, embedding_dim: int = 1536):
                self.agent_id = agent_id
                self.embedding_dim = embedding_dim
                self.index = faiss.IndexFlatL2(embedding_dim)  # –†–µ–∞–ª—å–Ω—ã–π FAISS
                self.chunks: List[TestKnowledgeChunk] = []
                self.metadata_store: Dict[int, Dict[str, Any]] = {}
            
            async def add_knowledge(self, content: str, metadata: Dict[str, Any] = None):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º fake embedding (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ—Ç OpenAI)
                fake_embedding = np.random.random(self.embedding_dim).astype('float32')
                chunk = TestKnowledgeChunk(content=content, metadata=metadata or {}, embedding=fake_embedding)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∞–ª—å–Ω—ã–π FAISS –∏–Ω–¥–µ–∫—Å
                chunk_id = len(self.chunks)
                self.index.add(chunk.embedding.reshape(1, -1))
                self.chunks.append(chunk)
                self.metadata_store[chunk_id] = chunk.metadata
                
                return chunk_id
            
            async def search(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict[str, Any]]]:
                if len(self.chunks) == 0:
                    return [("–ù–µ—Ç –∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑–µ", 1.0, {})]
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º fake query embedding
                query_embedding = np.random.random(self.embedding_dim).astype('float32')
                
                # –†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ FAISS
                distances, indices = self.index.search(query_embedding.reshape(1, -1), min(top_k, len(self.chunks)))
                
                results = []
                for distance, idx in zip(distances[0], indices[0]):
                    if idx < len(self.chunks):
                        chunk = self.chunks[idx]
                        similarity = 1.0 / (1.0 + distance)
                        results.append((chunk.content, similarity, chunk.metadata))
                
                return results
        
        async def run_pipeline_test():
            # –°–æ–∑–¥–∞–µ–º RAG —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            store = TestRAGVectorStore("test_agent")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è
            await store.add_knowledge("BANT –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –¥–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–∏–¥–æ–≤", {"category": "sales"})
            await store.add_knowledge("Core Web Vitals –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∞–π—Ç–æ–≤", {"category": "technical"})
            await store.add_knowledge("SEO —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è enterprise", {"category": "strategy"})
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = await store.search("–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏–¥–æ–≤", top_k=2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            assert len(results) >= 1
            assert all(len(result) == 3 for result in results)  # content, similarity, metadata
            
            return True
        
        result = asyncio.run(run_pipeline_test())
        
        if result:
            print("‚úÖ –ü–æ–ª–Ω—ã–π RAG pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏")
            return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ RAG pipeline: {e}")
        return False

def run_integration_tests():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤"""
    tests = [
        ("–†–µ–∞–ª—å–Ω—ã–π FAISS", test_real_faiss),
        ("OpenAI –∫–ª–∏–µ–Ω—Ç", test_openai_mock),
        ("Async –ø–æ–¥–¥–µ—Ä–∂–∫–∞", test_async_support),
        ("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞", test_project_structure),
        ("Core BaseAgent", test_core_base_agent),
        ("Knowledge –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏", test_knowledge_directories),
        ("–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–æ—É—Ç–±—É–∫–∞", test_notebook_dependencies),
        ("–ü–æ–ª–Ω—ã–π RAG pipeline", test_full_rag_pipeline)
    ]
    
    print(f"\nüß™ –ó–ê–ü–£–°–ö {len(tests)} –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–• –¢–ï–°–¢–û–í")
    print("-" * 50)
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\nüî¨ –¢–µ—Å—Ç: {test_name}")
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ {test_name}: {e}")
            failed += 1
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {passed}")
    print(f"‚ùå –ù–µ –ø—Ä–æ–π–¥–µ–Ω–æ: {failed}")
    print(f"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {passed/(passed+failed)*100:.1f}%")
    
    if passed >= len(tests) * 0.8:  # 80% —Ç–µ—Å—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–π—Ç–∏
        print("\nüéâ –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ï –¢–ï–°–¢–´ –í –û–°–ù–û–í–ù–û–ú –ü–†–û–ô–î–ï–ù–´!")
        print("üì± –ù–æ—É—Ç–±—É–∫ –≥–æ—Ç–æ–≤ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤ Google Colab!")
        return True
    else:
        print(f"\n‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {failed}")
        print("üîß –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –≤ Colab")
        return False

if __name__ == "__main__":
    success = run_integration_tests()
    
    print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê:")
    if success:
        print("‚úÖ RAG –Ω–æ—É—Ç–±—É–∫ –≥–æ—Ç–æ–≤ –∫ production —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤ Google Colab")
        print("üöÄ –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ —Ä–∞–±–æ—Ç–∞—é—Ç")
        print("üì± –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ Colab –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º OpenAI API")
    else:
        print("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
        print("üîß –ù–æ—É—Ç–±—É–∫ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å")
    
    sys.exit(0 if success else 1)