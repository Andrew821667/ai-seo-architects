"""
Content Strategy Agent - –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è SEO
Operational Level Agent
"""

import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
import logging

from core.base_agent import BaseAgent

logger = logging.getLogger(__name__)


class ContentStrategyAgent(BaseAgent):
    """
    Content Strategy Agent - Operational —É—Ä–æ–≤–µ–Ω—å
    
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: Keyword research, –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, E-E-A-T –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è,
    TF-IDF –∞–Ω–∞–ª–∏–∑, —Ç–æ–ø–∏–∫–æ–≤–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, content calendar –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    
    def __init__(self, data_provider, **kwargs):
        super().__init__(
            agent_id="content_strategy_agent",
            name="Content Strategy Agent",
            data_provider=data_provider,
            **kwargs
        )
        
        # Content strategy configuration
        self.content_config = {
            "keyword_research_depth": "comprehensive",  # basic, standard, comprehensive
            "content_quality_threshold": 85,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π E-E-A-T score
            "topic_cluster_size": 15,  # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            "content_calendar_horizon": 90,  # –î–Ω–µ–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–ø–µ—Ä–µ–¥
            "competitor_analysis_depth": 5,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            "industry_expertise_level": "expert"  # basic, intermediate, expert
        }
        
        # Supported industries
        self.industry_specialization = [
            "fintech", "ecommerce", "saas", "healthcare", 
            "real_estate", "education", "manufacturing", "consulting"
        ]
        
        # Content types expertise
        self.content_types = [
            "blog_posts", "pillar_pages", "landing_pages", "product_pages",
            "category_pages", "guides", "whitepapers", "case_studies",
            "infographics", "videos", "podcasts", "webinars"
        ]
        
        logger.info(f"üéØ Content Strategy Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"üìä Keyword Research: {self.content_config['keyword_research_depth']}")
        logger.info(f"üéØ Quality Threshold: {self.content_config['content_quality_threshold']}+ E-E-A-T")
        logger.info(f"üìÖ Planning Horizon: {self.content_config['content_calendar_horizon']} –¥–Ω–µ–π")
        logger.info(f"üè≠ Industries: {len(self.industry_specialization)} –≤–µ—Ä—Ç–∏–∫–∞–ª–µ–π")
    
    async def process_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á Content Strategy Agent
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á:
        - keyword_research: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        - content_audit: –ê—É–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        - content_strategy: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        - topic_clustering: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º
        - content_calendar: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–∞–ª–µ–Ω–¥–∞—Ä—è
        - competitor_content_analysis: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        - eeat_optimization: E-E-A-T –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        """
        
        start_time = datetime.now()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
            input_data = task_data.get("input_data", {})
            task_type = input_data.get("task_type", "content_strategy")
            
            logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ Content Strategy –∑–∞–¥–∞—á–∏: {task_type}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if task_type == "keyword_research":
                result = await self._process_keyword_research(input_data)
            elif task_type == "content_audit":
                result = await self._process_content_audit(input_data)
            elif task_type == "content_strategy":
                result = await self._process_content_strategy(input_data)
            elif task_type == "topic_clustering":
                result = await self._process_topic_clustering(input_data)
            elif task_type == "content_calendar":
                result = await self._process_content_calendar(input_data)
            elif task_type == "competitor_content_analysis":
                result = await self._process_competitor_analysis(input_data)
            elif task_type == "eeat_optimization":
                result = await self._process_eeat_optimization(input_data)
            else:
                # Default: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
                result = await self._process_comprehensive_strategy(input_data)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.metrics.record_task(True, processing_time)
            
            logger.info(f"‚úÖ Content Strategy –∑–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å")
            
            return {
                "success": True,
                "agent": self.agent_id,
                "task_type": task_type,
                "result": result,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat(),
                "content_quality": self._assess_content_quality(result),
                "strategy_confidence": self._calculate_strategy_confidence(result)
            }
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self.metrics.record_task(False, processing_time)
            
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Content Strategy Agent: {str(e)}")
            
            return {
                "success": False,
                "agent": self.agent_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "processing_time": processing_time
            }
    
    async def _process_keyword_research(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        
        domain = input_data.get("domain", "example.com")
        industry = input_data.get("industry", "general")
        seed_keywords = input_data.get("seed_keywords", [])
        research_depth = input_data.get("depth", self.content_config["keyword_research_depth"])
        
        logger.info(f"üîç Keyword research –¥–ª—è {domain} –≤ {industry}")
        
        # –°–∏–º—É–ª—è—Ü–∏—è comprehensive keyword research
        if research_depth == "comprehensive":
            primary_keywords = self._generate_primary_keywords(industry, seed_keywords, 50)
            long_tail_keywords = self._generate_long_tail_keywords(primary_keywords, 200)
            question_keywords = self._generate_question_keywords(primary_keywords, 100)
            commercial_keywords = self._generate_commercial_keywords(industry, 75)
        else:
            primary_keywords = self._generate_primary_keywords(industry, seed_keywords, 25)
            long_tail_keywords = self._generate_long_tail_keywords(primary_keywords, 100)
            question_keywords = self._generate_question_keywords(primary_keywords, 50)
            commercial_keywords = self._generate_commercial_keywords(industry, 35)
        
        return {
            "keyword_research_results": {
                "primary_keywords": primary_keywords,
                "long_tail_keywords": long_tail_keywords,
                "question_keywords": question_keywords,
                "commercial_keywords": commercial_keywords,
                "total_keywords": len(primary_keywords) + len(long_tail_keywords) + len(question_keywords) + len(commercial_keywords),
                "research_depth": research_depth,
                "industry_focus": industry
            },
            "keyword_difficulty_analysis": self._analyze_keyword_difficulty(primary_keywords),
            "search_intent_mapping": self._map_search_intent(primary_keywords + long_tail_keywords),
            "content_opportunities": self._identify_content_opportunities(primary_keywords, industry)
        }
    
    async def _process_content_strategy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        domain = input_data.get("domain", "example.com")
        industry = input_data.get("industry", "general")
        business_goals = input_data.get("business_goals", ["traffic_growth", "lead_generation"])
        target_audience = input_data.get("target_audience", {})
        content_budget = input_data.get("monthly_budget", 100000)  # —Ä—É–±–ª–µ–π
        
        logger.info(f"üìã Content strategy –¥–ª—è {domain}, –±—é–¥–∂–µ—Ç: {content_budget:,} ‚ÇΩ")
        
        # Content pillars –¥–ª—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∏
        content_pillars = self._define_content_pillars(industry, business_goals)
        
        # Content types recommendation
        recommended_content_types = self._recommend_content_types(industry, content_budget, business_goals)
        
        # Publishing frequency
        publishing_schedule = self._calculate_publishing_schedule(content_budget, industry)
        
        # ROI projection
        roi_projection = self._calculate_content_roi_projection(content_budget, industry, publishing_schedule)
        
        return {
            "content_strategy": {
                "industry": industry,
                "monthly_budget": content_budget,
                "content_pillars": content_pillars,
                "recommended_content_types": recommended_content_types,
                "publishing_schedule": publishing_schedule,
                "roi_projection": roi_projection
            },
            "implementation_roadmap": self._create_implementation_roadmap(content_pillars, publishing_schedule),
            "success_metrics": self._define_success_metrics(business_goals),
            "resource_requirements": self._calculate_resource_requirements(publishing_schedule, content_budget)
        }
    
    async def _process_topic_clustering(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        keywords = input_data.get("keywords", [])
        industry = input_data.get("industry", "general")
        cluster_strategy = input_data.get("cluster_strategy", "topic_based")  # topic_based, intent_based, funnel_based
        
        logger.info(f"üóÇÔ∏è Topic clustering: {len(keywords)} keywords, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {cluster_strategy}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Ç–µ–º
        if cluster_strategy == "topic_based":
            clusters = self._create_topic_based_clusters(keywords, industry)
        elif cluster_strategy == "intent_based":
            clusters = self._create_intent_based_clusters(keywords)
        else:  # funnel_based
            clusters = self._create_funnel_based_clusters(keywords, industry)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        prioritized_clusters = self._prioritize_clusters(clusters, industry)
        
        return {
            "topic_clusters": {
                "total_clusters": len(clusters),
                "clustering_strategy": cluster_strategy,
                "clusters": prioritized_clusters
            },
            "content_hub_recommendations": self._recommend_content_hubs(prioritized_clusters),
            "internal_linking_strategy": self._design_internal_linking_strategy(prioritized_clusters),
            "pillar_page_opportunities": self._identify_pillar_page_opportunities(prioritized_clusters)
        }
    
    async def _process_content_calendar(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–∞–ª–µ–Ω–¥–∞—Ä—è"""
        
        content_strategy = input_data.get("content_strategy", {})
        topic_clusters = input_data.get("topic_clusters", [])
        publishing_frequency = input_data.get("publishing_frequency", "weekly")
        calendar_horizon = input_data.get("calendar_horizon", self.content_config["content_calendar_horizon"])
        
        logger.info(f"üìÖ Content calendar –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {calendar_horizon} –¥–Ω–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞–ª–µ–Ω–¥–∞—Ä—å –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        content_calendar = self._generate_content_calendar(
            topic_clusters, publishing_frequency, calendar_horizon
        )
        
        # Seasonal content opportunities
        seasonal_content = self._identify_seasonal_opportunities(calendar_horizon)
        
        # Content distribution plan
        distribution_plan = self._create_distribution_plan(content_calendar)
        
        return {
            "content_calendar": {
                "planning_horizon_days": calendar_horizon,
                "publishing_frequency": publishing_frequency,
                "total_planned_content": len(content_calendar),
                "calendar": content_calendar
            },
            "seasonal_opportunities": seasonal_content,
            "distribution_plan": distribution_plan,
            "resource_allocation": self._calculate_calendar_resource_allocation(content_calendar)
        }
    
    async def _process_eeat_optimization(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """E-E-A-T –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        content_audit = input_data.get("content_audit", {})
        industry = input_data.get("industry", "general")
        expertise_level = input_data.get("target_expertise_level", "expert")
        
        logger.info(f"üéì E-E-A-T –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è {industry}, —É—Ä–æ–≤–µ–Ω—å: {expertise_level}")
        
        # E-E-A-T assessment
        current_eeat_score = self._assess_current_eeat_score(content_audit, industry)
        
        # Improvement recommendations
        eeat_improvements = self._recommend_eeat_improvements(current_eeat_score, expertise_level, industry)
        
        # Author strategy
        author_strategy = self._develop_author_strategy(industry, expertise_level)
        
        # Content credibility enhancements
        credibility_enhancements = self._recommend_credibility_enhancements(industry)
        
        return {
            "eeat_optimization": {
                "current_eeat_score": current_eeat_score,
                "target_expertise_level": expertise_level,
                "improvement_priority": eeat_improvements["priority_areas"],
                "implementation_plan": eeat_improvements["implementation_plan"]
            },
            "author_strategy": author_strategy,
            "credibility_enhancements": credibility_enhancements,
            "success_metrics": self._define_eeat_success_metrics(expertise_level)
        }
    
    async def _process_comprehensive_strategy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (default processing)"""
        
        domain = input_data.get("domain", "example.com")
        industry = input_data.get("industry", "general")
        
        logger.info(f"üìä Comprehensive content strategy –¥–ª—è {domain}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        keyword_research = await self._process_keyword_research(input_data)
        content_strategy = await self._process_content_strategy(input_data)
        topic_clustering = await self._process_topic_clustering({
            "keywords": keyword_research["keyword_research_results"]["primary_keywords"],
            "industry": industry
        })
        content_calendar = await self._process_content_calendar({
            "topic_clusters": topic_clustering["topic_clusters"]["clusters"][:10],
            "publishing_frequency": "bi-weekly"
        })
        
        return {
            "comprehensive_strategy": {
                "domain": domain,
                "industry": industry,
                "strategy_scope": "comprehensive"
            },
            "keyword_research": keyword_research,
            "content_strategy": content_strategy,
            "topic_clustering": topic_clustering,
            "content_calendar": content_calendar,
            "implementation_priority": self._prioritize_implementation_steps(
                keyword_research, content_strategy, topic_clustering
            )
        }
    
    def _generate_primary_keywords(self, industry: str, seed_keywords: List[str], count: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è primary keywords"""
        
        # Industry-specific keyword patterns
        industry_patterns = {
            "fintech": ["—Ñ–∏–Ω—Ç–µ—Ö", "—Ü–∏—Ñ—Ä–æ–≤–æ–π –±–∞–Ω–∫", "–º–æ–±–∏–ª—å–Ω—ã–π –±–∞–Ω–∫–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"],
            "ecommerce": ["–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–∞–≥–∞–∑–∏–Ω", "–æ–Ω–ª–∞–π–Ω –ø–æ–∫—É–ø–∫–∏", "–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ç–æ–≤–∞—Ä—ã", "—Å–∫–∏–¥–∫–∏"],
            "saas": ["–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ", "–æ–±–ª–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "crm —Å–∏—Å—Ç–µ–º–∞"],
            "healthcare": ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–∑–¥–æ—Ä–æ–≤—å–µ", "–ª–µ—á–µ–Ω–∏–µ", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–∫–ª–∏–Ω–∏–∫–∞"],
            "real_estate": ["–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "–∫–≤–∞—Ä—Ç–∏—Ä—ã", "–¥–æ–º–∞", "–∏–ø–æ—Ç–µ–∫–∞", "–∞—Ä–µ–Ω–¥–∞"],
            "education": ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ", "–∫—É—Ä—Å—ã", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "—à–∫–æ–ª–∞"],
            "general": ["—É—Å–ª—É–≥–∏", "–∫–æ–º–ø–∞–Ω–∏—è", "—Ä–µ—à–µ–Ω–∏—è", "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏", "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã"]
        }
        
        base_patterns = industry_patterns.get(industry, industry_patterns["general"])
        
        keywords = []
        for i, pattern in enumerate(base_patterns[:count]):
            keywords.append({
                "keyword": pattern,
                "search_volume": 10000 - (i * 500),
                "difficulty": 45 + (i * 3),
                "cpc": 150 + (i * 25),
                "intent": "informational" if i % 3 == 0 else "commercial",
                "priority": "high" if i < 10 else "medium"
            })
        
        return keywords
    
    def _generate_long_tail_keywords(self, primary_keywords: List[Dict[str, Any]], count: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è long-tail keywords"""
        
        long_tail = []
        modifiers = ["–∫–∞–∫", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–ª—É—á—à–∏–π", "—Ç–æ–ø", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "–æ—Ç–∑—ã–≤—ã", "—Ü–µ–Ω–∞"]
        
        for i, primary in enumerate(primary_keywords[:count//8]):
            for j, modifier in enumerate(modifiers):
                if len(long_tail) >= count:
                    break
                long_tail.append({
                    "keyword": f"{modifier} {primary['keyword']}",
                    "search_volume": max(100, primary["search_volume"] // 10),
                    "difficulty": max(15, primary["difficulty"] - 20),
                    "cpc": max(50, primary["cpc"] // 2),
                    "intent": "informational" if modifier in ["–∫–∞–∫", "—á—Ç–æ —Ç–∞–∫–æ–µ"] else "commercial",
                    "priority": "medium",
                    "parent_keyword": primary["keyword"]
                })
        
        return long_tail[:count]
    
    def _generate_question_keywords(self, primary_keywords: List[Dict[str, Any]], count: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è question-based keywords"""
        
        question_keywords = []
        question_patterns = ["–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å", "—á—Ç–æ –ª—É—á—à–µ", "–ø–æ—á–µ–º—É", "–≥–¥–µ –Ω–∞–π—Ç–∏", "–∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å"]
        
        for i, primary in enumerate(primary_keywords[:count//5]):
            for pattern in question_patterns:
                if len(question_keywords) >= count:
                    break
                question_keywords.append({
                    "keyword": f"{pattern} {primary['keyword']}",
                    "search_volume": max(50, primary["search_volume"] // 20),
                    "difficulty": max(10, primary["difficulty"] - 25),
                    "cpc": max(30, primary["cpc"] // 3),
                    "intent": "informational",
                    "priority": "high",  # Question-based content —á–∞—Å—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç—Å—è
                    "content_type": "faq_or_guide",
                    "parent_keyword": primary["keyword"]
                })
        
        return question_keywords[:count]
    
    def _generate_commercial_keywords(self, industry: str, count: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è commercial intent keywords"""
        
        commercial_patterns = {
            "fintech": ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–ø–æ–¥–∫–ª—é—á–∏—Ç—å"],
            "ecommerce": ["–∑–∞–∫–∞–∑–∞—Ç—å", "–∫—É–ø–∏—Ç—å", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–∫–∞—Ç–∞–ª–æ–≥"],
            "saas": ["–ø–æ–¥–ø–∏—Å–∫–∞", "–¥–µ–º–æ", "–ø—Ä–æ–±–Ω–∞—è –≤–µ—Ä—Å–∏—è", "—Ü–µ–Ω–∞", "—Ç–∞—Ä–∏—Ñ"],
            "general": ["–∑–∞–∫–∞–∑–∞—Ç—å", "–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "—É—Å–ª—É–≥–∏", "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è"]
        }
        
        patterns = commercial_patterns.get(industry, commercial_patterns["general"])
        
        commercial_keywords = []
        for i, pattern in enumerate(patterns):
            for j in range(count // len(patterns)):
                commercial_keywords.append({
                    "keyword": f"{pattern} {industry}",
                    "search_volume": 2000 - (i * 100),
                    "difficulty": 55 + (i * 5),
                    "cpc": 300 + (i * 50),
                    "intent": "commercial",
                    "priority": "high",
                    "conversion_potential": "high"
                })
        
        return commercial_keywords[:count]
    
    def _analyze_keyword_difficulty(self, keywords: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        
        difficulty_distribution = {"easy": 0, "medium": 0, "hard": 0}
        total_keywords = len(keywords)
        
        for keyword in keywords:
            difficulty = keyword.get("difficulty", 50)
            if difficulty < 30:
                difficulty_distribution["easy"] += 1
            elif difficulty < 60:
                difficulty_distribution["medium"] += 1
            else:
                difficulty_distribution["hard"] += 1
        
        return {
            "difficulty_distribution": difficulty_distribution,
            "average_difficulty": sum(kw.get("difficulty", 50) for kw in keywords) / total_keywords,
            "quick_wins": [kw for kw in keywords if kw.get("difficulty", 50) < 30],
            "competitive_keywords": [kw for kw in keywords if kw.get("difficulty", 50) > 70],
            "recommendations": self._generate_difficulty_recommendations(difficulty_distribution)
        }
    
    def _generate_difficulty_recommendations(self, difficulty_distribution: Dict[str, int]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        recommendations = []
        
        total_keywords = sum(difficulty_distribution.values())
        if total_keywords == 0:
            return ["–î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"]
        
        easy_percentage = difficulty_distribution.get("easy", 0) / total_keywords
        medium_percentage = difficulty_distribution.get("medium", 0) / total_keywords  
        hard_percentage = difficulty_distribution.get("hard", 0) / total_keywords
        
        if easy_percentage > 0.6:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–æ—Å—Ç–∞")
        elif easy_percentage < 0.2:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –Ω–∏–∑–∫–æ–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–æ–±–µ–¥")
        
        if hard_percentage > 0.4:
            recommendations.append("–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è —Å–Ω–∞—á–∞–ª–∞ –Ω–∞ –º–µ–Ω–µ–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö")
        
        if medium_percentage < 0.3:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞")
        
        return recommendations if recommendations else ["–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"]
    
    def _map_search_intent(self, keywords: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ú–∞–ø–ø–∏–Ω–≥ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤"""
        
        intent_mapping = {
            "informational": [],
            "commercial": [],
            "navigational": [],
            "transactional": []
        }
        
        for keyword in keywords:
            intent = keyword.get("intent", "informational")
            if intent in intent_mapping:
                intent_mapping[intent].append(keyword)
            else:
                intent_mapping["informational"].append(keyword)
        
        return {
            "intent_distribution": {intent: len(kws) for intent, kws in intent_mapping.items()},
            "intent_mapping": intent_mapping,
            "content_recommendations": self._generate_intent_based_content_recommendations(intent_mapping)
        }
    
    def _generate_intent_based_content_recommendations(self, intent_mapping: Dict[str, List]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤"""
        recommendations = []
        
        total_keywords = sum(len(keywords) for keywords in intent_mapping.values())
        if total_keywords == 0:
            return ["–î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ç–µ–Ω—Ç–æ–≤"]
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        informational_pct = len(intent_mapping.get("informational", [])) / total_keywords
        commercial_pct = len(intent_mapping.get("commercial", [])) / total_keywords
        transactional_pct = len(intent_mapping.get("transactional", [])) / total_keywords
        navigational_pct = len(intent_mapping.get("navigational", [])) / total_keywords
        
        if informational_pct > 0.6:
            recommendations.append("–°–æ–∑–¥–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        elif informational_pct < 0.2:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π")
        
        if commercial_pct < 0.2:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        if transactional_pct < 0.15:
            recommendations.append("–°–æ–∑–¥–∞—Ç—å –±–æ–ª—å—à–µ –ø—Ä–æ–¥–∞—é—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ª–µ–Ω–¥–∏–Ω–≥–æ–≤")
        
        if navigational_pct > 0.3:
            recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±—Ä–µ–Ω–¥–∏–Ω–≥ –∏ —É–∑–Ω–∞–≤–∞–µ–º–æ—Å—Ç—å")
        
        return recommendations if recommendations else ["–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Ç–æ–≤"]
    
    def _identify_content_opportunities(self, keywords: List[Dict[str, Any]], industry: str) -> List[Dict[str, Any]]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
        
        opportunities = []
        
        # High-volume, low-difficulty opportunities
        for keyword in keywords:
            if keyword.get("search_volume", 0) > 5000 and keyword.get("difficulty", 100) < 40:
                opportunities.append({
                    "type": "quick_win",
                    "keyword": keyword["keyword"],
                    "opportunity_score": keyword["search_volume"] / keyword["difficulty"],
                    "recommended_content_type": "blog_post",
                    "priority": "high"
                })
        
        # Question-based content opportunities
        question_keywords = [kw for kw in keywords if any(q in kw["keyword"].lower() 
                           for q in ["–∫–∞–∫", "—á—Ç–æ", "–ø–æ—á–µ–º—É", "–≥–¥–µ", "–∫–æ–≥–¥–∞"])]
        
        for keyword in question_keywords[:10]:
            opportunities.append({
                "type": "faq_content",
                "keyword": keyword["keyword"], 
                "opportunity_score": keyword.get("search_volume", 100) * 2,  # FAQ –∫–æ–Ω—Ç–µ–Ω—Ç —á–∞—Å—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç—Å—è
                "recommended_content_type": "faq_or_guide",
                "priority": "medium"
            })
        
        return sorted(opportunities, key=lambda x: x["opportunity_score"], reverse=True)[:20]
    
    def _define_content_pillars(self, industry: str, business_goals: List[str]) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Å—Ç–æ–ª–ø–æ–≤"""
        
        industry_pillars = {
            "fintech": [
                {"name": "–¶–∏—Ñ—Ä–æ–≤–æ–π –±–∞–Ω–∫–∏–Ω–≥", "description": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è"},
                {"name": "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –∏ —Ñ–∏–Ω–∞–Ω—Å—ã", "description": "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"},
                {"name": "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö", "description": "–§–∏–Ω—Ç–µ—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"},
                {"name": "–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "description": "–§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ"}
            ],
            "ecommerce": [
                {"name": "–¢–æ–≤–∞—Ä—ã –∏ –∫–∞—Ç–∞–ª–æ–≥", "description": "–ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"},
                {"name": "–ü–æ–∫—É–ø–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç", "description": "UX –∏ —Å–µ—Ä–≤–∏—Å"},
                {"name": "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ –ø—Ä–æ–¥–∞–∂–∏", "description": "–ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤"},
                {"name": "–õ–æ–≥–∏—Å—Ç–∏–∫–∞", "description": "–î–æ—Å—Ç–∞–≤–∫–∞ –∏ fulfillment"}
            ],
            "general": [
                {"name": "–≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞", "description": "–û—Ç—Ä–∞—Å–ª–µ–≤–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞"},
                {"name": "–†–µ—à–µ–Ω–∏—è", "description": "–ü—Ä–æ–¥—É–∫—Ç—ã –∏ —É—Å–ª—É–≥–∏"},
                {"name": "–ö–µ–π—Å—ã", "description": "–ò—Å—Ç–æ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞"},
                {"name": "–¢—Ä–µ–Ω–¥—ã", "description": "–û—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏"}
            ]
        }
        
        pillars = industry_pillars.get(industry, industry_pillars["general"])
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ–¥ –±–∏–∑–Ω–µ—Å-—Ü–µ–ª–∏
        for pillar in pillars:
            if "lead_generation" in business_goals:
                pillar["lead_generation_potential"] = "high"
            if "brand_awareness" in business_goals:
                pillar["brand_building_potential"] = "high"
        
        return pillars
    
    def _recommend_content_types(self, industry: str, budget: int, goals: List[str]) -> List[Dict[str, Any]]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # Budget-based recommendations
        if budget < 50000:  # Small budget
            recommended_types = [
                {"type": "blog_posts", "frequency": "weekly", "investment": 30000},
                {"type": "social_media", "frequency": "daily", "investment": 15000}
            ]
        elif budget < 200000:  # Medium budget
            recommended_types = [
                {"type": "blog_posts", "frequency": "bi-weekly", "investment": 60000},
                {"type": "guides", "frequency": "monthly", "investment": 40000},
                {"type": "infographics", "frequency": "bi-weekly", "investment": 30000},
                {"type": "videos", "frequency": "monthly", "investment": 50000}
            ]
        else:  # Large budget
            recommended_types = [
                {"type": "blog_posts", "frequency": "daily", "investment": 80000},
                {"type": "pillar_pages", "frequency": "weekly", "investment": 60000},
                {"type": "whitepapers", "frequency": "monthly", "investment": 40000},
                {"type": "webinars", "frequency": "bi-weekly", "investment": 50000},
                {"type": "case_studies", "frequency": "weekly", "investment": 30000}
            ]
        
        # Goal-based adjustments
        for content_type in recommended_types:
            if "lead_generation" in goals:
                content_type["lead_gen_score"] = 8 if content_type["type"] in ["whitepapers", "webinars"] else 5
            if "brand_awareness" in goals:
                content_type["brand_score"] = 9 if content_type["type"] in ["videos", "infographics"] else 6
        
        return recommended_types
    
    def _calculate_publishing_schedule(self, budget: int, industry: str) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""
        
        # Budget-based frequency calculation
        if budget < 50000:
            weekly_posts = 1
            monthly_premium = 0
        elif budget < 100000:
            weekly_posts = 2
            monthly_premium = 1
        elif budget < 200000:
            weekly_posts = 3
            monthly_premium = 2
        else:
            weekly_posts = 5
            monthly_premium = 4
        
        return {
            "weekly_blog_posts": weekly_posts,
            "monthly_premium_content": monthly_premium,
            "quarterly_major_content": max(1, monthly_premium // 2),
            "daily_social_content": min(7, budget // 10000),
            "total_monthly_pieces": (weekly_posts * 4) + monthly_premium,
            "annual_content_pieces": ((weekly_posts * 4) + monthly_premium) * 12
        }
    
    def _calculate_content_roi_projection(self, budget: int, industry: str, schedule: Dict[str, Any]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç ROI –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # Industry multipliers
        industry_multipliers = {
            "fintech": 3.2,
            "saas": 4.1,
            "ecommerce": 2.8,
            "healthcare": 3.5,
            "general": 2.5
        }
        
        multiplier = industry_multipliers.get(industry, 2.5)
        annual_budget = budget * 12
        
        # ROI calculation based on content volume and industry
        monthly_content_pieces = schedule.get("total_monthly_pieces", 4)
        content_scaling_factor = min(2.0, 1 + (monthly_content_pieces - 4) * 0.1)
        
        projected_annual_roi = annual_budget * multiplier * content_scaling_factor
        
        return {
            "annual_investment": annual_budget,
            "projected_annual_return": projected_annual_roi,
            "roi_multiplier": projected_annual_roi / annual_budget,
            "monthly_roi_projection": projected_annual_roi / 12,
            "payback_months": max(6, 12 / (projected_annual_roi / annual_budget)),
            "confidence_level": min(85, 60 + (content_scaling_factor * 15))
        }
    
    def _assess_content_quality(self, result: Dict[str, Any]) -> str:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        quality_score = 0
        max_score = 100
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if "keyword_research" in result:
            quality_score += 25
        if "content_strategy" in result:
            quality_score += 25  
        if "topic_clusters" in result:
            quality_score += 20
        if "content_calendar" in result:
            quality_score += 15
        if "eeat_optimization" in result:
            quality_score += 15
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É
        if quality_score >= 85:
            return "comprehensive"
        elif quality_score >= 65:
            return "good"
        elif quality_score >= 45:
            return "basic"
        else:
            return "incomplete"
    
    def _calculate_strategy_confidence(self, result: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        confidence = 0.6  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if "keyword_research" in result:
            keyword_count = result["keyword_research"].get("keyword_research_results", {}).get("total_keywords", 0)
            confidence += min(0.2, keyword_count / 1000)
        
        if "content_strategy" in result:
            budget = result["content_strategy"].get("content_strategy", {}).get("monthly_budget", 0)
            confidence += min(0.15, budget / 1000000)  # –ë–æ–ª—å—à–µ –±—é–¥–∂–µ—Ç = –±–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        if "topic_clusters" in result:
            cluster_count = result["topic_clusters"].get("topic_clusters", {}).get("total_clusters", 0)
            confidence += min(0.1, cluster_count / 50)
        
        return min(0.95, confidence)
    
    # Helper methods –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    def _create_topic_based_clusters(self, keywords: List[Dict[str, Any]], industry: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º"""
        # Simplified clustering logic
        clusters = []
        cluster_size = self.content_config["topic_cluster_size"]
        
        for i in range(0, len(keywords), cluster_size):
            cluster_keywords = keywords[i:i+cluster_size]
            if cluster_keywords:
                clusters.append({
                    "cluster_id": f"cluster_{len(clusters)+1}",
                    "main_topic": cluster_keywords[0]["keyword"],
                    "keywords": cluster_keywords,
                    "keyword_count": len(cluster_keywords),
                    "total_search_volume": sum(kw.get("search_volume", 0) for kw in cluster_keywords),
                    "avg_difficulty": sum(kw.get("difficulty", 50) for kw in cluster_keywords) / len(cluster_keywords),
                    "content_priority": "high" if i < cluster_size * 3 else "medium"
                })
        
        return clusters
    
    def _prioritize_clusters(self, clusters: List[Dict[str, Any]], industry: str) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ (–æ–±—ä–µ–º –ø–æ–∏—Å–∫–∞ / —Å–ª–æ–∂–Ω–æ—Å—Ç—å)
        for cluster in clusters:
            total_volume = cluster.get("total_search_volume", 1)
            avg_difficulty = cluster.get("avg_difficulty", 50)
            cluster["priority_score"] = total_volume / max(avg_difficulty, 1)
        
        return sorted(clusters, key=lambda x: x["priority_score"], reverse=True)
    
    def _generate_content_calendar(self, clusters: List[Dict[str, Any]], frequency: str, horizon: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–∞–ª–µ–Ω–¥–∞—Ä—è"""
        
        calendar = []
        frequency_map = {"daily": 1, "weekly": 7, "bi-weekly": 14, "monthly": 30}
        interval = frequency_map.get(frequency, 7)
        
        for i, cluster in enumerate(clusters):
            if i * interval < horizon:
                calendar.append({
                    "date": f"Day {(i * interval) + 1}",
                    "content_title": f"Content for {cluster.get('main_topic', f'Topic {i+1}')}",
                    "cluster_id": cluster.get("cluster_id", f"cluster_{i+1}"),
                    "content_type": "blog_post",
                    "target_keywords": cluster.get("keywords", [])[:5],  # Top 5 keywords
                    "estimated_effort": "4-6 hours",
                    "priority": cluster.get("content_priority", "medium")
                })
        
        return calendar[:horizon//7]  # Limit to reasonable number
    
    # Additional helper methods
    def _create_implementation_roadmap(self, pillars: List[Dict], schedule: Dict) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ roadmap'–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏"""
        return [
            {"phase": "Phase 1", "duration": "Month 1-2", "focus": "Keyword research & content pillars setup"},
            {"phase": "Phase 2", "duration": "Month 3-4", "focus": "Core content creation & optimization"},
            {"phase": "Phase 3", "duration": "Month 5-6", "focus": "Content distribution & performance optimization"}
        ]
    
    def _define_success_metrics(self, goals: List[str]) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —É—Å–ø–µ—Ö–∞"""
        return {
            "traffic_growth": "25% increase in 6 months",
            "keyword_rankings": "50% of target keywords in top 10",
            "engagement_metrics": "15% increase in time on page",
            "conversion_metrics": "10% increase in organic conversions"
        }
    
    def _calculate_resource_requirements(self, schedule: Dict, budget: int) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ä–µ—Å—É—Ä—Å–∞–º"""
        return {
            "content_writers": max(1, schedule.get("weekly_blog_posts", 1) // 2),
            "editors": 1,
            "designers": 1 if budget > 100000 else 0.5,
            "monthly_budget_allocation": {
                "writing": budget * 0.4,
                "design": budget * 0.3,
                "promotion": budget * 0.3
            }
        }
    
    def _recommend_content_hubs(self, clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Ö–∞–±–æ–≤"""
        content_hubs = []
        
        for cluster in clusters[:5]:  # –¢–æ–ø-5 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Ö–∞–±–æ–≤
            hub_keywords = cluster.get("keywords", [])
            if len(hub_keywords) >= 3:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ö–∞–±–∞
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ö–∞–±–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ–Ω—Ç–æ–≤
                hub_type = self._determine_hub_type(hub_keywords)
                
                content_hubs.append({
                    "hub_name": cluster.get("cluster_name", f"Hub {len(content_hubs) + 1}"),
                    "hub_type": hub_type,
                    "main_keywords": hub_keywords[:10],
                    "estimated_traffic_potential": sum(kw.get("search_volume", 0) for kw in hub_keywords),
                    "content_pieces_needed": min(max(len(hub_keywords) // 2, 5), 20),
                    "pillar_page_topic": cluster.get("cluster_name", ""),
                    "supporting_content": [
                        f"–ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {kw.get('keyword', '')}" 
                        for kw in hub_keywords[:3]
                    ],
                    "internal_linking_opportunities": len(hub_keywords) * 2,
                    "recommended_update_frequency": hub_type == "evergreen" and "monthly" or "weekly"
                })
        
        return content_hubs
    
    def _determine_hub_type(self, keywords: List[Dict[str, Any]]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ö–∞–±–∞"""
        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        informational_count = sum(1 for kw in keywords if kw.get("intent", "").lower() == "informational")
        commercial_count = sum(1 for kw in keywords if kw.get("intent", "").lower() == "commercial")
        transactional_count = sum(1 for kw in keywords if kw.get("intent", "").lower() == "transactional")
        
        total_keywords = len(keywords)
        if total_keywords == 0:
            return "mixed"
        
        info_pct = informational_count / total_keywords
        commercial_pct = commercial_count / total_keywords
        
        if info_pct > 0.6:
            return "educational"
        elif commercial_pct > 0.4:
            return "product_focused" 
        elif transactional_count > 0:
            return "conversion_focused"
        else:
            return "evergreen"
    
    def _design_internal_linking_strategy(self, clusters: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏"""
        
        linking_strategy = {
            "hub_to_spoke_links": [],
            "cross_cluster_opportunities": [],
            "anchor_text_suggestions": [],
            "linking_depth_strategy": "3-click rule implementation",
            "estimated_link_equity_flow": {}
        }
        
        # Hub-to-spoke linking –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        for cluster in clusters:
            cluster_name = cluster.get("cluster_name", "Unknown Cluster")
            keywords = cluster.get("keywords", [])
            
            if len(keywords) > 1:
                pillar_keyword = keywords[0].get("keyword", "") if keywords else ""
                
                linking_strategy["hub_to_spoke_links"].append({
                    "pillar_page": pillar_keyword,
                    "cluster": cluster_name,
                    "supporting_pages": [kw.get("keyword", "") for kw in keywords[1:6]],
                    "recommended_links_per_page": min(len(keywords), 8)
                })
        
        # Cross-cluster opportunities
        for i, cluster1 in enumerate(clusters):
            for cluster2 in clusters[i+1:i+3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
                similarity_score = self._calculate_cluster_similarity(cluster1, cluster2)
                if similarity_score > 0.3:
                    linking_strategy["cross_cluster_opportunities"].append({
                        "from_cluster": cluster1.get("cluster_name", ""),
                        "to_cluster": cluster2.get("cluster_name", ""),
                        "similarity_score": similarity_score,
                        "recommended_link_count": 2 if similarity_score > 0.5 else 1
                    })
        
        # Anchor text suggestions
        all_keywords = []
        for cluster in clusters:
            all_keywords.extend(cluster.get("keywords", []))
        
        linking_strategy["anchor_text_suggestions"] = [
            {
                "keyword": kw.get("keyword", ""),
                "variations": self._generate_anchor_variations(kw.get("keyword", "")),
                "recommended_distribution": "exact: 10%, partial: 40%, branded: 30%, generic: 20%"
            }
            for kw in all_keywords[:10]  # –¢–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        ]
        
        return linking_strategy
    
    def _calculate_cluster_similarity(self, cluster1: Dict[str, Any], cluster2: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords1 = set(kw.get("keyword", "").lower().split() for kw in cluster1.get("keywords", []))
        keywords2 = set(kw.get("keyword", "").lower().split() for kw in cluster2.get("keywords", []))
        
        # Flatten sets of word lists
        words1 = set()
        words2 = set()
        for word_list in keywords1:
            words1.update(word_list)
        for word_list in keywords2:
            words2.update(word_list)
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _generate_anchor_variations(self, keyword: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π –∞–Ω–∫–æ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if not keyword:
            return []
        
        base_keyword = keyword.lower()
        variations = [
            keyword,  # Exact match
            f"–ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ {base_keyword}",  # Partial match
            f"—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –ø—Ä–æ {base_keyword}",  # Generic
            f"—á—Ç–æ —Ç–∞–∫–æ–µ {base_keyword}",  # Question form
            f"{base_keyword} - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"  # Extended
        ]
        
        return variations[:4]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞—Ü–∏–π
    
    def _identify_pillar_page_opportunities(self, clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è pillar pages"""
        pillar_opportunities = []
        
        for cluster in clusters:
            keywords = cluster.get("keywords", [])
            if len(keywords) < 3:  # –ú–∏–Ω–∏–º—É–º 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ –¥–ª—è pillar page
                continue
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è pillar page
            total_volume = sum(kw.get("search_volume", 0) for kw in keywords)
            avg_difficulty = sum(kw.get("difficulty", 50) for kw in keywords) / len(keywords)
            
            if total_volume > 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º –ø–æ–∏—Å–∫–∞
                main_keyword = keywords[0] if keywords else {}
                
                pillar_opportunity = {
                    "pillar_topic": cluster.get("cluster_name", main_keyword.get("keyword", "")),
                    "main_keyword": main_keyword.get("keyword", ""),
                    "supporting_keywords": [kw.get("keyword", "") for kw in keywords[1:10]],
                    "total_search_volume": total_volume,
                    "average_difficulty": round(avg_difficulty, 1),
                    "estimated_word_count": min(max(len(keywords) * 200, 2000), 5000),
                    "content_sections": self._suggest_pillar_sections(keywords),
                    "supporting_content_needed": len(keywords) - 1,
                    "internal_linking_potential": len(keywords) * 3,
                    "update_frequency": "quarterly",
                    "priority_score": self._calculate_pillar_priority(total_volume, avg_difficulty, len(keywords))
                }
                
                pillar_opportunities.append(pillar_opportunity)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        pillar_opportunities.sort(key=lambda x: x["priority_score"], reverse=True)
        
        return pillar_opportunities[:5]  # –¢–æ–ø-5 –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
    
    def _suggest_pillar_sections(self, keywords: List[Dict[str, Any]]) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è pillar page"""
        sections = [
            "–í–≤–µ–¥–µ–Ω–∏–µ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è",
            "–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏", 
            "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã",
            "–ü–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏",
            "–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã",
            "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ –¥–∞–ª—å–Ω–µ–π—à–∏–µ —à–∞–≥–∏"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_based_sections = []
        for kw in keywords[:3]:
            keyword = kw.get("keyword", "")
            if keyword:
                keyword_based_sections.append(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {keyword}")
        
        return sections + keyword_based_sections
    
    def _calculate_pillar_priority(self, volume: int, difficulty: float, keyword_count: int) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ pillar page"""
        # –§–æ—Ä–º—É–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: (–æ–±—ä–µ–º –ø–æ–∏—Å–∫–∞ * –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤) / —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        if difficulty == 0:
            difficulty = 1  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        
        base_score = (volume * keyword_count) / difficulty
        
        # –ë–æ–Ω—É—Å—ã
        if volume > 10000:
            base_score *= 1.2  # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º
        if keyword_count > 10:
            base_score *= 1.1  # –ë–æ–Ω—É—Å –∑–∞ —à–∏—Ä–æ–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä
        if difficulty < 30:
            base_score *= 1.15  # –ë–æ–Ω—É—Å –∑–∞ –Ω–∏–∑–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        
        return round(base_score, 2)
    
    def _identify_seasonal_opportunities(self, horizon_days: int) -> List[Dict[str, Any]]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü –∏ —Å–µ–∑–æ–Ω –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        import datetime
        current_month = datetime.datetime.now().month
        
        # –ë–∞–∑–æ–≤—ã–µ —Å–µ–∑–æ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ä—ã–Ω–∫–∞
        seasonal_categories = {
            "winter": {
                "months": [12, 1, 2],
                "keywords": ["–Ω–æ–≤—ã–π –≥–æ–¥", "–∑–∏–º–Ω–∏–µ —Å–∫–∏–¥–∫–∏", "–∑–∏–º–Ω—è—è –æ–¥–µ–∂–¥–∞", "–æ—Ç–æ–ø–ª–µ–Ω–∏–µ", "–ø—Ä–∞–∑–¥–Ω–∏–∫–∏"],
                "content_types": ["gift_guides", "winter_maintenance", "holiday_campaigns"]
            },
            "spring": {
                "months": [3, 4, 5], 
                "keywords": ["–≤–µ—Å–µ–Ω–Ω–∏–µ —Å–∫–∏–¥–∫–∏", "–¥–∞—á–∞", "—Å–∞–¥", "—Ä–µ–º–æ–Ω—Ç", "8 –º–∞—Ä—Ç–∞"],
                "content_types": ["home_improvement", "garden_guides", "spring_cleaning"]
            },
            "summer": {
                "months": [6, 7, 8],
                "keywords": ["–æ—Ç–ø—É—Å–∫", "–ª–µ—Ç–æ", "–¥–∞—á–∞", "–æ—Ç–¥—ã—Ö", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä—ã"],
                "content_types": ["vacation_guides", "summer_products", "outdoor_activities"]
            },
            "autumn": {
                "months": [9, 10, 11],
                "keywords": ["—à–∫–æ–ª–∞", "—É—á–µ–±–∞", "–æ—Å–µ–Ω–Ω–∏–µ —Å–∫–∏–¥–∫–∏", "–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∏–º–µ"],
                "content_types": ["back_to_school", "winter_preparation", "autumn_sales"]
            }
        }
        
        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ –∏ —Å–æ–±—ã—Ç–∏—è
        russian_holidays = {
            1: ["–ù–æ–≤—ã–π –≥–æ–¥", "–†–æ–∂–¥–µ—Å—Ç–≤–æ"],
            2: ["–î–µ–Ω—å –∑–∞—â–∏—Ç–Ω–∏–∫–∞ –û—Ç–µ—á–µ—Å—Ç–≤–∞"],
            3: ["8 –ú–∞—Ä—Ç–∞"],
            4: ["–ü–∞—Å—Ö–∞", "–î–µ–Ω—å –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∏"],
            5: ["–ú–∞–π—Å–∫–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏", "9 –ú–∞—è"],
            6: ["–î–µ–Ω—å –†–æ—Å—Å–∏–∏", "–ù–∞—á–∞–ª–æ –ª–µ—Ç–∞"],
            9: ["–î–µ–Ω—å –∑–Ω–∞–Ω–∏–π", "–ù–∞—á–∞–ª–æ —É—á–µ–±–Ω–æ–≥–æ –≥–æ–¥–∞"],
            10: ["–î–µ–Ω—å —É—á–∏—Ç–µ–ª—è"],
            11: ["–î–µ–Ω—å –Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –µ–¥–∏–Ω—Å—Ç–≤–∞"],
            12: ["–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ù–æ–≤–æ–º—É –≥–æ–¥—É"]
        }
        
        seasonal_opportunities = []
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Å—è—Ü—ã –≤ —Ä–∞–º–∫–∞—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
        months_to_plan = min(horizon_days // 30, 12)  # –ú–∞–∫—Å–∏–º—É–º 12 –º–µ—Å—è—Ü–µ–≤
        
        for month_offset in range(1, months_to_plan + 1):
            target_month = ((current_month + month_offset - 1) % 12) + 1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∑–æ–Ω
            season = None
            for season_name, season_data in seasonal_categories.items():
                if target_month in season_data["months"]:
                    season = season_name
                    season_info = season_data
                    break
            
            if not season:
                continue
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–∑–æ–Ω–Ω—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
            opportunity = {
                "month": target_month,
                "season": season,
                "preparation_time": f"–∑–∞ {month_offset} –º–µ—Å—è—Ü–µ–≤",
                "seasonal_keywords": season_info["keywords"],
                "content_types": season_info["content_types"],
                "holidays": russian_holidays.get(target_month, []),
                "recommended_content": self._generate_seasonal_content_ideas(season, target_month),
                "traffic_multiplier": self._estimate_seasonal_traffic_boost(season, target_month),
                "preparation_deadline": f"Month {month_offset - 1 if month_offset > 1 else 1}",
                "priority": "high" if month_offset <= 2 else "medium"
            }
            
            seasonal_opportunities.append(opportunity)
        
        return seasonal_opportunities
    
    def _generate_seasonal_content_ideas(self, season: str, month: int) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–¥–µ–π —Å–µ–∑–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        content_templates = {
            "winter": [
                "–¢–æ–ø-10 –∑–∏–º–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤",
                "–ö–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Ö–æ–ª–æ–¥–∞–º", 
                "–ó–∏–º–Ω–∏–µ —Å–∫–∏–¥–∫–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
                "–ù–æ–≤–æ–≥–æ–¥–Ω–∏–µ –∏–¥–µ–∏ –∏ –ø–æ–¥–∞—Ä–∫–∏"
            ],
            "spring": [
                "–í–µ—Å–µ–Ω–Ω–∏–µ —Ç—Ä–µ–Ω–¥—ã –∏ –Ω–æ–≤–∏–Ω–∫–∏",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –¥–∞—á–Ω–æ–º—É —Å–µ–∑–æ–Ω—É", 
                "–í–µ—Å–µ–Ω–Ω–∏–π —Ä–µ–º–æ–Ω—Ç: —á—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å",
                "–ò–¥–µ–∏ –¥–ª—è –≤–µ—Å–µ–Ω–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"
            ],
            "summer": [
                "–õ–µ—Ç–Ω–∏–µ must-have —Ç–æ–≤–∞—Ä—ã",
                "–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–ø—É—Å–∫–∞",
                "–î–∞—á–Ω—ã–µ –ª–∞–π—Ñ—Ö–∞–∫–∏", 
                "–õ–µ—Ç–Ω–∏–µ —Å–∫–∏–¥–∫–∏ –∏ —Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∏"
            ],
            "autumn": [
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —É—á–µ–±–Ω–æ–º—É –≥–æ–¥—É",
                "–û—Å–µ–Ω–Ω–∏–µ —Ç—Ä–µ–Ω–¥—ã",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∏–º–µ",
                "–û—Å–µ–Ω–Ω–∏–µ —Å–∫–∏–¥–∫–∏"
            ]
        }
        
        base_ideas = content_templates.get(season, ["–°–µ–∑–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Å—è—Ü-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∏–¥–µ–∏
        if month == 12 or month == 1:
            base_ideas.extend(["–ù–æ–≤–æ–≥–æ–¥–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∏", "–ò—Ç–æ–≥–∏ –≥–æ–¥–∞"])
        elif month == 3:
            base_ideas.extend(["8 –ú–∞—Ä—Ç–∞: –ø–æ–¥–∞—Ä–∫–∏ –∏ –∏–¥–µ–∏"])
        elif month == 9:
            base_ideas.extend(["–î–µ–Ω—å –∑–Ω–∞–Ω–∏–π", "–®–∫–æ–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã"])
        
        return base_ideas[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–¥–µ–π
    
    def _estimate_seasonal_traffic_boost(self, season: str, month: int) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–µ–∑–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ —Ç—Ä–∞—Ñ–∏–∫–∞"""
        
        # –ë–∞–∑–æ–≤—ã–µ –º–Ω–æ–∂–∏—Ç–µ–ª–∏ –¥–ª—è —Å–µ–∑–æ–Ω–æ–≤
        season_multipliers = {
            "winter": 1.3,  # –í—ã—Å–æ–∫–∏–π —Å–µ–∑–æ–Ω –ø–æ–∫—É–ø–æ–∫
            "spring": 1.1,  # –£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–æ—Å—Ç
            "summer": 0.9,  # –°–ø–∞–¥ (–æ—Ç–ø—É—Å–∫–∞)
            "autumn": 1.2   # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        }
        
        base_multiplier = season_multipliers.get(season, 1.0)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –¥–ª—è –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤
        if month in [12, 1]:  # –ù–æ–≤–æ–≥–æ–¥–Ω–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏
            base_multiplier *= 1.4
        elif month in [3, 5]:  # 8 –ú–∞—Ä—Ç–∞, –º–∞–π—Å–∫–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏
            base_multiplier *= 1.2
        elif month == 9:  # –ù–∞—á–∞–ª–æ —É—á–µ–±–Ω–æ–≥–æ –≥–æ–¥–∞
            base_multiplier *= 1.15
        
        return round(base_multiplier, 2)
    
    def _create_distribution_plan(self, content_calendar: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        if not content_calendar:
            return {
                "channels": [],
                "total_reach": 0,
                "estimated_traffic": 0,
                "distribution_cost": 0
            }
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞–Ω–∞–ª—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è
        distribution_channels = [
            {
                "channel": "Organic Search",
                "type": "earned",
                "reach_percentage": 45,
                "effort_level": "ongoing",
                "cost_per_piece": 0,
                "expected_traffic_per_piece": 150
            },
            {
                "channel": "Social Media",
                "type": "owned",
                "reach_percentage": 25,
                "effort_level": "high",
                "cost_per_piece": 500,
                "expected_traffic_per_piece": 80
            },
            {
                "channel": "Email Newsletter",
                "type": "owned", 
                "reach_percentage": 15,
                "effort_level": "medium",
                "cost_per_piece": 0,
                "expected_traffic_per_piece": 120
            },
            {
                "channel": "Paid Promotion",
                "type": "paid",
                "reach_percentage": 10,
                "effort_level": "low",
                "cost_per_piece": 2000,
                "expected_traffic_per_piece": 300
            },
            {
                "channel": "Industry Partners",
                "type": "earned",
                "reach_percentage": 5,
                "effort_level": "high",
                "cost_per_piece": 0,
                "expected_traffic_per_piece": 200
            }
        ]
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        total_content_pieces = len(content_calendar)
        total_estimated_traffic = 0
        total_distribution_cost = 0
        
        for channel in distribution_channels:
            pieces_per_channel = int(total_content_pieces * (channel["reach_percentage"] / 100))
            channel["content_pieces"] = pieces_per_channel
            channel["total_traffic"] = pieces_per_channel * channel["expected_traffic_per_piece"]
            channel["total_cost"] = pieces_per_channel * channel["cost_per_piece"]
            
            total_estimated_traffic += channel["total_traffic"]
            total_distribution_cost += channel["total_cost"]
        
        # –ü–ª–∞–Ω –ø–æ –º–µ—Å—è—Ü–∞–º
        monthly_plan = []
        months = ["January", "February", "March", "April", "May", "June",
                 "July", "August", "September", "October", "November", "December"]
        
        for i, month in enumerate(months[:min(len(months), total_content_pieces)]):
            if i < len(content_calendar):
                content_piece = content_calendar[i]
                monthly_plan.append({
                    "month": month,
                    "content_title": content_piece.get("content_title", f"Content {i+1}"),
                    "primary_channel": "Organic Search",
                    "secondary_channels": ["Social Media", "Email Newsletter"],
                    "estimated_reach": 2500 + (i * 100),
                    "estimated_traffic": 350 + (i * 25),
                    "distribution_budget": 1000 + (i * 200)
                })
        
        return {
            "channels": distribution_channels,
            "total_content_pieces": total_content_pieces,
            "total_estimated_traffic": total_estimated_traffic,
            "total_distribution_cost": total_distribution_cost,
            "monthly_distribution_plan": monthly_plan,
            "key_recommendations": [
                "–§–æ–∫—É—Å –Ω–∞ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ ROI",
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∞–º–ø–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                "–†–µ–≥—É–ª—è—Ä–Ω–∞—è email-—Ä–∞—Å—Å—ã–ª–∫–∞ –¥–ª—è —É–¥–µ—Ä–∂–∞–Ω–∏—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏",
                "–°–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–ª–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
            ],
            "success_metrics": {
                "traffic_growth_target": "25% –≤ –∫–≤–∞—Ä—Ç–∞–ª",
                "engagement_rate_target": "15% —Å—Ä–µ–¥–Ω–∏–π CTR",
                "conversion_rate_target": "3% –æ—Ç —Ç—Ä–∞—Ñ–∏–∫–∞",
                "brand_awareness_lift": "20% –∑–∞ –ø–æ–ª–≥–æ–¥–∞"
            }
        }
    
    def _calculate_calendar_resource_allocation(self, content_calendar: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–∞–ª–µ–Ω–¥–∞—Ä—è"""
        
        if not content_calendar:
            return {
                "total_pieces": 0,
                "estimated_hours": 0,
                "team_size_needed": 0,
                "monthly_budget": 0
            }
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤
        hours_per_blog_post = 6
        hours_per_guide = 12
        hours_per_video = 8
        hours_per_infographic = 4
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ
        content_type_distribution = {
            "blog_post": 0,
            "guide": 0,
            "video": 0,
            "infographic": 0,
            "other": 0
        }
        
        total_estimated_hours = 0
        
        for content_piece in content_calendar:
            content_type = content_piece.get("content_type", "blog_post")
            
            if content_type in content_type_distribution:
                content_type_distribution[content_type] += 1
            else:
                content_type_distribution["other"] += 1
            
            # –†–∞—Å—á–µ—Ç —á–∞—Å–æ–≤ –Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
            if content_type == "blog_post":
                total_estimated_hours += hours_per_blog_post
            elif content_type == "guide":
                total_estimated_hours += hours_per_guide
            elif content_type == "video":
                total_estimated_hours += hours_per_video
            elif content_type == "infographic":
                total_estimated_hours += hours_per_infographic
            else:
                total_estimated_hours += hours_per_blog_post  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        # –†–∞—Å—á–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –∫–æ–º–∞–Ω–¥—ã
        monthly_working_hours = 160  # 20 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π * 8 —á–∞—Å–æ–≤
        total_content_pieces = len(content_calendar)
        
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫–∞–ª–µ–Ω–¥–∞—Ä—å –Ω–∞ 3 –º–µ—Å—è—Ü–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ)
        calendar_duration_months = min(max(1, total_content_pieces // 8), 12)
        monthly_hours_needed = total_estimated_hours / calendar_duration_months
        
        # –†–∞–∑–º–µ—Ä –∫–æ–º–∞–Ω–¥—ã (—Å –∑–∞–ø–∞—Å–æ–º 20%)
        team_size_needed = max(1, int((monthly_hours_needed * 1.2) / monthly_working_hours))
        
        # –ë—é–¥–∂–µ—Ç (–±–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —á–∞—Å–æ–≤–æ–π —Å—Ç–∞–≤–∫–µ)
        hourly_rate = 2000  # ‚ÇΩ –∑–∞ —á–∞—Å (—Å—Ä–µ–¥–Ω—è—è —Å—Ç–∞–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç-–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∞)
        monthly_budget = int(monthly_hours_needed * hourly_rate)
        
        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Ä–æ–ª—è–º
        role_distribution = {
            "content_writer": {
                "count": max(1, team_size_needed - 1),
                "monthly_hours": monthly_hours_needed * 0.7,
                "hourly_rate": 1800,
                "monthly_cost": int(monthly_hours_needed * 0.7 * 1800)
            },
            "editor": {
                "count": 1 if team_size_needed > 1 else 0.5,
                "monthly_hours": monthly_hours_needed * 0.2,
                "hourly_rate": 2500,
                "monthly_cost": int(monthly_hours_needed * 0.2 * 2500)
            },
            "designer": {
                "count": 0.5,
                "monthly_hours": monthly_hours_needed * 0.1,
                "hourly_rate": 2200,
                "monthly_cost": int(monthly_hours_needed * 0.1 * 2200)
            }
        }
        
        return {
            "total_pieces": total_content_pieces,
            "estimated_total_hours": total_estimated_hours,
            "monthly_hours_needed": int(monthly_hours_needed),
            "calendar_duration_months": calendar_duration_months,
            "team_size_needed": team_size_needed,
            "monthly_budget": monthly_budget,
            "content_type_distribution": content_type_distribution,
            "role_distribution": role_distribution,
            "efficiency_metrics": {
                "hours_per_piece": round(total_estimated_hours / max(1, total_content_pieces), 1),
                "pieces_per_month": round(total_content_pieces / calendar_duration_months, 1),
                "cost_per_piece": round(monthly_budget / max(1, (total_content_pieces / calendar_duration_months)), 0)
            },
            "scaling_recommendations": [
                f"–¢–µ–∫—É—â–∞—è –∫–æ–º–∞–Ω–¥–∞ –∏–∑ {team_size_needed} —á–µ–ª–æ–≤–µ–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞",
                f"–ü–ª–∞–Ω–∏—Ä—É–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {round(total_content_pieces / calendar_duration_months, 1)} –∫–æ–Ω—Ç–µ–Ω—Ç–∞/–º–µ—Å—è—Ü",
                f"–°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {round(monthly_budget / max(1, (total_content_pieces / calendar_duration_months)), 0):,.0f} ‚ÇΩ",
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–∑–µ—Ä–≤ –±—é–¥–∂–µ—Ç–∞ 15% –Ω–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏"
            ]
        }
    
    def _prioritize_implementation_steps(self, keyword_research: Dict, content_strategy: Dict, topic_clustering: Dict) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —à–∞–≥–æ–≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        implementation_steps = []
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        research_priority = self._calculate_research_priority(keyword_research, topic_clustering)
        implementation_steps.append({
            "step": 1,
            "phase": "Research & Analysis",
            "title": "–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤",
            "duration": "2-3 –Ω–µ–¥–µ–ª–∏",
            "priority": research_priority,
            "tasks": [
                "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ keyword research –ø–æ –≤—Å–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º",
                "–ê–Ω–∞–ª–∏–∑ —Ç–æ–ø-10 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ SERP",
                "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ content gaps –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π",
                "–í–∞–ª–∏–¥–∞—Ü–∏—è topic clusters —Å –±–∏–∑–Ω–µ—Å-—Ü–µ–ª—è–º–∏"
            ],
            "deliverables": ["Keyword matrix", "Competitor analysis", "Content gap analysis"],
            "resources_needed": ["SEO –∞–Ω–∞–ª–∏—Ç–∏–∫", "Content strategist"],
            "success_criteria": "100% –∫–ª—é—á–µ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã"
        })
        
        # –®–∞–≥ 2: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        technical_priority = self._calculate_technical_priority(content_strategy)
        implementation_steps.append({
            "step": 2,
            "phase": "Technical Setup",
            "title": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã",
            "duration": "1-2 –Ω–µ–¥–µ–ª–∏",
            "priority": technical_priority,
            "tasks": [
                "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CMS –∏ SEO-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
                "–°–æ–∑–¥–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ —Ç—Ä–µ–∫–∏–Ω–≥–∞",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ workflow –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–æ–º–∞–Ω–¥—ã"
            ],
            "deliverables": ["Content templates", "Analytics setup", "Workflow documentation"],
            "resources_needed": ["Technical SEO", "Web developer"],
            "success_criteria": "–í—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥–æ—Ç–æ–≤–∞ –∫ production"
        })
        
        # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –ø–∏–ª–æ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        pilot_priority = self._calculate_pilot_priority(keyword_research, topic_clustering)
        implementation_steps.append({
            "step": 3,
            "phase": "Pilot Content Creation",
            "title": "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∏–ª–æ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ç–æ–ø-–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º",
            "duration": "3-4 –Ω–µ–¥–µ–ª–∏",
            "priority": pilot_priority,
            "tasks": [
                "–°–æ–∑–¥–∞–Ω–∏–µ 5-10 –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤",
                "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ —Ü–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
                "A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ meta-–¥–∞–Ω–Ω—ã—Ö",
                "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏"
            ],
            "deliverables": ["Pilot content pieces", "Performance baselines", "Optimization insights"],
            "resources_needed": ["Content writer", "SEO specialist", "Editor"],
            "success_criteria": "Pilot –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –ø–æ–∑–∏—Ü–∏–π –≤ —Ç–æ–ø-20"
        })
        
        # –®–∞–≥ 4: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaling_priority = self._calculate_scaling_priority(content_strategy, topic_clustering)
        implementation_steps.append({
            "step": 4,
            "phase": "Scaling & Optimization",
            "title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞",
            "duration": "8-12 –Ω–µ–¥–µ–ª—å",
            "priority": scaling_priority,
            "tasks": [
                "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç-–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞",
                "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö SEO-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤",
                "–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Ö–∞–±–æ–≤ –∏ pillar pages",
                "Continuous optimization –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"
            ],
            "deliverables": ["Scaled content production", "Hub architecture", "Performance reports"],
            "resources_needed": ["Content team", "SEO manager", "Data analyst"],
            "success_criteria": "–°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ 25%+"
        })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        implementation_steps.sort(key=lambda x: x["priority"], reverse=True)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        implementation_summary = {
            "total_phases": len(implementation_steps),
            "estimated_timeline": "14-21 –Ω–µ–¥–µ–ª—è",
            "critical_success_factors": [
                "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π keyword research –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç",
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã",
                "–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                "Continuous monitoring –∏ optimization"
            ],
            "risk_mitigation": [
                "Weekly progress reviews",
                "Backup content calendar –Ω–∞ —Å–ª—É—á–∞–π –∑–∞–¥–µ—Ä–∂–µ–∫",
                "Cross-training –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º–æ—Å—Ç–∏",
                "Flexible resource allocation"
            ],
            "roi_expectations": {
                "month_3": "–ü–µ—Ä–≤—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤ SERP",
                "month_6": "25-40% —Ä–æ—Å—Ç –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞",
                "month_12": "–î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ç–æ–ø-3 –ø–æ–∑–∏—Ü–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º"
            }
        }
        
        return {
            "implementation_steps": implementation_steps,
            "summary": implementation_summary,
            "next_actions": [
                implementation_steps[0]["tasks"][0] if implementation_steps else "–ù–∞—á–∞—Ç—å keyword research",
                "–°–æ–±—Ä–∞—Ç—å –ø—Ä–æ–µ–∫—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É",
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å KPI –∏ –º–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞"
            ]
        }
    
    def _calculate_research_priority(self, keyword_research: Dict, topic_clustering: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —ç—Ç–∞–ø–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        base_priority = 0.9  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è research
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        if keyword_research.get("keyword_research_results", {}).get("total_keywords", 0) > 1000:
            base_priority += 0.05
        if topic_clustering.get("topic_clusters", {}).get("total_clusters", 0) > 10:
            base_priority += 0.03
        
        return min(1.0, base_priority)
    
    def _calculate_technical_priority(self, content_strategy: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏"""
        return 0.85  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    
    def _calculate_pilot_priority(self, keyword_research: Dict, topic_clustering: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –ø–∏–ª–æ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        base_priority = 0.8
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –µ—Å–ª–∏ –º–Ω–æ–≥–æ high-volume keywords
        total_keywords = keyword_research.get("keyword_research_results", {}).get("total_keywords", 0)
        if total_keywords > 500:
            base_priority += 0.1
        
        return min(1.0, base_priority)
    
    def _calculate_scaling_priority(self, content_strategy: Dict, topic_clustering: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return 0.7  # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É—Å–ø–µ—Ö–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–æ–≤