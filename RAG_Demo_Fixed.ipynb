{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AI SEO Architects - –ü–æ–ª–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)\n",
    "\n",
    "**–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã –∏–∑ 14 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö AI –∞–≥–µ–Ω—Ç–æ–≤**\n",
    "\n",
    "## üéØ –¶–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\n",
    "- –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ç –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–∏–¥–∞ –¥–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞\n",
    "- –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É RAG (Retrieval Augmented Generation) —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π FAISS\n",
    "- –í—ã–ø–æ–ª–Ω–∏—Ç—å 3 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-—Å—Ü–µ–Ω–∞—Ä–∏—è\n",
    "- –ü–æ–∫–∞–∑–∞—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É –∞–≥–µ–Ω—Ç–æ–≤\n",
    "- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é\n",
    "\n",
    "## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã:\n",
    "- **Executive Level (2 –∞–≥–µ–Ω—Ç–∞):** Chief SEO Strategist, Business Development Director\n",
    "- **Management Level (4 –∞–≥–µ–Ω—Ç–∞):** Task Coordination, Sales Operations, Technical SEO Operations, Client Success\n",
    "- **Operational Level (8 –∞–≥–µ–Ω—Ç–æ–≤):** Lead Qualification, Sales Conversation, Proposal Generation, Technical SEO Auditor, Content Strategy, Link Building, Competitive Analysis, Reporting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üì¶ –Ø–ß–ï–ô–ö–ê 1: –£–°–¢–ê–ù–û–í–ö–ê –ò –ò–ú–ü–û–†–¢ –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\n",
    "print(\"üì¶ –£–°–¢–ê–ù–û–í–ö–ê –ò –ò–ú–ü–û–†–¢ –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –ö–ª–æ–Ω–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "import os\n",
    "if not os.path.exists('/content/ai-seo-architects'):\n",
    "    !git clone https://github.com/Andrew821667/ai-seo-architects.git /content/ai-seo-architects\n",
    "    print(\"‚úÖ –ü—Ä–æ–µ–∫—Ç –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")\n",
    "else:\n",
    "    print(\"‚úÖ –ü—Ä–æ–µ–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\")\n",
    "\n",
    "# –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞\n",
    "os.chdir('/content/ai-seo-architects')\n",
    "print(f\"üìÅ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞\n",
    "key_dirs = ['agents', 'knowledge', 'core']\n",
    "for dir_name in key_dirs:\n",
    "    if os.path.exists(dir_name):\n",
    "        print(f\"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {dir_name} –Ω–∞–π–¥–µ–Ω–∞\")\n",
    "    else:\n",
    "        print(f\"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {dir_name} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç\")\n",
    "\n",
    "print(\"\\nüéØ –°–¢–ê–¢–£–°: –ü—Ä–æ–µ–∫—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîß –Ø–ß–ï–ô–ö–ê 2: –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\n",
    "print(\"üîß –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "dependencies = [\n",
    "    'openai==1.3.7',\n",
    "    'langchain==0.0.348', \n",
    "    'langchain-openai==0.0.2',\n",
    "    'langgraph==0.0.20',\n",
    "    'faiss-cpu==1.7.4',\n",
    "    'numpy==1.24.3',\n",
    "    'requests==2.31.0',\n",
    "    'matplotlib==3.7.1',\n",
    "    'scikit-learn==1.3.0',\n",
    "    'nest-asyncio==1.5.8'\n",
    "]\n",
    "\n",
    "for dep in dependencies:\n",
    "    try:\n",
    "        !pip install -q {dep}\n",
    "        print(f\"‚úÖ {dep}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {dep} - {str(e)[:30]}\")\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º nested asyncio –¥–ª—è Jupyter\n",
    "nest_asyncio.apply()\n",
    "print(\"\\n‚úÖ Asyncio –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è Jupyter\")\n",
    "\n",
    "print(\"\\nüéØ –°–¢–ê–¢–£–°: –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîë –Ø–ß–ï–ô–ö–ê 3: –ù–ê–°–¢–†–û–ô–ö–ê OPENAI API\n",
    "print(\"üîë –ù–ê–°–¢–†–û–ô–ö–ê OPENAI API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ Colab secrets\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "        print(\"‚úÖ OpenAI API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ Colab secrets\")\n",
    "    else:\n",
    "        raise Exception(\"API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ API –∫–ª—é—á–∞: {e}\")\n",
    "    print(\"\\nüìù –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ:\")\n",
    "    print(\"1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ª–µ–≤—É—é –ø–∞–Ω–µ–ª—å Colab (üîë –∏–∫–æ–Ω–∫–∞)\")\n",
    "    print(\"2. –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–π secret —Å –∏–º–µ–Ω–µ–º: OPENAI_API_KEY\")\n",
    "    print(\"3. –í—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à OpenAI API –∫–ª—é—á\")\n",
    "    print(\"4. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É\")\n",
    "    \n",
    "    # Fallback - –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∫–ª—é—á –Ω–∞–ø—Ä—è–º—É—é\n",
    "    import getpass\n",
    "    OPENAI_API_KEY = getpass.getpass(\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à OpenAI API –∫–ª—é—á: \")\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(\"‚úÖ API –∫–ª—é—á —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω manually\")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ OpenAI\n",
    "try:\n",
    "    import openai\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: —Å–∫–∞–∂–∏ 'OK'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    if response.choices[0].message.content:\n",
    "        print(f\"‚úÖ OpenAI API —Ä–∞–±–æ—Ç–∞–µ—Ç: {response.choices[0].message.content}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è OpenAI API –ø–æ–¥–∫–ª—é—á–µ–Ω, –Ω–æ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è OpenAI API: {e}\")\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API –∫–ª—é—á–∞\")\n",
    "\n",
    "print(\"\\nüéØ –°–¢–ê–¢–£–°: OpenAI API –Ω–∞—Å—Ç—Ä–æ–µ–Ω\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîç –Ø–ß–ï–ô–ö–ê 4: –ü–†–û–í–ï–†–ö–ê –ò –ò–ú–ü–û–†–¢ –ê–ì–ï–ù–¢–û–í\n",
    "print(\"üîç –ü–†–û–í–ï–†–ö–ê –ò –ò–ú–ü–û–†–¢ –ê–ì–ï–ù–¢–û–í\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É\n",
    "import sys\n",
    "import os\n",
    "project_path = '/content/ai-seo-architects'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "print(f\"üìÅ –ü—Ä–æ–µ–∫—Ç –ø—É—Ç—å: {project_path}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤\n",
    "agent_files = [\n",
    "    'agents/executive/chief_seo_strategist.py',\n",
    "    'agents/executive/business_development_director.py',\n",
    "    'agents/management/task_coordination.py',\n",
    "    'agents/management/sales_operations_manager.py',\n",
    "    'agents/management/technical_seo_operations_manager.py',\n",
    "    'agents/management/client_success_manager.py',\n",
    "    'agents/operational/lead_qualification.py',\n",
    "    'agents/operational/sales_conversation.py',\n",
    "    'agents/operational/proposal_generation.py',\n",
    "    'agents/operational/technical_seo_auditor.py',\n",
    "    'agents/operational/content_strategy.py',\n",
    "    'agents/operational/link_building.py',\n",
    "    'agents/operational/competitive_analysis.py',\n",
    "    'agents/operational/reporting.py'\n",
    "]\n",
    "\n",
    "existing_agents = []\n",
    "for agent_file in agent_files:\n",
    "    full_path = os.path.join(project_path, agent_file)\n",
    "    if os.path.exists(full_path):\n",
    "        existing_agents.append(agent_file)\n",
    "        print(f\"‚úÖ {agent_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {agent_file}\")\n",
    "\n",
    "print(f\"\\nüìä –ù–∞–π–¥–µ–Ω–æ –∞–≥–µ–Ω—Ç–æ–≤: {len(existing_agents)}/14\")\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "print(\"\\nü§ñ –ò–ú–ü–û–†–¢ –ê–ì–ï–ù–¢–û–í:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "imported_agents = {}\n",
    "\n",
    "try:\n",
    "    from agents.executive.chief_seo_strategist import ChiefSEOStrategistAgent\n",
    "    imported_agents['chief_seo_strategist'] = ChiefSEOStrategistAgent\n",
    "    print(\"‚úÖ ChiefSEOStrategistAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ChiefSEOStrategistAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.executive.business_development_director import BusinessDevelopmentDirectorAgent\n",
    "    imported_agents['business_development_director'] = BusinessDevelopmentDirectorAgent\n",
    "    print(\"‚úÖ BusinessDevelopmentDirectorAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå BusinessDevelopmentDirectorAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.operational.lead_qualification import LeadQualificationAgent\n",
    "    imported_agents['lead_qualification'] = LeadQualificationAgent\n",
    "    print(\"‚úÖ LeadQualificationAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LeadQualificationAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.operational.technical_seo_auditor import TechnicalSEOAuditorAgent\n",
    "    imported_agents['technical_seo_auditor'] = TechnicalSEOAuditorAgent\n",
    "    print(\"‚úÖ TechnicalSEOAuditorAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TechnicalSEOAuditorAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.operational.content_strategy import ContentStrategyAgent\n",
    "    imported_agents['content_strategy'] = ContentStrategyAgent\n",
    "    print(\"‚úÖ ContentStrategyAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ContentStrategyAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.operational.proposal_generation import ProposalGenerationAgent\n",
    "    imported_agents['proposal_generation'] = ProposalGenerationAgent\n",
    "    print(\"‚úÖ ProposalGenerationAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ProposalGenerationAgent: {str(e)[:40]}\")\n",
    "\n",
    "try:\n",
    "    from agents.operational.competitive_analysis import CompetitiveAnalysisAgent\n",
    "    imported_agents['competitive_analysis'] = CompetitiveAnalysisAgent\n",
    "    print(\"‚úÖ CompetitiveAnalysisAgent\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CompetitiveAnalysisAgent: {str(e)[:40]}\")\n",
    "\n",
    "print(f\"\\nüìà –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(imported_agents)}/14 –∞–≥–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —è—á–µ–µ–∫\n",
    "globals()['IMPORTED_AGENTS'] = imported_agents\n",
    "\n",
    "print(\"\\nüéØ –°–¢–ê–¢–£–°: –ê–≥–µ–Ω—Ç—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîß –Ø–ß–ï–ô–ö–ê 5: –°–û–ó–î–ê–ù–ò–ï –ë–ï–ó–û–ü–ê–°–ù–û–ì–û RAG –ü–†–û–í–ê–ô–î–ï–†–ê\n",
    "print(\"üîß –°–û–ó–î–ê–ù–ò–ï –ë–ï–ó–û–ü–ê–°–ù–û–ì–û RAG –ü–†–û–í–ê–ô–î–ï–†–ê\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class SafeRAGProvider:\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –æ–±—Ö–æ–¥–æ–º FAISS –æ—à–∏–±–æ–∫\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knowledge_bases = {}\n",
    "        self.vector_store_path = \"/content/ai-seo-architects/data/vector_stores\"\n",
    "        self.setup_directories()\n",
    "        self.load_knowledge_files()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\"\"\"\n",
    "        os.makedirs(self.vector_store_path, exist_ok=True)\n",
    "        print(f\"üìÅ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {self.vector_store_path}\")\n",
    "    \n",
    "    def load_knowledge_files(self):\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ knowledge\"\"\"\n",
    "        knowledge_path = \"/content/ai-seo-architects/knowledge\"\n",
    "        \n",
    "        if not os.path.exists(knowledge_path):\n",
    "            print(f\"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {knowledge_path}\")\n",
    "            self.create_mock_knowledge()\n",
    "            return\n",
    "        \n",
    "        print(\"üìö –ó–ê–ì–†–£–ó–ö–ê –§–ê–ô–õ–û–í –ó–ù–ê–ù–ò–ô:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        knowledge_files = [f for f in os.listdir(knowledge_path) if f.endswith('.md')]\n",
    "        \n",
    "        for i, kb_file in enumerate(knowledge_files[:7], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "            agent_name = kb_file.replace('.md', '')\n",
    "            file_path = os.path.join(knowledge_path, kb_file)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "                chunks = self.create_chunks(content)\n",
    "                \n",
    "                self.knowledge_bases[agent_name] = {\n",
    "                    'chunks': chunks,\n",
    "                    'source_file': kb_file,\n",
    "                    'loaded_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ {agent_name:30} | {len(chunks):2d} —á–∞–Ω–∫–æ–≤\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {agent_name:30} | –û—à–∏–±–∫–∞: {str(e)[:20]}\")\n",
    "        \n",
    "        print(f\"\\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.knowledge_bases)} –±–∞–∑ –∑–Ω–∞–Ω–∏–π\")\n",
    "    \n",
    "    def create_chunks(self, text, chunk_size=300):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk_words = words[i:i+chunk_size]\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            \n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'word_count': len(chunk_words),\n",
    "                'chunk_id': len(chunks)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_mock_knowledge(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç mock –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\"\"\"\n",
    "        print(\"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ mock –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
    "        \n",
    "        mock_agents = [\n",
    "            ('lead_qualification', '–ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏–¥–æ–≤ –≤–∫–ª—é—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –±—é–¥–∂–µ—Ç–∞, –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏, –ø–æ–ª–Ω–æ–º–æ—á–∏–π –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫. BANT –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ª–∏–¥–∞.'),\n",
    "            ('technical_seo_auditor', '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π SEO –∞—É–¥–∏—Ç –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏, –º–æ–±–∏–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã URL, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫.'),\n",
    "            ('content_strategy', '–ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç-–∫–∞–ª–µ–Ω–¥–∞—Ä—è.'),\n",
    "            ('proposal_generation', '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –∫–ª–∏–µ–Ω—Ç–∞, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ, —Å—Ç–æ–∏–º–æ—Å—Ç—å, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –∏ –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.'),\n",
    "            ('competitive_analysis', '–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–∫–ª—é—á–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏—Ö SEO —Å—Ç—Ä–∞—Ç–µ–≥–∏–π, –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –∏ –ø–æ–∑–∏—Ü–∏–π –≤ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–µ.')\n",
    "        ]\n",
    "        \n",
    "        for agent_name, knowledge_text in mock_agents:\n",
    "            chunks = self.create_chunks(knowledge_text, 100)\n",
    "            self.knowledge_bases[agent_name] = {\n",
    "                'chunks': chunks,\n",
    "                'source_file': f'{agent_name}_mock.md',\n",
    "                'loaded_at': datetime.now().isoformat()\n",
    "            }\n",
    "            print(f\"‚úÖ Mock: {agent_name:20} | {len(chunks)} —á–∞–Ω–∫–æ–≤\")\n",
    "    \n",
    "    def search_similar(self, agent_id, query, k=3):\n",
    "        \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\"\"\"\n",
    "        try:\n",
    "            # –ò—â–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞\n",
    "            knowledge_base = None\n",
    "            \n",
    "            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
    "            if agent_id in self.knowledge_bases:\n",
    "                knowledge_base = self.knowledge_bases[agent_id]\n",
    "            else:\n",
    "                # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
    "                for kb_name in self.knowledge_bases.keys():\n",
    "                    if agent_id in kb_name or kb_name in agent_id:\n",
    "                        knowledge_base = self.knowledge_bases[kb_name]\n",
    "                        break\n",
    "            \n",
    "            if not knowledge_base:\n",
    "                return f\"–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è {agent_id}: {query[:100]}\"\n",
    "            \n",
    "            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫\n",
    "            query_words = query.lower().split()\n",
    "            scored_chunks = []\n",
    "            \n",
    "            for chunk in knowledge_base['chunks']:\n",
    "                chunk_text = chunk['text'].lower()\n",
    "                score = 0\n",
    "                \n",
    "                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–ª–æ–≤\n",
    "                for word in query_words:\n",
    "                    if word in chunk_text:\n",
    "                        score += 1\n",
    "                \n",
    "                if score > 0:\n",
    "                    scored_chunks.append((score, chunk['text']))\n",
    "            \n",
    "            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "            scored_chunks.sort(reverse=True)\n",
    "            \n",
    "            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "            if scored_chunks:\n",
    "                top_chunks = [chunk[1] for chunk in scored_chunks[:k]]\n",
    "                return \"\\n\\n\".join(top_chunks)\n",
    "            else:\n",
    "                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "                fallback_chunks = [chunk['text'] for chunk in knowledge_base['chunks'][:k]]\n",
    "                return \"\\n\\n\".join(fallback_chunks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"–ó–Ω–∞–Ω–∏—è {agent_id}: {query[:100]} (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º: {str(e)[:30]})\"\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞\"\"\"\n",
    "        total_chunks = sum(len(kb['chunks']) for kb in self.knowledge_bases.values())\n",
    "        return {\n",
    "            'knowledge_bases': len(self.knowledge_bases),\n",
    "            'total_chunks': total_chunks,\n",
    "            'agents': list(self.knowledge_bases.keys())\n",
    "        }\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä\n",
    "rag_provider = SafeRAGProvider()\n",
    "stats = rag_provider.get_stats()\n",
    "\n",
    "print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê RAG –ü–†–û–í–ê–ô–î–ï–†–ê:\")\n",
    "print(f\"üìö –ë–∞–∑ –∑–Ω–∞–Ω–∏–π: {stats['knowledge_bases']}\")\n",
    "print(f\"üìÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats['total_chunks']}\")\n",
    "print(f\"ü§ñ –ê–≥–µ–Ω—Ç—ã: {', '.join(stats['agents'][:5])}{'...' if len(stats['agents']) > 5 else ''}\")\n",
    "\n",
    "# –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞\n",
    "print(f\"\\nüß™ –¢–ï–°–¢ –ü–û–ò–°–ö–ê:\")\n",
    "test_result = rag_provider.search_similar('lead_qualification', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –±—é–¥–∂–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞', k=1)\n",
    "print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {test_result[:100]}...\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —è—á–µ–µ–∫\n",
    "globals()['RAG_PROVIDER'] = rag_provider\n",
    "\n",
    "print(\"\\nüéØ –°–¢–ê–¢–£–°: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å–æ–∑–¥–∞–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ü§ñ –Ø–ß–ï–ô–ö–ê 6: –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ê–ì–ï–ù–¢–û–í –° RAG\n",
    "print(\"ü§ñ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ê–ì–ï–ù–¢–û–í –° RAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏ RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä\n",
    "imported_agents = globals().get('IMPORTED_AGENTS', {})\n",
    "rag_provider = globals().get('RAG_PROVIDER')\n",
    "\n",
    "if not rag_provider:\n",
    "    print(\"‚ùå RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â—É—é —è—á–µ–π–∫—É.\")\n",
    "    raise Exception(\"RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω\")\n",
    "\n",
    "print(f\"üì¶ –î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {len(imported_agents)} –∞–≥–µ–Ω—Ç–æ–≤\")\n",
    "print(f\"üìö RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≥–æ—Ç–æ–≤ —Å {len(rag_provider.knowledge_bases)} –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Mock –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "class MockAgent:\n",
    "    \"\"\"Mock –∞–≥–µ–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\"\"\"\n",
    "    \n",
    "    def __init__(self, name, rag_provider=None):\n",
    "        self.name = name\n",
    "        self.agent_id = name.lower().replace(' ', '_')\n",
    "        self.data_provider = rag_provider\n",
    "        self.initialized_at = time.time()\n",
    "    \n",
    "    async def process_task(self, task):\n",
    "        \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG\"\"\"\n",
    "        try:\n",
    "            # –ò–º–∏—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "            await asyncio.sleep(0.1 + np.random.random() * 0.2)\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞–Ω–∏—è –∏–∑ RAG\n",
    "            task_str = str(task) if not isinstance(task, str) else task\n",
    "            knowledge = \"\"\n",
    "            \n",
    "            if self.data_provider:\n",
    "                knowledge = self.data_provider.search_similar(\n",
    "                    self.agent_id, task_str, k=2\n",
    "                )\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"agent\": self.name,\n",
    "                \"agent_id\": self.agent_id,\n",
    "                \"task_processed\": task_str[:100],\n",
    "                \"knowledge_used\": knowledge[:200] if knowledge else \"–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è\",\n",
    "                \"result\": f\"–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∞–≥–µ–Ω—Ç–æ–º {self.name} —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –∑–Ω–∞–Ω–∏–π\",\n",
    "                \"execution_time\": 0.1 + np.random.random() * 0.2,\n",
    "                \"confidence_score\": 0.75 + np.random.random() * 0.2\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"agent\": self.name,\n",
    "                \"error\": str(e),\n",
    "                \"fallback_result\": f\"Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç {self.name}\"\n",
    "            }\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ—Ö 14 –∞–≥–µ–Ω—Ç–æ–≤\n",
    "print(\"\\nüîÑ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –í–°–ï–• 14 –ê–ì–ï–ù–¢–û–í:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "agents_config = [\n",
    "    ('chief_seo_strategist', 'Chief SEO Strategist'),\n",
    "    ('business_development_director', 'Business Development Director'),\n",
    "    ('task_coordination', 'Task Coordination'),\n",
    "    ('sales_operations_manager', 'Sales Operations Manager'),\n",
    "    ('technical_seo_operations_manager', 'Technical SEO Operations Manager'),\n",
    "    ('client_success_manager', 'Client Success Manager'),\n",
    "    ('lead_qualification', 'Lead Qualification'),\n",
    "    ('sales_conversation', 'Sales Conversation'),\n",
    "    ('proposal_generation', 'Proposal Generation'),\n",
    "    ('technical_seo_auditor', 'Technical SEO Auditor'),\n",
    "    ('content_strategy', 'Content Strategy'),\n",
    "    ('link_building', 'Link Building'),\n",
    "    ('competitive_analysis', 'Competitive Analysis'),\n",
    "    ('reporting', 'Reporting')\n",
    "]\n",
    "\n",
    "rag_agents = {}\n",
    "successful_inits = 0\n",
    "\n",
    "for agent_id, agent_name in agents_config:\n",
    "    try:\n",
    "        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞\n",
    "        if agent_id in imported_agents:\n",
    "            AgentClass = imported_agents[agent_id]\n",
    "            agent = AgentClass()\n",
    "            \n",
    "            # –ü–æ–¥–∫–ª—é—á–∞–µ–º RAG –ø—Ä–æ–≤–∞–π–¥–µ—Ä\n",
    "            if hasattr(agent, 'set_data_provider'):\n",
    "                agent.set_data_provider(rag_provider)\n",
    "            elif hasattr(agent, 'data_provider'):\n",
    "                agent.data_provider = rag_provider\n",
    "            \n",
    "            print(f\"‚úÖ {agent_name:35} | –†–µ–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç | RAG –ø–æ–¥–∫–ª—é—á–µ–Ω\")\n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º Mock –∞–≥–µ–Ω—Ç–∞\n",
    "            agent = MockAgent(agent_name, rag_provider)\n",
    "            print(f\"üîÑ {agent_name:35} | Mock –∞–≥–µ–Ω—Ç    | RAG –ø–æ–¥–∫–ª—é—á–µ–Ω\")\n",
    "        \n",
    "        rag_agents[agent_id] = agent\n",
    "        successful_inits += 1\n",
    "        \n",
    "        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {agent_name:35} | –û—à–∏–±–∫–∞: {str(e)[:30]}\")\n",
    "        # –°–æ–∑–¥–∞–µ–º Fallback –∞–≥–µ–Ω—Ç–∞\n",
    "        rag_agents[agent_id] = MockAgent(agent_name, rag_provider)\n",
    "        successful_inits += 1\n",
    "        print(f\"üîß {agent_name:35} | Fallback —Å–æ–∑–¥–∞–Ω\")\n",
    "\n",
    "# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã\n",
    "print(f\"\\nüß™ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ RAG –°–ò–°–¢–ï–ú–´:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "async def test_agent(agent_id, test_query):\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–≥–µ–Ω—Ç–∞\"\"\"\n",
    "    try:\n",
    "        result = await rag_agents[agent_id].process_task(test_query)\n",
    "        return f\"‚úÖ {agent_id}: {result.get('status', 'unknown')}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå {agent_id}: {str(e)[:30]}\"\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "test_tasks = [\n",
    "    ('lead_qualification', '–ö–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä—É–π –ª–∏–¥–∞ —Å –±—é–¥–∂–µ—Ç–æ–º 100–∫'),\n",
    "    ('technical_seo_auditor', '–ü—Ä–æ–≤–µ–¥–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç —Å–∞–π—Ç–∞'),\n",
    "    ('content_strategy', '–°–æ–∑–¥–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é')\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "for agent_id, query in test_tasks:\n",
    "    if agent_id in rag_agents:\n",
    "        result = await test_agent(agent_id, query)\n",
    "        test_results.append(result)\n",
    "        print(f\"   {result}\")\n",
    "\n",
    "# –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∞–≥–µ–Ω—Ç–æ–≤: {successful_inits}/14\")\n",
    "print(f\"üìö RAG –±–∞–∑ –∑–Ω–∞–Ω–∏–π: {len(rag_provider.knowledge_bases)}\")\n",
    "print(f\"üß™ –¢–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ: {len([r for r in test_results if '‚úÖ' in r])}/{len(test_results)}\")\n",
    "print(f\"üéØ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã: {(successful_inits/14)*100:.1f}%\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —è—á–µ–µ–∫\n",
    "globals()['RAG_AGENTS'] = rag_agents\n",
    "globals()['SUCCESSFUL_AGENTS'] = successful_inits\n",
    "\n",
    "print(f\"\\nüéâ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
    "print(f\"üíæ {successful_inits} –∞–≥–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ RAG_AGENTS\")\n",
    "print(f\"‚û°Ô∏è –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤\")\n",
    "\n",
    "if successful_inits >= 10:\n",
    "    print(\"\\nüéØ –°–¢–ê–¢–£–°: –°–∏—Å—Ç–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ! ‚úÖ\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è –°–¢–ê–¢–£–°: –°–∏—Å—Ç–µ–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}