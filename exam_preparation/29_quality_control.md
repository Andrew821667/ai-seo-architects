# –¢–µ–º–∞ 29: –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –ò–ò —Å–∏—Å—Ç–µ–º–∞—Ö

## –û—Å–Ω–æ–≤—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞

### –ü—Ä–∏–Ω—Ü–∏–ø—ã QA –≤ –ò–ò —Å–∏—Å—Ç–µ–º–∞—Ö
- **–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–æ–¥–µ–ª–∏**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ production
- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
- **–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å**: –°–±–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ**: –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
```python
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import statistics
import uuid

class QualityLevel(Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    UNACCEPTABLE = "unacceptable"

class TestType(Enum):
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    ACCURACY = "accuracy"
    USER_ACCEPTANCE = "user_acceptance"

@dataclass
class QualityMetric:
    """–ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
    name: str
    value: float
    threshold: float
    unit: str
    timestamp: datetime
    context: Dict[str, Any] = field(default_factory=dict)
    
    def is_passing(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏"""
        return self.value >= self.threshold
    
    def get_quality_level(self) -> QualityLevel:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –∫–∞—á–µ—Å—Ç–≤–∞"""
        ratio = self.value / self.threshold if self.threshold > 0 else 0
        
        if ratio >= 1.2:
            return QualityLevel.EXCELLENT
        elif ratio >= 1.0:
            return QualityLevel.GOOD
        elif ratio >= 0.8:
            return QualityLevel.ACCEPTABLE
        elif ratio >= 0.6:
            return QualityLevel.POOR
        else:
            return QualityLevel.UNACCEPTABLE

@dataclass
class TestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    test_id: str
    test_type: TestType
    test_name: str
    passed: bool
    score: float
    metrics: List[QualityMetric]
    execution_time: float
    timestamp: datetime
    error_message: Optional[str] = None
    details: Dict[str, Any] = field(default_factory=dict)

class QualityController:
    """–ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ò–ò —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.quality_standards = self._initialize_standards()
        self.evaluators = {}
        self.alerts = []
    
    def _initialize_standards(self) -> Dict[str, Dict[str, float]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞"""
        return {
            'accuracy': {
                'text_classification': 0.85,
                'sentiment_analysis': 0.80,
                'named_entity_recognition': 0.90,
                'text_generation_relevance': 0.75,
                'translation_quality': 0.85
            },
            'performance': {
                'response_time_ms': 2000,
                'throughput_requests_per_sec': 100,
                'memory_usage_mb': 1000,
                'cpu_utilization_percent': 80,
                'error_rate_percent': 1.0
            },
            'reliability': {
                'uptime_percent': 99.9,
                'availability_percent': 99.5,
                'data_integrity_percent': 100.0,
                'backup_success_rate': 100.0
            },
            'user_experience': {
                'user_satisfaction_score': 4.0,  # –∏–∑ 5
                'task_completion_rate': 0.90,
                'user_retention_rate': 0.80,
                'support_ticket_rate': 0.05
            }
        }
    
    def register_evaluator(self, test_type: str, evaluator_func: Callable):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ü–µ–Ω–∫–∏"""
        self.evaluators[test_type] = evaluator_func
    
    def run_quality_check(self, component: str, test_data: Any, 
                         test_types: List[TestType] = None) -> List[TestResult]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        
        if test_types is None:
            test_types = [TestType.ACCURACY, TestType.PERFORMANCE]
        
        results = []
        
        for test_type in test_types:
            start_time = datetime.now()
            
            try:
                result = self._execute_test(component, test_data, test_type)
                execution_time = (datetime.now() - start_time).total_seconds()
                
                test_result = TestResult(
                    test_id=str(uuid.uuid4()),
                    test_type=test_type,
                    test_name=f"{component}_{test_type.value}_test",
                    passed=result['passed'],
                    score=result['score'],
                    metrics=result['metrics'],
                    execution_time=execution_time,
                    timestamp=start_time,
                    details=result.get('details', {})
                )
                
                results.append(test_result)
                self.test_results.append(test_result)
                
            except Exception as e:
                error_result = TestResult(
                    test_id=str(uuid.uuid4()),
                    test_type=test_type,
                    test_name=f"{component}_{test_type.value}_test",
                    passed=False,
                    score=0.0,
                    metrics=[],
                    execution_time=(datetime.now() - start_time).total_seconds(),
                    timestamp=start_time,
                    error_message=str(e)
                )
                
                results.append(error_result)
                self.test_results.append(error_result)
        
        return results
    
    def _execute_test(self, component: str, test_data: Any, 
                     test_type: TestType) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞"""
        
        if test_type == TestType.ACCURACY:
            return self._test_accuracy(component, test_data)
        elif test_type == TestType.PERFORMANCE:
            return self._test_performance(component, test_data)
        elif test_type == TestType.USER_ACCEPTANCE:
            return self._test_user_acceptance(component, test_data)
        else:
            raise NotImplementedError(f"–¢–µ—Å—Ç —Ç–∏–ø–∞ {test_type} –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
    
    def _test_accuracy(self, component: str, test_data: Any) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        
        # –°–∏–º—É–ª—è—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        import random
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
        metrics = []
        
        if component == 'text_classifier':
            accuracy = random.uniform(0.75, 0.95)
            precision = random.uniform(0.70, 0.90)
            recall = random.uniform(0.70, 0.90)
            f1_score = 2 * (precision * recall) / (precision + recall)
            
            metrics = [
                QualityMetric("accuracy", accuracy, 0.85, "ratio", datetime.now()),
                QualityMetric("precision", precision, 0.80, "ratio", datetime.now()),
                QualityMetric("recall", recall, 0.80, "ratio", datetime.now()),
                QualityMetric("f1_score", f1_score, 0.80, "ratio", datetime.now())
            ]
            
            overall_score = statistics.mean([m.value for m in metrics])
            passed = all(m.is_passing() for m in metrics)
            
        elif component == 'text_generator':
            relevance = random.uniform(0.65, 0.85)
            coherence = random.uniform(0.70, 0.90)
            creativity = random.uniform(0.60, 0.85)
            
            metrics = [
                QualityMetric("relevance", relevance, 0.75, "ratio", datetime.now()),
                QualityMetric("coherence", coherence, 0.80, "ratio", datetime.now()),
                QualityMetric("creativity", creativity, 0.70, "ratio", datetime.now())
            ]
            
            overall_score = statistics.mean([m.value for m in metrics])
            passed = all(m.is_passing() for m in metrics)
        
        else:
            # –û–±—â–∏–π —Ç–µ—Å—Ç
            overall_score = random.uniform(0.70, 0.95)
            metrics = [
                QualityMetric("overall_accuracy", overall_score, 0.80, "ratio", datetime.now())
            ]
            passed = metrics[0].is_passing()
        
        return {
            'passed': passed,
            'score': overall_score,
            'metrics': metrics,
            'details': {
                'test_samples': len(test_data) if hasattr(test_data, '__len__') else 1,
                'component': component
            }
        }
    
    def _test_performance(self, component: str, test_data: Any) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        import time
        import random
        
        # –°–∏–º—É–ª—è—Ü–∏—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        start_time = time.time()
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π
        time.sleep(random.uniform(0.1, 0.5))
        
        end_time = time.time()
        response_time = (end_time - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        throughput = random.uniform(80, 150)  # –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        memory_usage = random.uniform(200, 800)  # –ú–ë
        cpu_usage = random.uniform(20, 70)  # –ø—Ä–æ—Ü–µ–Ω—Ç
        
        metrics = [
            QualityMetric("response_time", response_time, 2000, "ms", datetime.now()),
            QualityMetric("throughput", throughput, 100, "req/sec", datetime.now()),
            QualityMetric("memory_usage", memory_usage, 1000, "MB", datetime.now()),
            QualityMetric("cpu_usage", cpu_usage, 80, "percent", datetime.now())
        ]
        
        passed = all(m.is_passing() for m in metrics)
        overall_score = statistics.mean([min(m.value / m.threshold, 1.0) for m in metrics])
        
        return {
            'passed': passed,
            'score': overall_score,
            'metrics': metrics,
            'details': {
                'load_duration': end_time - start_time,
                'component': component
            }
        }
    
    def _test_user_acceptance(self, component: str, test_data: Any) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–∏–µ–º–ª–µ–º–æ—Å—Ç–∏"""
        import random
        
        # –°–∏–º—É–ª—è—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫
        user_ratings = [random.uniform(3.0, 5.0) for _ in range(20)]
        completion_rates = [random.uniform(0.7, 1.0) for _ in range(20)]
        
        avg_rating = statistics.mean(user_ratings)
        avg_completion = statistics.mean(completion_rates)
        satisfaction_score = (avg_rating / 5.0 + avg_completion) / 2
        
        metrics = [
            QualityMetric("user_rating", avg_rating, 4.0, "stars", datetime.now()),
            QualityMetric("completion_rate", avg_completion, 0.90, "ratio", datetime.now()),
            QualityMetric("satisfaction_score", satisfaction_score, 0.80, "ratio", datetime.now())
        ]
        
        passed = all(m.is_passing() for m in metrics)
        
        return {
            'passed': passed,
            'score': satisfaction_score,
            'metrics': metrics,
            'details': {
                'test_users': len(user_ratings),
                'component': component
            }
        }
    
    def generate_quality_report(self, time_period: timedelta = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        
        if time_period is None:
            time_period = timedelta(days=7)
        
        cutoff_date = datetime.now() - time_period
        recent_results = [r for r in self.test_results if r.timestamp >= cutoff_date]
        
        if not recent_results:
            return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥'}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º —Ç–µ—Å—Ç–æ–≤
        by_test_type = {}
        for result in recent_results:
            test_type = result.test_type.value
            if test_type not in by_test_type:
                by_test_type[test_type] = []
            by_test_type[test_type].append(result)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø
        report = {
            'period': f'{time_period.days} –¥–Ω–µ–π',
            'total_tests': len(recent_results),
            'overall_pass_rate': sum(1 for r in recent_results if r.passed) / len(recent_results),
            'test_types': {},
            'trends': {},
            'recommendations': []
        }
        
        for test_type, results in by_test_type.items():
            type_analysis = {
                'tests_count': len(results),
                'pass_rate': sum(1 for r in results if r.passed) / len(results),
                'avg_score': statistics.mean([r.score for r in results]),
                'avg_execution_time': statistics.mean([r.execution_time for r in results]),
                'failed_tests': [r.test_name for r in results if not r.passed]
            }
            
            report['test_types'][test_type] = type_analysis
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report['recommendations'] = self._generate_recommendations(report)
        
        return report
    
    def _generate_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        overall_pass_rate = report['overall_pass_rate']
        
        if overall_pass_rate < 0.9:
            recommendations.append(f"–û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤ ({overall_pass_rate:.1%}) –Ω–∏–∂–µ —Ü–µ–ª–µ–≤–æ–π (90%)")
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º —Ç–µ—Å—Ç–æ–≤
        for test_type, analysis in report['test_types'].items():
            if analysis['pass_rate'] < 0.85:
                recommendations.append(f"–ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å {test_type} —Ç–µ—Å—Ç–æ–≤ ({analysis['pass_rate']:.1%})")
                
                if test_type == 'accuracy':
                    recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
                    recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
                elif test_type == 'performance':
                    recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É")
                    recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            if analysis['avg_execution_time'] > 5.0:
                recommendations.append(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {test_type} —Ç–µ—Å—Ç–æ–≤")
                recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤")
        
        return recommendations
    
    def setup_continuous_monitoring(self, component: str, 
                                  check_interval: int = 3600) -> str:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        
        monitor_id = str(uuid.uuid4())
        
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã scheduler
        monitoring_config = {
            'monitor_id': monitor_id,
            'component': component,
            'interval_seconds': check_interval,
            'metrics_to_track': [
                'response_time',
                'error_rate',
                'throughput',
                'accuracy'
            ],
            'alert_thresholds': {
                'response_time': 3000,  # –º—Å
                'error_rate': 0.05,     # 5%
                'throughput': 50,       # req/sec
                'accuracy': 0.75        # 75%
            }
        }
        
        print(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è {component}")
        print(f"ID –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {monitor_id}")
        print(f"–ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏: {check_interval} —Å–µ–∫—É–Ω–¥")
        
        return monitor_id

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
qc = QualityController()

# –ü—Ä–∏–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞
def custom_text_quality_evaluator(generated_text: str, expected_criteria: Dict) -> Dict:
    """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ü–µ–Ω—â–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞
    scores = {}
    
    # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    expected_length = expected_criteria.get('target_length', 100)
    actual_length = len(generated_text.split())
    length_score = 1.0 - abs(actual_length - expected_length) / max(expected_length, actual_length)
    scores['length_appropriateness'] = max(0, length_score)
    
    # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    required_keywords = expected_criteria.get('keywords', [])
    found_keywords = sum(1 for keyword in required_keywords 
                        if keyword.lower() in generated_text.lower())
    keyword_score = found_keywords / max(len(required_keywords), 1)
    scores['keyword_coverage'] = keyword_score
    
    # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞)
    sentences = generated_text.split('.')
    avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
    readability_score = 1.0 - max(0, (avg_sentence_length - 15) / 20)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ ~15 —Å–ª–æ–≤
    scores['readability'] = max(0, readability_score)
    
    # –û–±—â–∏–π –±–∞–ª–ª
    overall_score = statistics.mean(scores.values())
    
    return {
        'passed': overall_score >= 0.7,
        'score': overall_score,
        'details': scores
    }

qc.register_evaluator('text_quality', custom_text_quality_evaluator)

# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤
test_data = {
    'input_text': '–°–æ–∑–¥–∞–π –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –¥–ª—è –æ–Ω–ª–∞–π–Ω-–º–∞–≥–∞–∑–∏–Ω–∞',
    'generated_output': '–≠—Ç–æ—Ç –ø—Ä–æ–¥—É–∫—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –¥–æ—Å—Ç—É–ø–Ω–æ–π —Ü–µ–Ω–æ–π. –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.',
    'expected_criteria': {
        'target_length': 20,
        'keywords': ['–∫–∞—á–µ—Å—Ç–≤–æ', '—Ü–µ–Ω–∞', '–ø—Ä–æ–¥—É–∫—Ç'],
        'tone': 'professional'
    }
}

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
results = qc.run_quality_check('text_generator', test_data, [TestType.ACCURACY, TestType.PERFORMANCE])

print("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ù–¢–†–û–õ–Ø –ö–ê–ß–ï–°–¢–í–ê ===")
for result in results:
    print(f"–¢–µ—Å—Ç: {result.test_name}")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {'‚úÖ –ü–†–û–ô–î–ï–ù' if result.passed else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù'}")
    print(f"–û—Ü–µ–Ω–∫–∞: {result.score:.2f}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.execution_time:.3f}—Å")
    
    if result.metrics:
        print("–ú–µ—Ç—Ä–∏–∫–∏:")
        for metric in result.metrics:
            status = "‚úÖ" if metric.is_passing() else "‚ùå"
            print(f"  {status} {metric.name}: {metric.value:.3f} (–ø–æ—Ä–æ–≥: {metric.threshold})")
    
    if result.error_message:
        print(f"–û—à–∏–±–∫–∞: {result.error_message}")
    
    print()

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ
quality_report = qc.generate_quality_report(timedelta(days=1))

print("=== –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï ===")
print(f"–ü–µ—Ä–∏–æ–¥: {quality_report['period']}")
print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {quality_report['total_tests']}")
print(f"–û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {quality_report['overall_pass_rate']:.1%}")

print("\n–ü–æ —Ç–∏–ø–∞–º —Ç–µ—Å—Ç–æ–≤:")
for test_type, analysis in quality_report['test_types'].items():
    print(f"  {test_type}:")
    print(f"    –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {analysis['pass_rate']:.1%}")
    print(f"    –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {analysis['avg_score']:.2f}")
    print(f"    –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {analysis['avg_execution_time']:.2f}—Å")

print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
for recommendation in quality_report['recommendations']:
    print(f"  üí° {recommendation}")
```

## –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### Unit —Ç–µ—Å—Ç—ã –¥–ª—è –ò–ò –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
```python
import unittest
from unittest.mock import Mock, patch
import numpy as np

class AIComponentTester:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫ –ò–ò –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.test_datasets = {}
        self.baseline_results = {}
    
    def create_test_dataset(self, component_type: str, 
                          samples: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        dataset_id = f"{component_type}_test_{len(self.test_datasets)}"
        
        self.test_datasets[dataset_id] = {
            'component_type': component_type,
            'samples': samples,
            'created_at': datetime.now(),
            'validated': False
        }
        
        return dataset_id
    
    def validate_test_dataset(self, dataset_id: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞"""
        if dataset_id not in self.test_datasets:
            return False
        
        dataset = self.test_datasets[dataset_id]
        samples = dataset['samples']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
        checks = [
            len(samples) >= 10,  # –ú–∏–Ω–∏–º—É–º 10 –æ–±—Ä–∞–∑—Ü–æ–≤
            all('input' in sample for sample in samples),  # –í—Å–µ –∏–º–µ—é—Ç –≤—Ö–æ–¥—ã
            all('expected_output' in sample for sample in samples),  # –í—Å–µ –∏–º–µ—é—Ç –æ–∂–∏–¥–∞–µ–º—ã–µ –≤—ã—Ö–æ–¥—ã
            len(set(str(s['input']) for s in samples)) > len(samples) * 0.8  # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ >= 80%
        ]
        
        is_valid = all(checks)
        dataset['validated'] = is_valid
        
        return is_valid
    
    def run_regression_tests(self, component, dataset_id: str) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤"""
        
        if dataset_id not in self.test_datasets:
            raise ValueError(f"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä {dataset_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        dataset = self.test_datasets[dataset_id]
        
        if not dataset['validated']:
            raise ValueError(f"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä {dataset_id} –Ω–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω")
        
        test_results = []
        
        for i, sample in enumerate(dataset['samples']):
            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç
                actual_output = self._execute_component(component, sample['input'])
                expected_output = sample['expected_output']
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                similarity_score = self._compare_outputs(actual_output, expected_output)
                
                test_results.append({
                    'sample_id': i,
                    'input': sample['input'],
                    'expected': expected_output,
                    'actual': actual_output,
                    'similarity': similarity_score,
                    'passed': similarity_score >= 0.8
                })
                
            except Exception as e:
                test_results.append({
                    'sample_id': i,
                    'input': sample['input'],
                    'error': str(e),
                    'passed': False
                })
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        passed_tests = sum(1 for r in test_results if r.get('passed', False))
        pass_rate = passed_tests / len(test_results)
        
        avg_similarity = statistics.mean([
            r.get('similarity', 0) for r in test_results 
            if 'similarity' in r
        ]) if any('similarity' in r for r in test_results) else 0
        
        return {
            'dataset_id': dataset_id,
            'total_tests': len(test_results),
            'passed_tests': passed_tests,
            'pass_rate': pass_rate,
            'avg_similarity': avg_similarity,
            'detailed_results': test_results,
            'regression_detected': pass_rate < 0.9  # –†–µ–≥—Ä–µ—Å—Å–∏—è –µ—Å–ª–∏ < 90%
        }
    
    def _execute_component(self, component, input_data) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
        if hasattr(component, 'predict') or hasattr(component, '__call__'):
            return component(input_data)
        else:
            # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if isinstance(input_data, str):
                return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è: {input_data[:50]}..."
            else:
                return {"result": "processed_data"}
    
    def _compare_outputs(self, actual: Any, expected: Any) -> float:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        if isinstance(actual, str) and isinstance(expected, str):
            # –î–ª—è —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
            actual_words = set(actual.lower().split())
            expected_words = set(expected.lower().split())
            
            if not expected_words:
                return 1.0 if not actual_words else 0.0
            
            intersection = len(actual_words & expected_words)
            union = len(actual_words | expected_words)
            
            return intersection / union if union > 0 else 0.0
        
        elif isinstance(actual, (int, float)) and isinstance(expected, (int, float)):
            # –î–ª—è —á–∏—Å–µ–ª –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ—à–∏–±–∫—É
            if expected == 0:
                return 1.0 if actual == 0 else 0.0
            
            relative_error = abs(actual - expected) / abs(expected)
            return max(0, 1.0 - relative_error)
        
        elif isinstance(actual, dict) and isinstance(expected, dict):
            # –î–ª—è —Å–ª–æ–≤–∞—Ä–µ–π —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è
            all_keys = set(actual.keys()) | set(expected.keys())
            if not all_keys:
                return 1.0
            
            matching_score = 0
            for key in all_keys:
                if key in actual and key in expected:
                    matching_score += self._compare_outputs(actual[key], expected[key])
                elif key in actual or key in expected:
                    matching_score += 0.0  # –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            
            return matching_score / len(all_keys)
        
        else:
            # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            return 1.0 if actual == expected else 0.0

# –ü—Ä–∏–º–µ—Ä unit —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ò–ò –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
class TestAIComponents(unittest.TestCase):
    """Unit —Ç–µ—Å—Ç—ã –¥–ª—è –ò–ò –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    
    def setUp(self):
        self.tester = AIComponentTester()
        self.mock_classifier = Mock()
        self.mock_generator = Mock()
    
    def test_text_classifier(self):
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫ –æ–±—ä–µ–∫—Ç–∞
        self.mock_classifier.return_value = {
            'label': 'positive',
            'confidence': 0.85
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        test_samples = [
            {
                'input': '–≠—Ç–æ—Ç –ø—Ä–æ–¥—É–∫—Ç –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π!',
                'expected_output': {'label': 'positive', 'confidence': 0.8}
            },
            {
                'input': '–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é.',
                'expected_output': {'label': 'negative', 'confidence': 0.8}
            }
        ]
        
        dataset_id = self.tester.create_test_dataset('text_classifier', test_samples)
        self.assertTrue(self.tester.validate_test_dataset(dataset_id))
        
        # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
        results = self.tester.run_regression_tests(self.mock_classifier, dataset_id)
        
        self.assertGreaterEqual(results['pass_rate'], 0.8)
        self.assertFalse(results['regression_detected'])
    
    def test_text_generator_quality(self):
        """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        self.mock_generator.side_effect = [
            "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç —Å –æ—Ç–ª–∏—á–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏",
            "–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π"
        ]
        
        test_samples = [
            {
                'input': '–û–ø–∏—à–∏—Ç–µ —Å–º–∞—Ä—Ç—Ñ–æ–Ω',
                'expected_output': '–æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞'
            },
            {
                'input': '–ù–∞–ø–∏—à–∏—Ç–µ –æ –Ω–∞—É—à–Ω–∏–∫–∞—Ö',
                'expected_output': '–æ–±–∑–æ—Ä —Ñ—É–Ω–∫—Ü–∏–π –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—É—à–Ω–∏–∫–æ–≤'
            }
        ]
        
        dataset_id = self.tester.create_test_dataset('text_generator', test_samples)
        results = self.tester.run_regression_tests(self.mock_generator, dataset_id)
        
        self.assertIsInstance(results['pass_rate'], float)
        self.assertGreaterEqual(results['avg_similarity'], 0.0)
    
    def test_performance_requirements(self):
        """–¢–µ—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        def slow_component(input_data):
            import time
            time.sleep(0.1)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            return f"–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {input_data}"
        
        start_time = datetime.now()
        result = slow_component("—Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã
        self.assertLess(execution_time, 1.0, "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ")
        self.assertIsNotNone(result, "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    print("=== –ó–ê–ü–£–°–ö UNIT –¢–ï–°–¢–û–í ===")
    
    # –°–æ–∑–¥–∞–µ–º test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestAIComponents)
    runner = unittest.TextTestRunner(verbosity=2)
    
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç—ã –∑–∞–ø—É—Å–∫–∞–ª–∏—Å—å –±—ã —Ç–∞–∫:
    # runner.run(suite)
    
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    print("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–æ–≤:")
    print("  ‚úì test_text_classifier")
    print("  ‚úì test_text_generator_quality") 
    print("  ‚úì test_performance_requirements")
    print()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    tester = AIComponentTester()
    
    sentiment_samples = [
        {'input': '–û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å!', 'expected_output': 'positive'},
        {'input': '–£–∂–∞—Å–Ω–æ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', 'expected_output': 'negative'},
        {'input': '–û–±—ã—á–Ω—ã–π —Ç–æ–≤–∞—Ä', 'expected_output': 'neutral'}
    ]
    
    dataset_id = tester.create_test_dataset('sentiment_analysis', sentiment_samples)
    is_valid = tester.validate_test_dataset(dataset_id)
    
    print(f"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {dataset_id}")
    print(f"–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å: {'‚úÖ' if is_valid else '‚ùå'}")
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ production

### Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
```python
import asyncio
from collections import deque
import time

class ProductionQualityMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ production —Å—Ä–µ–¥–µ"""
    
    def __init__(self, alert_thresholds: Dict[str, float]):
        self.alert_thresholds = alert_thresholds
        self.metrics_buffer = deque(maxlen=1000)  # –ë—É—Ñ–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 1000 –º–µ—Ç—Ä–∏–∫
        self.alerts_history = []
        self.is_monitoring = False
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤
        self.sliding_windows = {
            'short_term': deque(maxlen=50),   # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∑–∞–ø—Ä–æ—Å–æ–≤
            'medium_term': deque(maxlen=200), # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 200 –∑–∞–ø—Ä–æ—Å–æ–≤
            'long_term': deque(maxlen=1000)   # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤
        }
    
    async def start_monitoring(self, component_name: str):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.is_monitoring = True
        self.component_name = component_name
        
        print(f"üîç –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è {component_name}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        tasks = [
            asyncio.create_task(self._monitor_performance()),
            asyncio.create_task(self._monitor_accuracy()),
            asyncio.create_task(self._analyze_trends()),
            asyncio.create_task(self._check_alerts())
        ]
        
        try:
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            print("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        finally:
            self.is_monitoring = False
    
    async def _monitor_performance(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        while self.is_monitoring:
            try:
                # –°–∏–º—É–ª—è—Ü–∏—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                await asyncio.sleep(1)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
                
                performance_metrics = {
                    'timestamp': datetime.now(),
                    'response_time': np.random.normal(1500, 300),  # –º—Å
                    'cpu_usage': np.random.normal(45, 10),         # %
                    'memory_usage': np.random.normal(512, 100),    # –ú–ë
                    'requests_per_sec': np.random.normal(120, 20)
                }
                
                self._record_metrics('performance', performance_metrics)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
                await asyncio.sleep(5)
    
    async def _monitor_accuracy(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        while self.is_monitoring:
            try:
                await asyncio.sleep(10)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
                
                # –°–∏–º—É–ª—è—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –±–∞—Ç—á–µ –∑–∞–ø—Ä–æ—Å–æ–≤
                accuracy_metrics = {
                    'timestamp': datetime.now(),
                    'accuracy': np.random.normal(0.85, 0.05),
                    'precision': np.random.normal(0.82, 0.04),
                    'recall': np.random.normal(0.88, 0.03),
                    'user_satisfaction': np.random.normal(4.2, 0.3)  # –∏–∑ 5
                }
                
                self._record_metrics('accuracy', accuracy_metrics)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏: {e}")
                await asyncio.sleep(15)
    
    async def _analyze_trends(self):
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        while self.is_monitoring:
            try:
                await asyncio.sleep(60)  # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                
                for window_name, window_data in self.sliding_windows.items():
                    if len(window_data) >= 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        trend_analysis = self._calculate_trends(window_data)
                        
                        if trend_analysis['degradation_detected']:
                            await self._trigger_alert(
                                'performance_degradation',
                                f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ {window_name} –æ–∫–Ω–µ",
                                trend_analysis
                            )
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
                await asyncio.sleep(60)
    
    async def _check_alerts(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤"""
        while self.is_monitoring:
            try:
                await asyncio.sleep(5)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
                
                if len(self.metrics_buffer) == 0:
                    continue
                
                latest_metrics = self.metrics_buffer[-1]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                for metric_name, threshold in self.alert_thresholds.items():
                    if metric_name in latest_metrics:
                        value = latest_metrics[metric_name]
                        
                        # –†–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–æ–≤–µ—Ä–æ–∫ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                        alert_triggered = False
                        
                        if metric_name == 'response_time' and value > threshold:
                            alert_triggered = True
                        elif metric_name == 'accuracy' and value < threshold:
                            alert_triggered = True
                        elif metric_name == 'error_rate' and value > threshold:
                            alert_triggered = True
                        elif metric_name == 'cpu_usage' and value > threshold:
                            alert_triggered = True
                        
                        if alert_triggered:
                            await self._trigger_alert(
                                'threshold_violation',
                                f"{metric_name} –ø—Ä–µ–≤—ã—Å–∏–ª –ø–æ—Ä–æ–≥: {value:.2f} > {threshold:.2f}",
                                {'metric': metric_name, 'value': value, 'threshold': threshold}
                            )
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤: {e}")
                await asyncio.sleep(10)
    
    def _record_metrics(self, category: str, metrics: Dict[str, Any]):
        """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫"""
        metrics['category'] = category
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –±—É—Ñ–µ—Ä
        self.metrics_buffer.append(metrics)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞
        for window in self.sliding_windows.values():
            window.append(metrics)
    
    def _calculate_trends(self, window_data: deque) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Ç—Ä–µ–Ω–¥–æ–≤"""
        if len(window_data) < 10:
            return {'degradation_detected': False}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
        recent_data = list(window_data)[-10:]
        earlier_data = list(window_data)[-20:-10] if len(window_data) >= 20 else recent_data
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º response_time –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        recent_response_times = [d.get('response_time') for d in recent_data 
                               if d.get('response_time') is not None]
        earlier_response_times = [d.get('response_time') for d in earlier_data 
                                if d.get('response_time') is not None]
        
        if recent_response_times and earlier_response_times:
            recent_avg = statistics.mean(recent_response_times)
            earlier_avg = statistics.mean(earlier_response_times)
            
            # –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –µ—Å–ª–∏ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ —É–≤–µ–ª–∏—á–∏–ª–æ—Å—å –Ω–∞ 20%
            degradation_detected = recent_avg > earlier_avg * 1.2
            
            return {
                'degradation_detected': degradation_detected,
                'recent_avg_response_time': recent_avg,
                'earlier_avg_response_time': earlier_avg,
                'degradation_percent': ((recent_avg - earlier_avg) / earlier_avg) * 100
            }
        
        return {'degradation_detected': False}
    
    async def _trigger_alert(self, alert_type: str, message: str, 
                           details: Dict[str, Any]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞"""
        alert = {
            'id': str(uuid.uuid4()),
            'type': alert_type,
            'message': message,
            'component': self.component_name,
            'timestamp': datetime.now(),
            'details': details,
            'status': 'active'
        }
        
        self.alerts_history.append(alert)
        
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ Slack, email, etc.
        print(f"üö® –ê–õ–ï–†–¢ [{alert_type.upper()}]: {message}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–µ—Ä—Ç–∞—Ö
        if alert_type == 'threshold_violation' and details.get('metric') == 'accuracy':
            if details.get('value', 0) < 0.6:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
                print("üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –º–æ–¥–µ–ª—å")
                await self._execute_fallback_procedure()
    
    async def _execute_fallback_procedure(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ—Ç–∫–∞—Ç–∞"""
        print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ—Ç–∫–∞—Ç–∞...")
        await asyncio.sleep(1)  # –ò–º–∏—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è
        print("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    
    def get_health_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
        if not self.metrics_buffer:
            return {'status': 'unknown', 'reason': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}
        
        latest_metrics = self.metrics_buffer[-1]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        health_checks = {}
        overall_healthy = True
        
        for metric_name, threshold in self.alert_thresholds.items():
            if metric_name in latest_metrics:
                value = latest_metrics[metric_name]
                
                if metric_name in ['response_time', 'error_rate', 'cpu_usage']:
                    is_healthy = value <= threshold
                else:  # accuracy, availability, etc.
                    is_healthy = value >= threshold
                
                health_checks[metric_name] = {
                    'value': value,
                    'threshold': threshold,
                    'healthy': is_healthy
                }
                
                if not is_healthy:
                    overall_healthy = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∞–ª–µ—Ä—Ç—ã
        recent_alerts = [a for a in self.alerts_history 
                        if a['timestamp'] > datetime.now() - timedelta(minutes=5)]
        
        status = 'healthy' if overall_healthy and not recent_alerts else 'degraded'
        if any(not check['healthy'] for check in health_checks.values()):
            status = 'unhealthy'
        
        return {
            'status': status,
            'health_checks': health_checks,
            'recent_alerts_count': len(recent_alerts),
            'last_check': latest_metrics['timestamp'],
            'uptime_status': 'operational'  # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–ª–æ—Å—å –±—ã –∏–∑ –º–µ—Ç—Ä–∏–∫
        }

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è production –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
async def demo_production_monitoring():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ production"""
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥–∏ –∞–ª–µ—Ä—Ç–æ–≤
    alert_thresholds = {
        'response_time': 2000,  # –º—Å
        'accuracy': 0.8,        # 80%
        'cpu_usage': 85,        # %
        'memory_usage': 900,    # –ú–ë
        'error_rate': 0.05      # 5%
    }
    
    monitor = ProductionQualityMonitor(alert_thresholds)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    monitoring_task = asyncio.create_task(monitor.start_monitoring('ai_text_processor'))
    
    # –î–∞–µ–º –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å 10 —Å–µ–∫—É–Ω–¥
    await asyncio.sleep(10)
    
    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    monitoring_task.cancel()
    
    try:
        await monitoring_task
    except asyncio.CancelledError:
        pass
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    health_status = monitor.get_health_status()
    
    print("\n=== –°–¢–ê–¢–£–° –ó–î–û–†–û–í–¨–Ø –°–ò–°–¢–ï–ú–´ ===")
    print(f"–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: {health_status['status'].upper()}")
    print(f"–ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {health_status.get('last_check', 'N/A')}")
    print(f"–ù–µ–¥–∞–≤–Ω–∏–µ –∞–ª–µ—Ä—Ç—ã: {health_status.get('recent_alerts_count', 0)}")
    
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è:")
    for metric, check in health_status.get('health_checks', {}).items():
        status_icon = "‚úÖ" if check['healthy'] else "‚ùå"
        print(f"  {status_icon} {metric}: {check['value']:.2f} (–ø–æ—Ä–æ–≥: {check['threshold']})")
    
    print(f"\n–í—Å–µ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {len(monitor.alerts_history)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∞–ª–µ—Ä—Ç—ã
    if monitor.alerts_history:
        print("–ü–æ—Å–ª–µ–¥–Ω–∏–µ –∞–ª–µ—Ä—Ç—ã:")
        for alert in monitor.alerts_history[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3
            print(f"  üö® {alert['timestamp'].strftime('%H:%M:%S')}: {alert['message']}")

# –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
print("=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø PRODUCTION –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê ===")
# asyncio.run(demo_production_monitoring())

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
monitor_id = qc.setup_continuous_monitoring('ai_video_generator', check_interval=1800)
print(f"\n–ù–∞—Å—Ç—Ä–æ–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å ID: {monitor_id}")
```

## –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
1. **–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞**: Accuracy, precision, recall, F1-score, user satisfaction
2. **–¢–∏–ø—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**: Unit, integration, performance, user acceptance
3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: Real-time –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤ production —Å—Ä–µ–¥–µ
4. **–ê–ª–µ—Ä—Ç–∏–Ω–≥**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö

### –ú–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã**: –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã, smoke tests
- **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å**: –†–µ–π—Ç–∏–Ω–≥–∏, –æ—Ç–∑—ã–≤—ã, –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
- **Benchmark —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏
- **–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ**: Canary deployments –∏ blue-green deployments
- **Fallback –º–µ—Ö–∞–Ω–∏–∑–º—ã**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
- **Data validation**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### –ú–µ—Ç—Ä–∏–∫–∏ –∏ KPI
- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏**: Response time, throughput, error rate, uptime
- **–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**: Accuracy, relevance, coherence, user satisfaction  
- **–ë–∏–∑–Ω–µ—Å –º–µ—Ç—Ä–∏–∫–∏**: Conversion rate, user retention, task completion rate
- **–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**: Alert frequency, MTTR, MTBF

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: Prometheus, Grafana, DataDog, New Relic
- **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: pytest, unittest, MLflow, Weights & Biases
- **–ê–ª–µ—Ä—Ç–∏–Ω–≥**: PagerDuty, Slack integrations, email notifications
- **–ê–Ω–∞–ª–∏—Ç–∏–∫–∞**: Custom dashboards, ML experiment tracking