"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è AI SEO Architects —Å ChromaDB
–£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –ø–æ–∏—Å–∫–æ–º –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å ChromaDB –∏ OpenAI Embeddings
"""
import os
import uuid
from typing import Dict, List, Optional, Any
from pathlib import Path
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from core.config import config


class ChromaVectorStore:
    """ChromaDB-based –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å OpenAI Embeddings"""
    
    def __init__(self, collection_name: str, embeddings_model: Optional[OpenAIEmbeddings]):
        self.collection_name = collection_name
        self.embeddings_model = embeddings_model
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB –∫–ª–∏–µ–Ω—Ç
        chroma_settings = Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=config.VECTOR_STORE_PATH,
            anonymized_telemetry=False
        )
        
        self.client = chromadb.Client(chroma_settings)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=self._get_embedding_function()
        )
        
        print(f"‚úÖ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def _get_embedding_function(self):
        """–°–æ–∑–¥–∞–µ—Ç embedding —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è ChromaDB"""
        if self.embeddings_model is None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é ChromaDB
            return None
        
        # –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è OpenAI Embeddings
        class OpenAIEmbeddingFunction:
            def __init__(self, embeddings_model):
                self.embeddings_model = embeddings_model
            
            def __call__(self, texts):
                if isinstance(texts, str):
                    texts = [texts]
                return self.embeddings_model.embed_documents(texts)
        
        return OpenAIEmbeddingFunction(self.embeddings_model)
    
    def add_documents(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ChromaDB
            texts = [doc.page_content for doc in documents]
            metadatas = [doc.metadata for doc in documents]
            ids = [str(uuid.uuid4()) for _ in documents]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection.add(
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}'")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB: {e}")
            return False
    
    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB"""
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ ChromaDB
            results = self.collection.query(
                query_texts=[query],
                n_results=min(k, self.collection.count())
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Document –æ–±—ä–µ–∫—Ç—ã
            documents = []
            if results['documents'] and len(results['documents'][0]) > 0:
                for i in range(len(results['documents'][0])):
                    doc = Document(
                        page_content=results['documents'][0][i],
                        metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}
                    )
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ ChromaDB: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            count = self.collection.count()
            return {
                "name": self.collection_name,
                "documents_count": count,
                "status": "active"
            }
        except Exception as e:
            return {
                "name": self.collection_name,
                "error": str(e),
                "status": "error"
            }


class ChromaKnowledgeManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ ChromaDB"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–Ω–∞–Ω–∏–π —Å ChromaDB"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.RAG_CHUNK_SIZE,
            chunk_overlap=config.RAG_CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
        self.vector_stores: Dict[str, ChromaVectorStore] = {}
        self.knowledge_base_path = Path(config.KNOWLEDGE_BASE_PATH)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI Embeddings
        try:
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=config.OPENAI_API_KEY,
                model="text-embedding-ada-002"
            )
            print("‚úÖ OpenAI Embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI Embeddings: {e}")
            self.embeddings = None
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs(config.VECTOR_STORE_PATH, exist_ok=True)
    
    def load_agent_knowledge(self, agent_name: str, agent_level: str) -> ChromaVectorStore:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –≤ ChromaDB
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'lead_qualification')
            agent_level: –£—Ä–æ–≤–µ–Ω—å –∞–≥–µ–Ω—Ç–∞ ('executive', 'management', 'operational')
            
        Returns:
            ChromaVectorStore: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –∑–Ω–∞–Ω–∏—è–º–∏ –∞–≥–µ–Ω—Ç–∞
        """
        if agent_name in self.vector_stores:
            return self.vector_stores[agent_name]
        
        # –°–æ–∑–¥–∞–µ–º ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        collection_name = f"{agent_level}_{agent_name}"
        vector_store = ChromaVectorStore(collection_name, self.embeddings)
        
        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞  
        knowledge_path = self.knowledge_base_path / agent_level
        
        documents = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º markdown —Ñ–∞–π–ª—ã —Å –∑–Ω–∞–Ω–∏—è–º–∏
        for md_file in knowledge_path.glob("*.md"):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∞–π–ª–∞ –∞–≥–µ–Ω—Ç—É
            if agent_name in md_file.stem or md_file.stem.replace('_', '') in agent_name.replace('_', ''):
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                        chunks = self.text_splitter.split_text(content)
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    "agent": agent_name,
                                    "level": agent_level,
                                    "source": md_file.name,
                                    "chunk_id": i
                                }
                            )
                            documents.append(doc)
                            
                        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {md_file.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ ChromaDB
        if documents:
            success = vector_store.add_documents(documents)
            if success:
                self.vector_stores[agent_name] = vector_store
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ ChromaDB –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è {agent_name} ({len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                return vector_store
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ChromaDB —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è {agent_name}")
                return None
        else:
            print(f"‚ö†Ô∏è –ó–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {knowledge_path}")
            return None
    
    def search_knowledge(self, agent_name: str, query: str, k: int = None) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ ChromaDB
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if agent_name not in self.vector_stores:
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []
            
        k = k or config.RAG_TOP_K
        
        try:
            results = self.vector_stores[agent_name].similarity_search(query, k=k)
            return results
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
            return []
    
    def get_knowledge_context(self, agent_name: str, query: str, k: int = None) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç–µ
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π
        """
        relevant_docs = self.search_knowledge(agent_name, query, k)
        
        if not relevant_docs:
            return ""
        
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            source = doc.metadata.get('source', 'unknown')
            content = doc.page_content.strip()
            context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def initialize_all_agents_knowledge(self) -> Dict[str, bool]:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ ChromaDB
        
        Returns:
            Dict[str, bool]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        """
        results = {}
        
        # –°–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏—Ö —É—Ä–æ–≤–Ω–µ–π
        agent_mappings = {
            # Executive level
            'chief_seo_strategist': 'executive',
            'business_development_director': 'executive',
            
            # Management level  
            'task_coordination': 'management',
            'sales_operations_manager': 'management',
            'technical_seo_operations_manager': 'management', 
            'client_success_manager': 'management',
            
            # Operational level
            'lead_qualification': 'operational',
            'sales_conversation': 'operational',
            'proposal_generation': 'operational',
            'technical_seo_auditor': 'operational',
            'content_strategy': 'operational',
            'link_building': 'operational',
            'competitive_analysis': 'operational',
            'reporting': 'operational'
        }
        
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ ChromaDB...")
        
        for agent_name, agent_level in agent_mappings.items():
            try:
                vector_store = self.load_agent_knowledge(agent_name, agent_level)
                results[agent_name] = vector_store is not None
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
                results[agent_name] = False
        
        successful_count = sum(results.values())
        total_count = len(results)
        
        print(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_count}/{total_count} –∞–≥–µ–Ω—Ç–æ–≤")
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ChromaDB –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        stats = {
            "vector_stores_count": len(self.vector_stores),
            "embeddings_available": self.embeddings is not None,
            "stores": {}
        }
        
        for agent_name, store in self.vector_stores.items():
            stats["stores"][agent_name] = store.get_collection_stats()
        
        return stats


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∑–Ω–∞–Ω–∏–π —Å ChromaDB
knowledge_manager = ChromaKnowledgeManager()