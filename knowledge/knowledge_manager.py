"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è AI SEO Architects
–£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –ø–æ–∏—Å–∫–æ–º –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å FAISS –∏ OpenAI Embeddings
"""
import os
import pickle
import numpy as np
from typing import Dict, List, Optional, Any
from pathlib import Path
import faiss
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from core.config import config

class FAISSVectorStore:
    """FAISS-based –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å OpenAI Embeddings"""
    
    def __init__(self, documents: List[Document], embeddings_model: Optional[OpenAIEmbeddings]):
        self.documents = documents
        self.embeddings_model = embeddings_model
        self.index = None
        self.embeddings_cache = None
        self.dimension = 1536  # OpenAI text-embedding-ada-002 dimension
        
        if documents:
            self._build_index()
    
    def _build_index(self):
        """–°—Ç—Ä–æ–∏–º FAISS –∏–Ω–¥–µ–∫—Å —Å OpenAI —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å OpenAI Embeddings
        if self.embeddings_model is None:
            print("‚ö†Ô∏è OpenAI Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
            self._build_simple_index()
            return
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            texts = [doc.page_content for doc in self.documents]
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ OpenAI
            embeddings_list = self.embeddings_model.embed_documents(texts)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
            self.embeddings_cache = np.array(embeddings_list).astype('float32')
            
            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å (L2 distance)
            self.index = faiss.IndexFlatL2(self.dimension)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –∏–Ω–¥–µ–∫—Å
            self.index.add(self.embeddings_cache)
            
            print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
            self._build_simple_index()
    
    def _build_simple_index(self):
        """Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É –µ—Å–ª–∏ OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É...")
        self.simple_index = {}
        for i, doc in enumerate(self.documents):
            words = set(doc.page_content.lower().split())
            self.simple_index[i] = words
    
    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.index is not None:
            return self._faiss_search(query, k)
        else:
            return self._simple_search(query, k)
    
    def _faiss_search(self, query: str, k: int) -> List[Document]:
        """FAISS –ø–æ–∏—Å–∫ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        if self.embeddings_model is None:
            return self._simple_search(query, k)
            
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embeddings_model.embed_query(query)
            query_vector = np.array([query_embedding]).astype('float32')
            
            # –ü–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ
            scores, indices = self.index.search(query_vector, min(k, len(self.documents)))
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx != -1 and idx < len(self.documents):  # –í–∞–ª–∏–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                    results.append(self.documents[idx])
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FAISS –ø–æ–∏—Å–∫–∞: {e}")
            return self._simple_search(query, k)
    
    def _simple_search(self, query: str, k: int) -> List[Document]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback"""
        if not hasattr(self, 'simple_index'):
            self._build_simple_index()
        
        query_words = set(query.lower().split())
        scores = []
        
        for i, doc_words in self.simple_index.items():
            intersection = query_words & doc_words
            if intersection:
                score = len(intersection) / len(query_words | doc_words)
                scores.append((score, i))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø-k
        scores.sort(reverse=True)
        results = []
        for score, i in scores[:k]:
            if score > 0.1:
                results.append(self.documents[i])
        
        return results
    
    def save_index(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫"""
        try:
            if self.index is not None:
                faiss.write_index(self.index, f"{path}/faiss.index")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = {
                    'documents': self.documents,
                    'embeddings': self.embeddings_cache.tolist() if self.embeddings_cache is not None else None
                }
                
                with open(f"{path}/metadata.pkl", 'wb') as f:
                    pickle.dump(metadata, f)
                
                print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    def load_index(self, path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞"""
        try:
            index_path = f"{path}/faiss.index"
            metadata_path = f"{path}/metadata.pkl"
            
            if os.path.exists(index_path) and os.path.exists(metadata_path):
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
                self.index = faiss.read_index(index_path)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                self.documents = metadata['documents']
                if metadata['embeddings']:
                    self.embeddings_cache = np.array(metadata['embeddings']).astype('float32')
                
                print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {path}")
                return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
        
        return False


class KnowledgeManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–Ω–∞–Ω–∏–π"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.RAG_CHUNK_SIZE,
            chunk_overlap=config.RAG_CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
        self.vector_stores: Dict[str, FAISSVectorStore] = {}
        self.knowledge_base_path = Path(config.KNOWLEDGE_BASE_PATH)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI Embeddings
        try:
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=config.OPENAI_API_KEY,
                model="text-embedding-ada-002"
            )
            print("‚úÖ OpenAI Embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI Embeddings: {e}")
            self.embeddings = None
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs(config.VECTOR_STORE_PATH, exist_ok=True)
        
    def load_agent_knowledge(self, agent_name: str, agent_level: str) -> FAISSVectorStore:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'lead_qualification')
            agent_level: –£—Ä–æ–≤–µ–Ω—å –∞–≥–µ–Ω—Ç–∞ ('executive', 'management', 'operational')
            
        Returns:
            FAISSVectorStore: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –∑–Ω–∞–Ω–∏—è–º–∏ –∞–≥–µ–Ω—Ç–∞
        """
        if agent_name in self.vector_stores:
            return self.vector_stores[agent_name]
            
        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞
        knowledge_path = self.knowledge_base_path / agent_level
        
        documents = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º markdown —Ñ–∞–π–ª—ã —Å –∑–Ω–∞–Ω–∏—è–º–∏
        for md_file in knowledge_path.glob("*.md"):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∞–π–ª–∞ –∞–≥–µ–Ω—Ç—É
            if agent_name in md_file.stem or md_file.stem.replace('_', '') in agent_name.replace('_', ''):
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                        chunks = self.text_splitter.split_text(content)
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    "agent": agent_name,
                                    "level": agent_level,
                                    "source": md_file.name,
                                    "chunk_id": i
                                }
                            )
                            documents.append(doc)
                            
                        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {md_file.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
        
        # –°–æ–∑–¥–∞–µ–º FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if documents:
            if self.embeddings is None:
                print(f"‚ö†Ô∏è OpenAI Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                # –°–æ–∑–¥–∞–µ–º FAISSVectorStore –±–µ–∑ embeddings (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω simple fallback)
                vector_store = FAISSVectorStore(documents, None)
            else:
                vector_store = FAISSVectorStore(documents, self.embeddings)
                
            self.vector_stores[agent_name] = vector_store
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è {agent_name} ({len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            index_path = f"{config.VECTOR_STORE_PATH}/{agent_name}"
            if vector_store.load_index(index_path):
                print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è {agent_name}")
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
                os.makedirs(index_path, exist_ok=True)
                vector_store.save_index(index_path)
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è {agent_name}")
            
            return vector_store
        else:
            print(f"‚ö†Ô∏è –ó–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {knowledge_path}")
            return None
    
    def search_knowledge(self, agent_name: str, query: str, k: int = None) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if agent_name not in self.vector_stores:
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []
            
        k = k or config.RAG_TOP_K
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
            results = self.vector_stores[agent_name].similarity_search(query, k=k)
            return results
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
            return []
    
    def add_knowledge(self, agent_name: str, content: str, metadata: Dict[str, Any]) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –≤ –±–∞–∑—É –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–Ω–∞–Ω–∏–π
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        if agent_name not in self.vector_stores:
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_text(content)
        
        documents = []
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={**metadata, "chunk_id": i}
            )
            documents.append(doc)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–ø–∏—Å–æ–∫
        current_docs = self.vector_stores[agent_name].documents
        current_docs.extend(documents)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        if self.embeddings is not None:
            self.vector_stores[agent_name] = FAISSVectorStore(current_docs, self.embeddings)
        else:
            self.vector_stores[agent_name] = FAISSVectorStore(current_docs, None)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        index_path = f"{config.VECTOR_STORE_PATH}/{agent_name}"
        os.makedirs(index_path, exist_ok=True)
        self.vector_stores[agent_name].save_index(index_path)
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –∑–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name}, –∏–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª–µ–Ω")
    
    def get_knowledge_context(self, agent_name: str, query: str, k: int = None) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç–µ
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π
        """
        relevant_docs = self.search_knowledge(agent_name, query, k)
        
        if not relevant_docs:
            return ""
        
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            source = doc.metadata.get('source', 'unknown')
            content = doc.page_content.strip()
            context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def initialize_all_agents_knowledge(self) -> Dict[str, bool]:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        
        Returns:
            Dict[str, bool]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        """
        results = {}
        
        # –°–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏—Ö —É—Ä–æ–≤–Ω–µ–π
        agent_mappings = {
            # Executive level
            'chief_seo_strategist': 'executive',
            'business_development_director': 'executive',
            
            # Management level  
            'task_coordination': 'management',
            'sales_operations_manager': 'management',
            'technical_seo_operations_manager': 'management', 
            'client_success_manager': 'management',
            
            # Operational level
            'lead_qualification': 'operational',
            'sales_conversation': 'operational',
            'proposal_generation': 'operational',
            'technical_seo_auditor': 'operational',
            'content_strategy': 'operational',
            'link_building': 'operational',
            'competitive_analysis': 'operational',
            'reporting': 'operational'
        }
        
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤...")
        
        for agent_name, agent_level in agent_mappings.items():
            try:
                vector_store = self.load_agent_knowledge(agent_name, agent_level)
                results[agent_name] = vector_store is not None
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
                results[agent_name] = False
        
        successful_count = sum(results.values())
        total_count = len(results)
        
        print(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_count}/{total_count} –∞–≥–µ–Ω—Ç–æ–≤")
        
        return results

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∑–Ω–∞–Ω–∏–π
knowledge_manager = KnowledgeManager()
