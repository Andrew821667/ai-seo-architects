"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è AI SEO Architects
–£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –ø–æ–∏—Å–∫–æ–º –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤
"""
import os
from typing import Dict, List, Optional, Any
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from core.config import config

# –ü—Ä–æ—Å—Ç–∞—è in-memory –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
import re
import math

class SimpleVectorStore:
    """–ü—Ä–æ—Å—Ç–∞—è in-memory –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    def __init__(self, documents: List[Document]):
        self.documents = documents
        self.index = {}
        self._build_index()
    
    def _build_index(self):
        """–°—Ç—Ä–æ–∏–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF"""
        for i, doc in enumerate(self.documents):
            words = re.findall(r'\b\w+\b', doc.page_content.lower())
            self.index[i] = set(words)
    
    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é —Å–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º scoring"""
        query_words = set(re.findall(r'\b\w+\b', query.lower()))
        scores = []
        
        for i, doc_words in self.index.items():
            doc_content = self.documents[i].page_content.lower()
            
            # 1. –ñ–∞–∫–∞—Ä–¥–æ–≤–æ —Å—Ö–æ–¥—Å—Ç–≤–æ
            intersection = query_words & doc_words
            union = query_words | doc_words
            jaccard_score = len(intersection) / len(union) if union else 0
            
            # 2. –ü—Ä—è–º–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–≤ (–±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
            direct_matches = sum(1 for word in query_words if word in doc_content)
            direct_score = direct_matches / len(query_words) if query_words else 0
            
            # 3. –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            phrase_bonus = 0
            query_text = query.lower()
            if query_text in doc_content:
                phrase_bonus = 0.3
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
            final_score = (jaccard_score * 0.4) + (direct_score * 0.5) + phrase_bonus
            scores.append((final_score, i))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É –∏ –±–µ—Ä–µ–º —Ç–æ–ø-k
        scores.sort(reverse=True)
        results = []
        for score, i in scores[:k]:
            if score > 0.05:  # –°–Ω–∏–∂–∞–µ–º threshold –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                results.append(self.documents[i])
        
        return results


class KnowledgeManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–Ω–∞–Ω–∏–π"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.RAG_CHUNK_SIZE,
            chunk_overlap=config.RAG_CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
        self.vector_stores: Dict[str, SimpleVectorStore] = {}
        self.knowledge_base_path = Path(config.KNOWLEDGE_BASE_PATH)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs(os.path.dirname(config.CHROMA_PERSIST_DIR), exist_ok=True)
        
    def load_agent_knowledge(self, agent_name: str, agent_level: str) -> SimpleVectorStore:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'lead_qualification')
            agent_level: –£—Ä–æ–≤–µ–Ω—å –∞–≥–µ–Ω—Ç–∞ ('executive', 'management', 'operational')
            
        Returns:
            SimpleVectorStore: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –∑–Ω–∞–Ω–∏—è–º–∏ –∞–≥–µ–Ω—Ç–∞
        """
        if agent_name in self.vector_stores:
            return self.vector_stores[agent_name]
            
        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞
        knowledge_path = self.knowledge_base_path / agent_level
        
        documents = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º markdown —Ñ–∞–π–ª—ã —Å –∑–Ω–∞–Ω–∏—è–º–∏
        for md_file in knowledge_path.glob("*.md"):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∞–π–ª–∞ –∞–≥–µ–Ω—Ç—É
            if agent_name in md_file.stem or md_file.stem.replace('_', '') in agent_name.replace('_', ''):
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                        chunks = self.text_splitter.split_text(content)
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    "agent": agent_name,
                                    "level": agent_level,
                                    "source": md_file.name,
                                    "chunk_id": i
                                }
                            )
                            documents.append(doc)
                            
                        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {md_file.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if documents:
            vector_store = SimpleVectorStore(documents)
            self.vector_stores[agent_name] = vector_store
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è {agent_name} ({len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            return vector_store
        else:
            print(f"‚ö†Ô∏è –ó–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {knowledge_path}")
            return None
    
    def search_knowledge(self, agent_name: str, query: str, k: int = None) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if agent_name not in self.vector_stores:
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []
            
        k = k or config.RAG_TOP_K
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
            results = self.vector_stores[agent_name].similarity_search(query, k=k)
            return results
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
            return []
    
    def add_knowledge(self, agent_name: str, content: str, metadata: Dict[str, Any]) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –≤ –±–∞–∑—É –∞–≥–µ–Ω—Ç–∞
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–Ω–∞–Ω–∏–π
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        if agent_name not in self.vector_stores:
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_text(content)
        
        documents = []
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={**metadata, "chunk_id": i}
            )
            documents.append(doc)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–ø–∏—Å–æ–∫
        current_docs = self.vector_stores[agent_name].documents
        current_docs.extend(documents)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        self.vector_stores[agent_name] = SimpleVectorStore(current_docs)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –∑–Ω–∞–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ {agent_name}")
    
    def get_knowledge_context(self, agent_name: str, query: str, k: int = None) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç–µ
        
        Args:
            agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏–π
        """
        relevant_docs = self.search_knowledge(agent_name, query, k)
        
        if not relevant_docs:
            return ""
        
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            source = doc.metadata.get('source', 'unknown')
            content = doc.page_content.strip()
            context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def initialize_all_agents_knowledge(self) -> Dict[str, bool]:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        
        Returns:
            Dict[str, bool]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        """
        results = {}
        
        # –°–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏—Ö —É—Ä–æ–≤–Ω–µ–π
        agent_mappings = {
            # Executive level
            'chief_seo_strategist': 'executive',
            'business_development_director': 'executive',
            
            # Management level  
            'task_coordination': 'management',
            'sales_operations_manager': 'management',
            'technical_seo_operations_manager': 'management', 
            'client_success_manager': 'management',
            
            # Operational level
            'lead_qualification': 'operational',
            'sales_conversation': 'operational',
            'proposal_generation': 'operational',
            'technical_seo_auditor': 'operational',
            'content_strategy': 'operational',
            'link_building': 'operational',
            'competitive_analysis': 'operational',
            'reporting': 'operational'
        }
        
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤...")
        
        for agent_name, agent_level in agent_mappings.items():
            try:
                vector_store = self.load_agent_knowledge(agent_name, agent_level)
                results[agent_name] = vector_store is not None
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è {agent_name}: {e}")
                results[agent_name] = False
        
        successful_count = sum(results.values())
        total_count = len(results)
        
        print(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_count}/{total_count} –∞–≥–µ–Ω—Ç–æ–≤")
        
        return results

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∑–Ω–∞–Ω–∏–π
knowledge_manager = KnowledgeManager()
